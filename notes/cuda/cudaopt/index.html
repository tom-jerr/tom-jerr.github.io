<!DOCTYPE html><html lang=en class=no-js><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta name=description content="在大规模语言模型（LLM）推理中，优化 CUDA 代码对于提升性能和效率至关重要。本文档介绍了一些关键的 CUDA 优化技术，帮助开发者更好地利用 GPU 资源进行 LLM 推理。"><meta name=author content=tom-jerr><link href=https://tom-jerr.github.io/notes/cuda/cudaopt/ rel=canonical><link href=../%E4%BB%8EGEMM%E5%AE%9E%E8%B7%B5CUDA%E4%BC%98%E5%8C%96/ rel=prev><link href=../Vector%20Add%20Optimization%20Example/ rel=next><link rel=alternate type=application/rss+xml title="RSS feed" href=../../../feed_rss_created.xml><link rel=alternate type=application/rss+xml title="RSS feed of updated content" href=../../../feed_rss_updated.xml><link rel=icon href=../../../img/rocket.png><meta name=generator content="mkdocs-1.6.1, mkdocs-material-9.7.1"><title>CUDA Optimization for LLM Inference - Want to be a MlSys wizard</title><link rel=stylesheet href=../../../assets/stylesheets/main.484c7ddc.min.css><link rel=stylesheet href=../../../assets/stylesheets/palette.ab4e12ef.min.css><link rel=preconnect href=https://fonts.gstatic.com crossorigin><link rel=stylesheet href="https://fonts.googleapis.com/css?family=JetBrains+Mono:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback"><style>:root{--md-text-font:"JetBrains Mono";--md-code-font:"Roboto Mono"}</style><link rel=stylesheet href=../../../css/custom.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-screen-webfont@1.1.0/style.css><link rel=stylesheet href=https://gcore.jsdelivr.net/npm/lxgw-wenkai-webfont@1.1.0/style.css><link rel=stylesheet href=../../../css/card.css><link rel=stylesheet href=../../../css/flink.css><link rel=stylesheet href=../../../css/tasklist.css><link rel=stylesheet href=../../../css/footer-stats.css><link rel=stylesheet href=../../../css/ai-summary.css><link rel=stylesheet href=../../../assets/document_dates/fonts/material-icons.css><link rel=stylesheet href=../../../assets/document_dates/tippy/light.css><link rel=stylesheet href=../../../assets/document_dates/tippy/material.css><link rel=stylesheet href=../../../assets/document_dates/tippy/shift-away.css><link rel=stylesheet href=../../../assets/document_dates/tippy/scale.css><link rel=stylesheet href=../../../assets/document_dates/tippy/backdrop.css><link rel=stylesheet href=../../../assets/document_dates/tippy/tippy.css><link rel=stylesheet href=../../../assets/document_dates/core/core.css><link rel=stylesheet href=../../../assets/document_dates/user.config.css><script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script><script id=__analytics>function __md_analytics(){function e(){dataLayer.push(arguments)}window.dataLayer=window.dataLayer||[],e("js",new Date),e("config","G-NKJWZM1JHX"),document.addEventListener("DOMContentLoaded",(function(){document.forms.search&&document.forms.search.query.addEventListener("blur",(function(){this.value&&e("event","search",{search_term:this.value})}));document$.subscribe((function(){var t=document.forms.feedback;if(void 0!==t)for(var a of t.querySelectorAll("[type=submit]"))a.addEventListener("click",(function(a){a.preventDefault();var n=document.location.pathname,d=this.getAttribute("data-md-value");e("event","feedback",{page:n,data:d}),t.firstElementChild.disabled=!0;var r=t.querySelector(".md-feedback__note [data-md-value='"+d+"']");r&&(r.hidden=!1)})),t.hidden=!1})),location$.subscribe((function(t){e("config","G-NKJWZM1JHX",{page_path:t.pathname})}))}));var t=document.createElement("script");t.async=!0,t.src="https://www.googletagmanager.com/gtag/js?id=G-NKJWZM1JHX",document.getElementById("__analytics").insertAdjacentElement("afterEnd",t)}</script><script>"undefined"!=typeof __md_analytics&&__md_analytics()</script><link href=../../../assets/stylesheets/glightbox.min.css rel=stylesheet><script src=../../../assets/javascripts/glightbox.min.js></script><style id=glightbox-style>
            html.glightbox-open { overflow: initial; height: 100%; }
            .gslide-title { margin-top: 0px; user-select: text; }
            .gslide-desc { color: #666; user-select: text; }
            .gslide-image img { background: white; }
            .gscrollbar-fixer { padding-right: 15px; }
            .gdesc-inner { font-size: 0.75rem; }
            body[data-md-color-scheme="slate"] .gdesc-inner { background: var(--md-default-bg-color); }
            body[data-md-color-scheme="slate"] .gslide-title { color: var(--md-default-fg-color); }
            body[data-md-color-scheme="slate"] .gslide-desc { color: var(--md-default-fg-color); }
        </style></head> <body dir=ltr data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo> <input class=md-toggle data-md-toggle=drawer type=checkbox id=__drawer autocomplete=off> <input class=md-toggle data-md-toggle=search type=checkbox id=__search autocomplete=off> <label class=md-overlay for=__drawer></label> <div data-md-component=skip> <a href=#cuda-optimization-for-llm-inference class=md-skip> Skip to content </a> </div> <div data-md-component=announce> </div> <header class=md-header data-md-component=header> <nav class="md-header__inner md-grid" aria-label=Header> <a href=../../.. title="Want to be a MlSys wizard" class="md-header__button md-logo" aria-label="Want to be a MlSys wizard" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg> </a> <label class="md-header__button md-icon" for=__drawer> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"></path></svg> </label> <div class=md-header__title data-md-component=header-title> <div class=md-header__ellipsis> <div class=md-header__topic> <span class=md-ellipsis> Want to be a MlSys wizard </span> </div> <div class=md-header__topic data-md-component=header-topic> <span class=md-ellipsis> CUDA Optimization for LLM Inference </span> </div> </div> </div> <form class=md-header__option data-md-component=palette> <input class=md-option data-md-color-media="(prefers-color-scheme: light)" data-md-color-scheme=default data-md-color-primary=white data-md-color-accent=indigo aria-label="light mode" type=radio name=__palette id=__palette_0> <label class="md-header__button md-icon" title="light mode" for=__palette_1 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 8a4 4 0 0 0-4 4 4 4 0 0 0 4 4 4 4 0 0 0 4-4 4 4 0 0 0-4-4m0 10a6 6 0 0 1-6-6 6 6 0 0 1 6-6 6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> <input class=md-option data-md-color-media="(prefers-color-scheme: dark)" data-md-color-scheme=slate data-md-color-primary=black data-md-color-accent=teal aria-label="dark mode" type=radio name=__palette id=__palette_1> <label class="md-header__button md-icon" title="dark mode" for=__palette_0 hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 18c-.89 0-1.74-.2-2.5-.55C11.56 16.5 13 14.42 13 12s-1.44-4.5-3.5-5.45C10.26 6.2 11.11 6 12 6a6 6 0 0 1 6 6 6 6 0 0 1-6 6m8-9.31V4h-4.69L12 .69 8.69 4H4v4.69L.69 12 4 15.31V20h4.69L12 23.31 15.31 20H20v-4.69L23.31 12z"></path></svg> </label> </form> <script>var palette=__md_get("__palette");if(palette&&palette.color){if("(prefers-color-scheme)"===palette.color.media){var media=matchMedia("(prefers-color-scheme: light)"),input=document.querySelector(media.matches?"[data-md-color-media='(prefers-color-scheme: light)']":"[data-md-color-media='(prefers-color-scheme: dark)']");palette.color.media=input.getAttribute("data-md-color-media"),palette.color.scheme=input.getAttribute("data-md-color-scheme"),palette.color.primary=input.getAttribute("data-md-color-primary"),palette.color.accent=input.getAttribute("data-md-color-accent")}for(var[key,value]of Object.entries(palette.color))document.body.setAttribute("data-md-color-"+key,value)}</script> <label class="md-header__button md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> </label> <div class=md-search data-md-component=search role=dialog> <label class=md-search__overlay for=__search></label> <div class=md-search__inner role=search> <form class=md-search__form name=search> <input type=text class=md-search__input name=query aria-label=Search placeholder=Search autocapitalize=off autocorrect=off autocomplete=off spellcheck=false data-md-component=search-query required> <label class="md-search__icon md-icon" for=__search> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"></path></svg> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"></path></svg> </label> <nav class=md-search__options aria-label=Search> <a href=javascript:void(0) class="md-search__icon md-icon" title=Share aria-label=Share data-clipboard data-clipboard-text data-md-component=search-share tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 16.08c-.76 0-1.44.3-1.96.77L8.91 12.7c.05-.23.09-.46.09-.7s-.04-.47-.09-.7l7.05-4.11c.54.5 1.25.81 2.04.81a3 3 0 0 0 3-3 3 3 0 0 0-3-3 3 3 0 0 0-3 3c0 .24.04.47.09.7L8.04 9.81C7.5 9.31 6.79 9 6 9a3 3 0 0 0-3 3 3 3 0 0 0 3 3c.79 0 1.5-.31 2.04-.81l7.12 4.15c-.05.21-.08.43-.08.66 0 1.61 1.31 2.91 2.92 2.91s2.92-1.3 2.92-2.91A2.92 2.92 0 0 0 18 16.08"></path></svg> </a> <button type=reset class="md-search__icon md-icon" title=Clear aria-label=Clear tabindex=-1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"></path></svg> </button> </nav> <div class=md-search__suggest data-md-component=search-suggest></div> </form> <div class=md-search__output> <div class=md-search__scrollwrap tabindex=0 data-md-scrollfix> <div class=md-search-result data-md-component=search-result> <div class=md-search-result__meta> Initializing search </div> <ol class=md-search-result__list role=presentation></ol> </div> </div> </div> </div> </div> <div class=md-header__source> <a href=https://github.com/tom-jerr/tom-jerr.github.io/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> tom-jerr's Site </div> </a> </div> </nav> </header> <div class=md-container data-md-component=container> <nav class=md-tabs aria-label=Tabs data-md-component=tabs> <div class=md-grid> <ul class=md-tabs__list> <li class=md-tabs__item> <a href=../../.. class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"></path></svg> Home </a> </li> <li class=md-tabs__item> <a href=../../ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> Notes </a> </li> <li class=md-tabs__item> <a href=../../../paperreadings/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"></path></svg> PaperReading </a> </li> <li class="md-tabs__item md-tabs__item--active"> <a href=../../../blogs/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M224 24c0-13.3 10.7-24 24-24 145.8 0 264 118.2 264 264 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-119.3-96.7-216-216-216-13.3 0-24-10.7-24-24M80 96c26.5 0 48 21.5 48 48v224c0 26.5 21.5 48 48 48s48-21.5 48-48-21.5-48-48-48c-8.8 0-16-7.2-16-16v-64c0-8.8 7.2-16 16-16 79.5 0 144 64.5 144 144s-64.5 144-144 144S32 447.5 32 368V144c0-26.5 21.5-48 48-48m168 0c92.8 0 168 75.2 168 168 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-66.3-53.7-120-120-120-13.3 0-24-10.7-24-24s10.7-24 24-24"></path></svg> Blogs </a> </li> <li class=md-tabs__item> <a href=../../../summary/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> Summaries </a> </li> <li class=md-tabs__item> <a href=../../../tags/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"></path></svg> Tags </a> </li> <li class=md-tabs__item> <a href=../../../links/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.78 3.653a3.936 3.936 0 1 1 5.567 5.567l-3.627 3.627a3.936 3.936 0 0 1-5.88-.353.75.75 0 0 0-1.18.928 5.436 5.436 0 0 0 8.12.486l3.628-3.628a5.436 5.436 0 1 0-7.688-7.688l-3 3a.75.75 0 0 0 1.06 1.061z"></path><path d="M7.28 11.153a3.936 3.936 0 0 1 5.88.353.75.75 0 0 0 1.18-.928 5.436 5.436 0 0 0-8.12-.486L2.592 13.72a5.436 5.436 0 1 0 7.688 7.688l3-3a.75.75 0 1 0-1.06-1.06l-3 3a3.936 3.936 0 0 1-5.567-5.568z"></path></svg> Links </a> </li> <li class=md-tabs__item> <a href=../../../about/ class=md-tabs__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-90.6-190.1c20.4 28 53.4 46.1 90.6 46.1s70.2-18.1 90.6-46.1c7.8-10.7 22.8-13.1 33.5-5.3s13.1 22.8 5.3 33.5C356.3 390 309.2 416 256 416s-100.3-26-129.4-65.9c-7.8-10.7-5.4-25.7 5.3-33.5s25.7-5.4 33.5 5.3M144 208a32 32 0 1 1 64 0 32 32 0 1 1-64 0m164 8c0 11-9 20-20 20s-20-9-20-20c0-33.1 26.9-60 60-60h16c33.1 0 60 26.9 60 60 0 11-9 20-20 20s-20-9-20-20-9-20-20-20h-16c-11 0-20 9-20 20"></path></svg> About </a> </li> </ul> </div> </nav> <main class=md-main data-md-component=main> <div class="md-main__inner md-grid"> <div class="md-sidebar md-sidebar--primary" data-md-component=sidebar data-md-type=navigation> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--primary md-nav--lifted" aria-label=Navigation data-md-level=0> <label class=md-nav__title for=__drawer> <a href=../../.. title="Want to be a MlSys wizard" class="md-nav__button md-logo" aria-label="Want to be a MlSys wizard" data-md-component=logo> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M2 21h18v-2H2M20 8h-2V5h2m0-2H4v10a4 4 0 0 0 4 4h6a4 4 0 0 0 4-4v-3h2a2 2 0 0 0 2-2V5a2 2 0 0 0-2-2"></path></svg> </a> Want to be a MlSys wizard </label> <div class=md-nav__source> <a href=https://github.com/tom-jerr/tom-jerr.github.io/ title="Go to repository" class=md-source data-md-component=source> <div class="md-source__icon md-icon"> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"></path></svg> </div> <div class=md-source__repository> tom-jerr's Site </div> </a> </div> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../.. class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M10 20v-6h4v6h5v-8h3L12 3 2 12h3v8z"></path></svg> <span class=md-ellipsis> Home </span> </a> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2> <div class="md-nav__link md-nav__container"> <a href=../../ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M18 22a2 2 0 0 0 2-2V4a2 2 0 0 0-2-2h-6v7L9.5 7.5 7 9V2H6a2 2 0 0 0-2 2v16a2 2 0 0 0 2 2z"></path></svg> <span class=md-ellipsis> Notes </span> </a> <label class="md-nav__link " for=__nav_2 id=__nav_2_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2> <span class="md-nav__icon md-icon"></span> Notes </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_2> <label class=md-nav__link for=__nav_2_2 id=__nav_2_2_label tabindex=0> <span class=md-ellipsis> Python </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_2_label aria-expanded=false> <label class=md-nav__title for=__nav_2_2> <span class="md-nav__icon md-icon"></span> Python </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../python/Python%20%E4%B8%AD%E7%9A%84%E8%A3%85%E9%A5%B0%E5%99%A8/ class=md-nav__link> <span class=md-ellipsis> Python 中的装饰器 </span> </a> </li> <li class=md-nav__item> <a href=../../python/Python%20%E4%B8%AD%E7%9A%84%E5%B9%B6%E8%A1%8C/ class=md-nav__link> <span class=md-ellipsis> Python 中的并行 </span> </a> </li> <li class=md-nav__item> <a href=../../python/Asyncio/ class=md-nav__link> <span class=md-ellipsis> Asyncio 基础 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_3> <label class=md-nav__link for=__nav_2_3 id=__nav_2_3_label tabindex=0> <span class=md-ellipsis> tiny-llm </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_3_label aria-expanded=false> <label class=md-nav__title for=__nav_2_3> <span class="md-nav__icon md-icon"></span> tiny-llm </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../tiny-llm/Group%20Query%20Attention/ class=md-nav__link> <span class=md-ellipsis> Group Query Attention </span> </a> </li> <li class=md-nav__item> <a href=../../tiny-llm/RMSNorm%20%26%20MLP/ class=md-nav__link> <span class=md-ellipsis> RMSNorm &amp; MLP </span> </a> </li> <li class=md-nav__item> <a href=../../tiny-llm/Batching%20Inference%20%26%20KV%20Cache/ class=md-nav__link> <span class=md-ellipsis> Batch Inference &amp; KV Cache </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_4> <label class=md-nav__link for=__nav_2_4 id=__nav_2_4_label tabindex=0> <span class=md-ellipsis> Active SLAM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_4_label aria-expanded=false> <label class=md-nav__title for=__nav_2_4> <span class="md-nav__icon md-icon"></span> Active SLAM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paperreadings/activeslam/vins%E5%9F%BA%E7%A1%80/ class=md-nav__link> <span class=md-ellipsis> VINS 基础 </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/activeslam/active%20slam/ class=md-nav__link> <span class=md-ellipsis> Active SLAM 概述 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_5> <label class=md-nav__link for=__nav_2_5 id=__nav_2_5_label tabindex=0> <span class=md-ellipsis> nebula </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_5_label aria-expanded=false> <label class=md-nav__title for=__nav_2_5> <span class="md-nav__icon md-icon"></span> nebula </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/1.visitor%20pattern/ class=md-nav__link> <span class=md-ellipsis> Vistor Pattern in nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/2.memory%20management/ class=md-nav__link> <span class=md-ellipsis> Memory Management in nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/3.concurrent%20lru%20cache/ class=md-nav__link> <span class=md-ellipsis> Concurrent LRU Cache in nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/4.folly%20future%20promise/ class=md-nav__link> <span class=md-ellipsis> Folly Future Promise framework in nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/5.nGQL%20life/ class=md-nav__link> <span class=md-ellipsis> nGQL Life in Nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/6.raft-wal/ class=md-nav__link> <span class=md-ellipsis> Raft Wal in Nebula </span> </a> </li> <li class=md-nav__item> <a href=../../../projects/nebula-vsearch/understanding/7.how%20to%20modify%20sql/ class=md-nav__link> <span class=md-ellipsis> How to Modify nGQL </span> </a> </li> <li class=md-nav__item> <a href="../../../projects/nebula-vsearch/understandings/8.a kv life.md" class=md-nav__link> <span class=md-ellipsis> A Life of KV Pairs </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_6> <label class=md-nav__link for=__nav_2_6 id=__nav_2_6_label tabindex=0> <span class=md-ellipsis> CS61C </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_6_label aria-expanded=false> <label class=md-nav__title for=__nav_2_6> <span class="md-nav__icon md-icon"></span> CS61C </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CS61C/lecture2-Intro%20to%20C/ class=md-nav__link> <span class=md-ellipsis> 1 C Intro </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture4_C%20Memory%20Endianness/ class=md-nav__link> <span class=md-ellipsis> 4 C Memory Endianness </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture10_Combinational%20Logic/ class=md-nav__link> <span class=md-ellipsis> 10 Combinational Logic </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture11_FSMs/ class=md-nav__link> <span class=md-ellipsis> 11 FSMs </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture14_Datapath/ class=md-nav__link> <span class=md-ellipsis> 14 Datapath </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture15_Data-LevelParallelism/ class=md-nav__link> <span class=md-ellipsis> 15 Data-Level Parallelism </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture16_Thread-LevelParallelism/ class=md-nav__link> <span class=md-ellipsis> 16 Thread-Level Parallelism </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture17_Process_LevelParallelism/ class=md-nav__link> <span class=md-ellipsis> 17 Process-Level Parallelism </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture18_Caches/ class=md-nav__link> <span class=md-ellipsis> 18 Caches </span> </a> </li> <li class=md-nav__item> <a href=../../CS61C/lecture21_Virtual_Memory/ class=md-nav__link> <span class=md-ellipsis> 21 Virtual Memory </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_7> <label class=md-nav__link for=__nav_2_7 id=__nav_2_7_label tabindex=0> <span class=md-ellipsis> CSAPP </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_7_label aria-expanded=false> <label class=md-nav__title for=__nav_2_7> <span class="md-nav__icon md-icon"></span> CSAPP </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CSAPP/5-%E4%BC%98%E5%8C%96%E7%A8%8B%E5%BA%8F%E6%80%A7%E8%83%BD/ class=md-nav__link> <span class=md-ellipsis> 5-优化程序性能 </span> </a> </li> <li class=md-nav__item> <a href=../../CSAPP/6-%E5%AD%98%E5%82%A8%E5%99%A8%E5%B1%82%E6%AC%A1%E7%BB%93%E6%9E%84/ class=md-nav__link> <span class=md-ellipsis> 6-存储器层次结构 </span> </a> </li> <li class=md-nav__item> <a href=../../CSAPP/7-%E9%93%BE%E6%8E%A5/ class=md-nav__link> <span class=md-ellipsis> 7-链接 </span> </a> </li> <li class=md-nav__item> <a href=../../CSAPP/8-%E5%BC%82%E5%B8%B8%E6%8E%A7%E5%88%B6%E6%B5%81/ class=md-nav__link> <span class=md-ellipsis> 8-异常控制流 </span> </a> </li> <li class=md-nav__item> <a href=../../CSAPP/%E7%A8%8B%E5%BA%8F%E7%9A%84%E7%BB%93%E6%9E%84%E5%92%8C%E6%89%A7%E8%A1%8C/ class=md-nav__link> <span class=md-ellipsis> 程序的结构和执行 </span> </a> </li> <li class=md-nav__item> <a href=../../CSAPP/%E5%9C%A8%E7%B3%BB%E7%BB%9F%E4%B8%8A%E8%BF%90%E8%A1%8C%E7%A8%8B%E5%BA%8F/ class=md-nav__link> <span class=md-ellipsis> 在系统上运行程序 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_8> <label class=md-nav__link for=__nav_2_8 id=__nav_2_8_label tabindex=0> <span class=md-ellipsis> 6.s081 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_8_label aria-expanded=false> <label class=md-nav__title for=__nav_2_8> <span class="md-nav__icon md-icon"></span> 6.s081 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../6.s081/1-OS%E7%9A%84%E9%9A%94%E7%A6%BB%E6%80%A7/ class=md-nav__link> <span class=md-ellipsis> 1 OS的隔离性 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/2-%E8%99%9A%E6%8B%9F%E5%86%85%E5%AD%98%E5%AE%9E%E7%8E%B0/ class=md-nav__link> <span class=md-ellipsis> 2 虚拟内存实现 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/3-%E7%B3%BB%E7%BB%9F%E8%B0%83%E7%94%A8%E5%92%8C%E9%99%B7%E5%85%A5/ class=md-nav__link> <span class=md-ellipsis> 3 系统调用和陷入 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/4-%E7%BC%BA%E9%A1%B5%E4%B8%AD%E6%96%AD/ class=md-nav__link> <span class=md-ellipsis> 4 缺页中断 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/5-%E7%A1%AC%E4%BB%B6%E4%B8%AD%E6%96%AD/ class=md-nav__link> <span class=md-ellipsis> 5 硬件中断 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/6-%E5%A4%9A%E6%A0%B8%E5%92%8C%E9%94%81/ class=md-nav__link> <span class=md-ellipsis> 6 多核和锁 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/7-%E8%BF%9B%E7%A8%8B%E5%88%87%E6%8D%A2/ class=md-nav__link> <span class=md-ellipsis> 7 进程切换 </span> </a> </li> <li class=md-nav__item> <a href=../../6.s081/8-%E7%9D%A1%E7%9C%A0%E5%92%8C%E5%94%A4%E9%86%92/ class=md-nav__link> <span class=md-ellipsis> 8 睡眠和唤醒 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_9> <label class=md-nav__link for=__nav_2_9 id=__nav_2_9_label tabindex=0> <span class=md-ellipsis> C++ Primer </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_9_label aria-expanded=false> <label class=md-nav__title for=__nav_2_9> <span class="md-nav__icon md-icon"></span> C++ Primer </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/2_%E5%8F%98%E9%87%8F%E5%92%8C%E5%9F%BA%E6%9C%AC%E7%B1%BB%E5%9E%8B/ class=md-nav__link> <span class=md-ellipsis> 2_变量和基本类型 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/3_%E5%AD%97%E7%AC%A6%E4%B8%B2%E5%90%91%E9%87%8F%E6%95%B0%E7%BB%84/ class=md-nav__link> <span class=md-ellipsis> 3_字符串向量数组 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/4_%E8%A1%A8%E8%BE%BE%E5%BC%8F/ class=md-nav__link> <span class=md-ellipsis> 4_表达式 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/5_%E8%AF%AD%E5%8F%A5/ class=md-nav__link> <span class=md-ellipsis> 5_语句 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/6_%E5%87%BD%E6%95%B0/ class=md-nav__link> <span class=md-ellipsis> 6_函数 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/7_%E7%B1%BB/ class=md-nav__link> <span class=md-ellipsis> 7_类 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/8_IO%E5%BA%93/ class=md-nav__link> <span class=md-ellipsis> 8_IO库 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/9_%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8/ class=md-nav__link> <span class=md-ellipsis> 9_顺序容器 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/10_%E6%B3%9B%E5%9E%8B%E7%AE%97%E6%B3%95/ class=md-nav__link> <span class=md-ellipsis> 10_泛型算法 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/11_%E5%85%B3%E8%81%94%E5%AE%B9%E5%99%A8/ class=md-nav__link> <span class=md-ellipsis> 11_关联容器 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/12_%E5%8A%A8%E6%80%81%E5%86%85%E5%AD%98/ class=md-nav__link> <span class=md-ellipsis> 12_动态内存 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/13_%E6%8B%B7%E8%B4%9D%E6%8E%A7%E5%88%B6/ class=md-nav__link> <span class=md-ellipsis> 13_拷贝控制 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/14_%E9%87%8D%E8%BD%BD%E8%BF%90%E7%AE%97%E5%92%8C%E7%B1%BB%E5%9E%8B%E8%BD%AC%E6%8D%A2/ class=md-nav__link> <span class=md-ellipsis> 14_重载运算和类型转换 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/15_%E9%9D%A2%E5%90%91%E5%AF%B9%E8%B1%A1%E7%A8%8B%E5%BA%8F%E8%AE%BE%E8%AE%A1/ class=md-nav__link> <span class=md-ellipsis> 15_面向对象程序设计 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/%E5%B9%B6%E5%8F%91%E4%B8%8E%E5%A4%9A%E7%BA%BF%E7%A8%8B%E7%BC%96%E7%A8%8B/ class=md-nav__link> <span class=md-ellipsis> 并发与多线程编程 </span> </a> </li> <li class=md-nav__item> <a href=../../c%2B%2Bprimer/Makefile%E5%AD%A6%E4%B9%A0/ class=md-nav__link> <span class=md-ellipsis> Makefile学习 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_10> <label class=md-nav__link for=__nav_2_10 id=__nav_2_10_label tabindex=0> <span class=md-ellipsis> C++ 新特性 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_10_label aria-expanded=false> <label class=md-nav__title for=__nav_2_10> <span class="md-nav__icon md-icon"></span> C++ 新特性 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../C%2B%2B/1-Container/ class=md-nav__link> <span class=md-ellipsis> 1-Container </span> </a> </li> <li class=md-nav__item> <a href=../../C%2B%2B/2-Algorithm/ class=md-nav__link> <span class=md-ellipsis> 2-Algorithm </span> </a> </li> <li class=md-nav__item> <a href=../../C%2B%2B/3-Iterator/ class=md-nav__link> <span class=md-ellipsis> 3-Iterator </span> </a> </li> <li class=md-nav__item> <a href=../../C++/4-FileSystem.md class=md-nav__link> <span class=md-ellipsis> 4-FileSystem </span> </a> </li> <li class=md-nav__item> <a href=../../C%2B%2B/5-View/ class=md-nav__link> <span class=md-ellipsis> 5-View </span> </a> </li> <li class=md-nav__item> <a href=../../C%2B%2B/6-Span/ class=md-nav__link> <span class=md-ellipsis> 6-Span </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_11> <label class=md-nav__link for=__nav_2_11 id=__nav_2_11_label tabindex=0> <span class=md-ellipsis> C++ Concurrency in Action </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_11_label aria-expanded=false> <label class=md-nav__title for=__nav_2_11> <span class="md-nav__icon md-icon"></span> C++ Concurrency in Action </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../C%2B%2BConcurrency/1-thread/ class=md-nav__link> <span class=md-ellipsis> 1-Thread </span> </a> </li> <li class=md-nav__item> <a href=../../C%2B%2BConcurrency/2-Memory%20Model/ class=md-nav__link> <span class=md-ellipsis> 2-Memory Model in C++ </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_12> <label class=md-nav__link for=__nav_2_12 id=__nav_2_12_label tabindex=0> <span class=md-ellipsis> CMU15445 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_12_label aria-expanded=false> <label class=md-nav__title for=__nav_2_12> <span class="md-nav__icon md-icon"></span> CMU15445 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../CMU15445/3-Database%20Storage/ class=md-nav__link> <span class=md-ellipsis> 3-Database Storage </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/4-Database_Storage_II/ class=md-nav__link> <span class=md-ellipsis> 4-Database_Storage_II </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/5-Buffer%20Pool/ class=md-nav__link> <span class=md-ellipsis> 5-Buffer Pool </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/6-Hash%20Table/ class=md-nav__link> <span class=md-ellipsis> 6-Hash Table </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/7-B%2BTree/ class=md-nav__link> <span class=md-ellipsis> 7-B+Tree </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/8-Index%20Concurrency/ class=md-nav__link> <span class=md-ellipsis> 8-Index Concurrency </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/9-Sort%26Aggregation/ class=md-nav__link> <span class=md-ellipsis> 9-Sort&amp;Aggregation </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/10-Join%20Algorithm/ class=md-nav__link> <span class=md-ellipsis> 10-Join Algorithm </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/11-Query_Execution/ class=md-nav__link> <span class=md-ellipsis> 11-Query_Execution </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/12-Query_Plan%26Optimization/ class=md-nav__link> <span class=md-ellipsis> 12-Query_Plan&amp;Optimization </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/13-Concurrency_Control_Theory/ class=md-nav__link> <span class=md-ellipsis> 13-Concurrency_Control_Theory </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/14-Two_Phase_Lock/ class=md-nav__link> <span class=md-ellipsis> 14-Two_Phase_Lock </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/15-Timestamp_Ordering_Concurrency/ class=md-nav__link> <span class=md-ellipsis> 15-Timestamp_Ordering_Concurrency </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/16-Mult-Version_Concurrency/ class=md-nav__link> <span class=md-ellipsis> 16-Mult-Version_Concurrency </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/17-Database_Logging/ class=md-nav__link> <span class=md-ellipsis> 17-Database_Logging </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/18-Database_Recovery/ class=md-nav__link> <span class=md-ellipsis> 18-Database_Recovery </span> </a> </li> <li class=md-nav__item> <a href=../../CMU15445/19-Introducd_to_distributed_database/ class=md-nav__link> <span class=md-ellipsis> 19-Introducd_to_distributed_database </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_13> <label class=md-nav__link for=__nav_2_13 id=__nav_2_13_label tabindex=0> <span class=md-ellipsis> 6.824 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_13_label aria-expanded=false> <label class=md-nav__title for=__nav_2_13> <span class="md-nav__icon md-icon"></span> 6.824 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../6.824/MapReduce/ class=md-nav__link> <span class=md-ellipsis> MapReduce 论文阅读 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_2_14> <label class=md-nav__link for=__nav_2_14 id=__nav_2_14_label tabindex=0> <span class=md-ellipsis> Linux Tools </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_2_14_label aria-expanded=false> <label class=md-nav__title for=__nav_2_14> <span class="md-nav__icon md-icon"></span> Linux Tools </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../linux%20tools/1-grep/ class=md-nav__link> <span class=md-ellipsis> grep </span> </a> </li> <li class=md-nav__item> <a href=../../linux%20tools/2-sed/ class=md-nav__link> <span class=md-ellipsis> sed </span> </a> </li> <li class=md-nav__item> <a href=../../linux%20tools/3-awk/ class=md-nav__link> <span class=md-ellipsis> awk </span> </a> </li> <li class=md-nav__item> <a href=../../linux%20tools/4-find/ class=md-nav__link> <span class=md-ellipsis> find </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3> <div class="md-nav__link md-nav__container"> <a href=../../../paperreadings/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M384 512H96c-53 0-96-43-96-96V96C0 43 43 0 96 0h304c26.5 0 48 21.5 48 48v288c0 20.9-13.4 38.7-32 45.3V448c17.7 0 32 14.3 32 32s-14.3 32-32 32zM96 384c-17.7 0-32 14.3-32 32s14.3 32 32 32h256v-64zm32-232c0 13.3 10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24H152c-13.3 0-24 10.7-24 24m24 72c-13.3 0-24 10.7-24 24s10.7 24 24 24h176c13.3 0 24-10.7 24-24s-10.7-24-24-24z"></path></svg> <span class=md-ellipsis> PaperReading </span> </a> <label class="md-nav__link " for=__nav_3 id=__nav_3_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3> <span class="md-nav__icon md-icon"></span> PaperReading </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_2> <label class=md-nav__link for=__nav_3_2 id=__nav_3_2_label tabindex=0> <span class=md-ellipsis> Database Systems </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_2_label aria-expanded=false> <label class=md-nav__title for=__nav_3_2> <span class="md-nav__icon md-icon"></span> Database Systems </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paperreadings/db/storage/NoMMAP/ class=md-nav__link> <span class=md-ellipsis> Do not use MMAP in DBMS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_3> <label class=md-nav__link for=__nav_3_3 id=__nav_3_3_label tabindex=0> <span class=md-ellipsis> Vector Search </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_3_label aria-expanded=false> <label class=md-nav__title for=__nav_3_3> <span class="md-nav__icon md-icon"></span> Vector Search </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paperreadings/vector%20search/IQAN/ class=md-nav__link> <span class=md-ellipsis> IQAN </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/vector%20search/MIRAGE-ANNS/ class=md-nav__link> <span class=md-ellipsis> MIRAGE-ANNS </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/vector%20search/LSM-VEC/ class=md-nav__link> <span class=md-ellipsis> LSM-VEC </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/vector%20search/PANNS/ class=md-nav__link> <span class=md-ellipsis> PANNS </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_3_4> <label class=md-nav__link for=__nav_3_4 id=__nav_3_4_label tabindex=0> <span class=md-ellipsis> Active SLAM </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_3_4_label aria-expanded=false> <label class=md-nav__title for=__nav_3_4> <span class="md-nav__icon md-icon"></span> Active SLAM </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../paperreadings/activeslam/PL-VINS/ class=md-nav__link> <span class=md-ellipsis> PL-VINS </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/activeslam/3D%20Active%20Metric-Semantic%20SLAM/ class=md-nav__link> <span class=md-ellipsis> 3D Active Metric-Semantic SLAM </span> </a> </li> <li class=md-nav__item> <a href=../../../paperreadings/activeslam/Exploration%20with%20Global%20Consistency%20%20Using%20Real-Time%20Re-integration%20and%20Active%20Loop%20Closure/ class=md-nav__link> <span class=md-ellipsis> Exploration with Global Consistency Using Real-Time Re-integration and Active Loop Closure </span> </a> </li> </ul> </nav> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4 checked> <div class="md-nav__link md-nav__container"> <a href=../../../blogs/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M224 24c0-13.3 10.7-24 24-24 145.8 0 264 118.2 264 264 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-119.3-96.7-216-216-216-13.3 0-24-10.7-24-24M80 96c26.5 0 48 21.5 48 48v224c0 26.5 21.5 48 48 48s48-21.5 48-48-21.5-48-48-48c-8.8 0-16-7.2-16-16v-64c0-8.8 7.2-16 16-16 79.5 0 144 64.5 144 144s-64.5 144-144 144S32 447.5 32 368V144c0-26.5 21.5-48 48-48m168 0c92.8 0 168 75.2 168 168 0 13.3-10.7 24-24 24s-24-10.7-24-24c0-66.3-53.7-120-120-120-13.3 0-24-10.7-24-24s10.7-24 24-24"></path></svg> <span class=md-ellipsis> Blogs </span> </a> <label class="md-nav__link " for=__nav_4 id=__nav_4_label tabindex> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_4_label aria-expanded=true> <label class=md-nav__title for=__nav_4> <span class="md-nav__icon md-icon"></span> Blogs </label> <ul class=md-nav__list data-md-scrollfix> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_2> <label class=md-nav__link for=__nav_4_2 id=__nav_4_2_label tabindex=0> <span class=md-ellipsis> SGLang 专题 </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_2_label aria-expanded=false> <label class=md-nav__title for=__nav_4_2> <span class="md-nav__icon md-icon"></span> SGLang 专题 </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../sglang/%E4%B8%80%E6%9D%A1%20Request%20%E5%9C%A8%20SGLang%20%E7%9A%84%E5%89%8D%E4%B8%96%E4%BB%8A%E7%94%9F/ class=md-nav__link> <span class=md-ellipsis> 一条 Request 在 SGLang 的前世今生 </span> </a> </li> <li class=md-nav__item> <a href=../../sglang/SGLang%20Scheduler%20%E6%8A%80%E6%9C%AF%E5%8F%98%E8%BF%81/ class=md-nav__link> <span class=md-ellipsis> SGLang Schedular 技术变迁 </span> </a> </li> <li class=md-nav__item> <a href=../../sglang/RadixAttention%20%E4%BD%A0%E9%9C%80%E8%A6%81%E7%9F%A5%E9%81%93%E7%9A%84%E5%85%A8%E9%83%A8%E7%BB%86%E8%8A%82/ class=md-nav__link> <span class=md-ellipsis> RadixAttention 你需要知道的细节 </span> </a> </li> <li class=md-nav__item> <a href="../../sgalng/SGLang 中的 TP + PP.md" class=md-nav__link> <span class=md-ellipsis> SGLang 中的 TP + PP 流程浅析(以 Qwen2 为例) </span> </a> </li> <li class=md-nav__item> <a href=../../sglang/%E4%BB%8E%E4%BB%A3%E7%A0%81%E7%9C%8B%20SGLang%20%E7%9A%84%20KV%20Cache/ class=md-nav__link> <span class=md-ellipsis> 从代码看 SGLang 的 KV Cache </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_3> <label class=md-nav__link for=__nav_4_3 id=__nav_4_3_label tabindex=0> <span class=md-ellipsis> LLM Basic Concepts </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_3_label aria-expanded=false> <label class=md-nav__title for=__nav_4_3> <span class="md-nav__icon md-icon"></span> LLM Basic Concepts </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../llm_inference/attention%26transformer/ class=md-nav__link> <span class=md-ellipsis> Attention &amp; Transformers </span> </a> </li> <li class=md-nav__item> <a href=../../llm_inference/transformerbasedllm/ class=md-nav__link> <span class=md-ellipsis> Transformer-Based LLM Architecture </span> </a> </li> <li class=md-nav__item> <a href=../../llm_inference/Introduction/ class=md-nav__link> <span class=md-ellipsis> Introduction for LLM Inference </span> </a> </li> <li class=md-nav__item> <a href=../../llm_inference/parallelization/ class=md-nav__link> <span class=md-ellipsis> Parallelizatoin Concepts </span> </a> </li> <li class=md-nav__item> <a href=../../llm_inference/LLMparallelization/ class=md-nav__link> <span class=md-ellipsis> Parallelization in LLM Inference </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_4> <label class=md-nav__link for=__nav_4_4 id=__nav_4_4_label tabindex=0> <span class=md-ellipsis> LLM Serving </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_4_label aria-expanded=false> <label class=md-nav__title for=__nav_4_4> <span class="md-nav__icon md-icon"></span> LLM Serving </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../blogs/posts/PageAttention/ class=md-nav__link> <span class=md-ellipsis> PageAttention </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/FlashAttention%20%E5%8E%9F%E7%90%86%20v1-v2/ class=md-nav__link> <span class=md-ellipsis> FlashAttention </span> </a> </li> <li class=md-nav__item> <a href=../../sglang/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%8E%A8%E7%90%86%E6%9C%8D%E5%8A%A1%E4%B8%AD%E7%9A%84%20Batching/ class=md-nav__link> <span class=md-ellipsis> 大模型推理服务中的 Batching </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--active md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_4_5 checked> <label class=md-nav__link for=__nav_4_5 id=__nav_4_5_label tabindex=0> <span class=md-ellipsis> CUDA </span> <span class="md-nav__icon md-icon"></span> </label> <nav class=md-nav data-md-level=2 aria-labelledby=__nav_4_5_label aria-expanded=true> <label class=md-nav__title for=__nav_4_5> <span class="md-nav__icon md-icon"></span> CUDA </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../%E4%BB%8EGEMM%E5%AE%9E%E8%B7%B5CUDA%E4%BC%98%E5%8C%96/ class=md-nav__link> <span class=md-ellipsis> 从 GEMM 实践 CUDA 优化 </span> </a> </li> <li class="md-nav__item md-nav__item--active"> <input class="md-nav__toggle md-toggle" type=checkbox id=__toc> <label class="md-nav__link md-nav__link--active" for=__toc> <span class=md-ellipsis> CUDA Optimization for LLM Inference </span> <span class="md-nav__icon md-icon"></span> </label> <a href=./ class="md-nav__link md-nav__link--active"> <span class=md-ellipsis> CUDA Optimization for LLM Inference </span> </a> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#gpu class=md-nav__link> <span class=md-ellipsis> GPU 硬件架构 </span> </a> <nav class=md-nav aria-label="GPU 硬件架构"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#streaming-multiprocessor-sm-based-on-h100-hopper-architecture class=md-nav__link> <span class=md-ellipsis> Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture) </span> </a> <nav class=md-nav aria-label="Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#functional-units-h100-specifics class=md-nav__link> <span class=md-ellipsis> Functional Units (H100 Specifics) </span> </a> </li> <li class=md-nav__item> <a href=#warp-contexts-occupancy class=md-nav__link> <span class=md-ellipsis> Warp Contexts &amp; Occupancy </span> </a> </li> <li class=md-nav__item> <a href=#warp-scheduler-h100 class=md-nav__link> <span class=md-ellipsis> Warp Scheduler (H100) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gpu-memory-hierarchy-h100-enhanced class=md-nav__link> <span class=md-ellipsis> GPU Memory Hierarchy (H100 Enhanced) </span> </a> </li> <li class=md-nav__item> <a href=#warp-h100-scheduling-execution class=md-nav__link> <span class=md-ellipsis> Warp (H100 Scheduling &amp; Execution) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cuda-cuda2 class=md-nav__link> <span class=md-ellipsis> CUDA 计算模型[^cuda2] </span> </a> <nav class=md-nav aria-label="CUDA 计算模型[^cuda2]"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 与硬件的对应 </span> </a> <nav class=md-nav aria-label=与硬件的对应> <ul class=md-nav__list> <li class=md-nav__item> <a href=#thread class=md-nav__link> <span class=md-ellipsis> Thread </span> </a> </li> <li class=md-nav__item> <a href=#thread-block class=md-nav__link> <span class=md-ellipsis> Thread Block </span> </a> </li> <li class=md-nav__item> <a href=#grid class=md-nav__link> <span class=md-ellipsis> Grid </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#kernel class=md-nav__link> <span class=md-ellipsis> Kernel 执行流程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#roofline class=md-nav__link> <span class=md-ellipsis> Roofline 模型 </span> </a> </li> <li class=md-nav__item> <a href=#cuda-kernel class=md-nav__link> <span class=md-ellipsis> CUDA Kernel 性能调优 </span> </a> <nav class=md-nav aria-label="CUDA Kernel 性能调优"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#memory-bound class=md-nav__link> <span class=md-ellipsis> Memory-Bound </span> </a> <nav class=md-nav aria-label=Memory-Bound> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-instruction-issue-efficiency class=md-nav__link> <span class=md-ellipsis> 1. 指令层级：指令发射效率 (Instruction Issue Efficiency) </span> </a> </li> <li class=md-nav__item> <a href=#2-mlp-memory-level-parallelism class=md-nav__link> <span class=md-ellipsis> 2. 数据层级：内存级并行度 (MLP, Memory Level Parallelism) </span> </a> </li> <li class=md-nav__item> <a href=#3-transaction-utilization class=md-nav__link> <span class=md-ellipsis> 3. 硬件层级：传输粒度与利用率 (Transaction &amp; Utilization) </span> </a> </li> <li class=md-nav__item> <a href=#4-physical-conflict class=md-nav__link> <span class=md-ellipsis> 4. 架构层级：物理冲突 (Physical Conflict) </span> </a> </li> <li class=md-nav__item> <a href=#5-asynchronous-copy-cpasync class=md-nav__link> <span class=md-ellipsis> 5. Asynchronous Copy (cp.async) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#compute-bound class=md-nav__link> <span class=md-ellipsis> Compute-Bound </span> </a> <nav class=md-nav aria-label=Compute-Bound> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-tensor-cores-wmma-mma class=md-nav__link> <span class=md-ellipsis> 1. 使用 Tensor Cores (WMMA / MMA) </span> </a> </li> <li class=md-nav__item> <a href=#2-precision-reduction class=md-nav__link> <span class=md-ellipsis> 2. 降低精度 (Precision Reduction) </span> </a> </li> <li class=md-nav__item> <a href=#3-ilp-instruction-level-parallelism class=md-nav__link> <span class=md-ellipsis> 3. 指令级并行 (ILP, Instruction Level Parallelism) </span> </a> </li> <li class=md-nav__item> <a href=#4-dual-issue class=md-nav__link> <span class=md-ellipsis> 4. 双发射 (Dual Issue) / 并发执行 </span> </a> </li> <li class=md-nav__item> <a href=#5-fma-fused-multiply-add class=md-nav__link> <span class=md-ellipsis> 5. 使用 FMA (Fused Multiply-Add) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cuda class=md-nav__link> <span class=md-ellipsis> CUDA 常用优化技巧 </span> </a> <nav class=md-nav aria-label="CUDA 常用优化技巧"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#maximize-compiler-computationgpu-miarch class=md-nav__link> <span class=md-ellipsis> Maximize Compiler Computation[^gpu-miarch] </span> </a> </li> <li class=md-nav__item> <a href=#coalescing-memory-access class=md-nav__link> <span class=md-ellipsis> Coalescing Memory Access </span> </a> </li> <li class=md-nav__item> <a href=#avoid-bank-conflicts-in-shared-memory class=md-nav__link> <span class=md-ellipsis> Avoid Bank Conflicts in Shared Memory </span> </a> <nav class=md-nav aria-label="Avoid Bank Conflicts in Shared Memory"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#solutionsbankconflict class=md-nav__link> <span class=md-ellipsis> Solutions[^bankconflict] </span> </a> </li> <li class=md-nav__item> <a href=#matmul-bank-conflict-avoidance-examplebankconflict2 class=md-nav__link> <span class=md-ellipsis> Matmul Bank Conflict Avoidance Example[^bankconflict2] </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#double-bufferingbankconflict2 class=md-nav__link> <span class=md-ellipsis> Double Buffering[^bankconflict2] </span> </a> </li> <li class=md-nav__item> <a href=#warp-shuffle-reduction class=md-nav__link> <span class=md-ellipsis> Warp Shuffle &amp; Reduction </span> </a> </li> <li class=md-nav__item> <a href=#tilematmul class=md-nav__link> <span class=md-ellipsis> Tile[^matmul] </span> </a> </li> <li class=md-nav__item> <a href=#wrap-tilematmul class=md-nav__link> <span class=md-ellipsis> Wrap Tile[^matmul] </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#transformer class=md-nav__link> <span class=md-ellipsis> Transformer 内部算子优化 </span> </a> <nav class=md-nav aria-label="Transformer 内部算子优化"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#softmax-layernorm class=md-nav__link> <span class=md-ellipsis> Softmax &amp; LayerNorm 优化 </span> </a> </li> <li class=md-nav__item> <a href=#flash-attention class=md-nav__link> <span class=md-ellipsis> Flash Attention 优化 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 参考资料 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../Vector%20Add%20Optimization%20Example/ class=md-nav__link> <span class=md-ellipsis> Vector Add Optimization Example </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../blogs/posts/%E4%B8%8A%E7%AF%87%EF%BC%9A%E5%88%9D%E8%AF%86%20Nebula%20Graph%20%E2%80%94%E2%80%94%20%E5%90%91%E9%87%8F%E7%B1%BB%E5%9E%8B%E6%94%AF%E6%8C%81/ class=md-nav__link> <span class=md-ellipsis> 上篇：初识 Nebula Graph —— 向量类型支持 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/%E4%B8%AD%E7%AF%87%EF%BC%9AVector%20%E7%B1%BB%E5%9E%8B%E7%9A%84%20DDL%26DML%20%E9%80%82%E9%85%8D/ class=md-nav__link> <span class=md-ellipsis> 中篇：Vector 类型的 DDL &amp; DML 适配 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/%E4%B8%8B%E7%AF%87%EF%BC%9A%E5%90%91%E9%87%8F%E7%B4%A2%E5%BC%95%E4%B8%8E%E7%9B%B8%E4%BC%BC%E5%BA%A6%E6%90%9C%E7%B4%A2%20%E2%80%94%E2%80%94%20Nebula%20Graph%20%E7%9A%84%20ANN%20%E5%AE%9E%E7%8E%B0%E4%B9%8B%E8%B7%AF/ class=md-nav__link> <span class=md-ellipsis> 下篇：向量索引与相似度搜索 —— Nebula Graph 的 ANN 实现之路 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/C%2B%2B%E5%BC%82%E6%AD%A5%E6%96%B9%E6%A1%88/ class=md-nav__link> <span class=md-ellipsis> C++ 异步方案演进 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/bustub%E9%80%9A%E5%85%B3%E6%8C%87%E5%8C%97/ class=md-nav__link> <span class=md-ellipsis> Bustub 通关指北 </span> </a> </li> <li class=md-nav__item> <a href=../../../blogs/posts/Implement%20of%20Concurrent/ class=md-nav__link> <span class=md-ellipsis> 并发组件实现浅析 </span> </a> </li> </ul> </nav> </li> <li class="md-nav__item md-nav__item--nested"> <input class="md-nav__toggle md-toggle " type=checkbox id=__nav_5> <div class="md-nav__link md-nav__container"> <a href=../../../summary/ class="md-nav__link "> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> <span class=md-ellipsis> Summaries </span> </a> <label class="md-nav__link " for=__nav_5 id=__nav_5_label tabindex=0> <span class="md-nav__icon md-icon"></span> </label> </div> <nav class=md-nav data-md-level=1 aria-labelledby=__nav_5_label aria-expanded=false> <label class=md-nav__title for=__nav_5> <span class="md-nav__icon md-icon"></span> Summaries </label> <ul class=md-nav__list data-md-scrollfix> <li class=md-nav__item> <a href=../../../summary/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%80%BB%E7%BB%93/%E7%A0%94%E4%B8%80%E4%B8%8A%E6%80%BB%E7%BB%93/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> <span class=md-ellipsis> 研一上总结 </span> </a> </li> <li class=md-nav__item> <a href=../../../summary/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%80%BB%E7%BB%93/%E7%A0%94%E4%B8%80%E4%B8%8B%E6%80%BB%E7%BB%93/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> <span class=md-ellipsis> 研一下总结 </span> </a> </li> <li class=md-nav__item> <a href=../../../summary/%E7%A0%94%E7%A9%B6%E7%94%9F%E6%80%BB%E7%BB%93/%E7%A0%94%E4%BA%8C%E4%B8%8A%E6%80%BB%E7%BB%93/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M6 1h2v2h8V1h2v2h1a2 2 0 0 1 2 2v14c0 1.11-.89 2-2 2H5a2 2 0 0 1-2-2V5c0-1.11.89-2 2-2h1zM5 8v11h14V8zm2 2h10v2H7z"></path></svg> <span class=md-ellipsis> 研二上总结 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=../../../tags/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M5.5 7A1.5 1.5 0 0 1 4 5.5 1.5 1.5 0 0 1 5.5 4 1.5 1.5 0 0 1 7 5.5 1.5 1.5 0 0 1 5.5 7m15.91 4.58-9-9C12.05 2.22 11.55 2 11 2H4c-1.11 0-2 .89-2 2v7c0 .55.22 1.05.59 1.41l8.99 9c.37.36.87.59 1.42.59s1.05-.23 1.41-.59l7-7c.37-.36.59-.86.59-1.41 0-.56-.23-1.06-.59-1.42"></path></svg> <span class=md-ellipsis> Tags </span> </a> </li> <li class=md-nav__item> <a href=../../../links/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M14.78 3.653a3.936 3.936 0 1 1 5.567 5.567l-3.627 3.627a3.936 3.936 0 0 1-5.88-.353.75.75 0 0 0-1.18.928 5.436 5.436 0 0 0 8.12.486l3.628-3.628a5.436 5.436 0 1 0-7.688-7.688l-3 3a.75.75 0 0 0 1.06 1.061z"></path><path d="M7.28 11.153a3.936 3.936 0 0 1 5.88.353.75.75 0 0 0 1.18-.928 5.436 5.436 0 0 0-8.12-.486L2.592 13.72a5.436 5.436 0 1 0 7.688 7.688l3-3a.75.75 0 1 0-1.06-1.06l-3 3a3.936 3.936 0 0 1-5.567-5.568z"></path></svg> <span class=md-ellipsis> Links </span> </a> </li> <li class=md-nav__item> <a href=../../../about/ class=md-nav__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M256 512a256 256 0 1 0 0-512 256 256 0 1 0 0 512m-90.6-190.1c20.4 28 53.4 46.1 90.6 46.1s70.2-18.1 90.6-46.1c7.8-10.7 22.8-13.1 33.5-5.3s13.1 22.8 5.3 33.5C356.3 390 309.2 416 256 416s-100.3-26-129.4-65.9c-7.8-10.7-5.4-25.7 5.3-33.5s25.7-5.4 33.5 5.3M144 208a32 32 0 1 1 64 0 32 32 0 1 1-64 0m164 8c0 11-9 20-20 20s-20-9-20-20c0-33.1 26.9-60 60-60h16c33.1 0 60 26.9 60 60 0 11-9 20-20 20s-20-9-20-20-9-20-20-20h-16c-11 0-20 9-20 20"></path></svg> <span class=md-ellipsis> About </span> </a> </li> </ul> </nav> </div> </div> </div> <div class="md-sidebar md-sidebar--secondary" data-md-component=sidebar data-md-type=toc> <div class=md-sidebar__scrollwrap> <div class=md-sidebar__inner> <nav class="md-nav md-nav--secondary" aria-label="Table of contents"> <label class=md-nav__title for=__toc> <span class="md-nav__icon md-icon"></span> Table of contents </label> <ul class=md-nav__list data-md-component=toc data-md-scrollfix> <li class=md-nav__item> <a href=#overview class=md-nav__link> <span class=md-ellipsis> Overview </span> </a> </li> <li class=md-nav__item> <a href=#gpu class=md-nav__link> <span class=md-ellipsis> GPU 硬件架构 </span> </a> <nav class=md-nav aria-label="GPU 硬件架构"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#streaming-multiprocessor-sm-based-on-h100-hopper-architecture class=md-nav__link> <span class=md-ellipsis> Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture) </span> </a> <nav class=md-nav aria-label="Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture)"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#functional-units-h100-specifics class=md-nav__link> <span class=md-ellipsis> Functional Units (H100 Specifics) </span> </a> </li> <li class=md-nav__item> <a href=#warp-contexts-occupancy class=md-nav__link> <span class=md-ellipsis> Warp Contexts &amp; Occupancy </span> </a> </li> <li class=md-nav__item> <a href=#warp-scheduler-h100 class=md-nav__link> <span class=md-ellipsis> Warp Scheduler (H100) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#gpu-memory-hierarchy-h100-enhanced class=md-nav__link> <span class=md-ellipsis> GPU Memory Hierarchy (H100 Enhanced) </span> </a> </li> <li class=md-nav__item> <a href=#warp-h100-scheduling-execution class=md-nav__link> <span class=md-ellipsis> Warp (H100 Scheduling &amp; Execution) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cuda-cuda2 class=md-nav__link> <span class=md-ellipsis> CUDA 计算模型[^cuda2] </span> </a> <nav class=md-nav aria-label="CUDA 计算模型[^cuda2]"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#_1 class=md-nav__link> <span class=md-ellipsis> 与硬件的对应 </span> </a> <nav class=md-nav aria-label=与硬件的对应> <ul class=md-nav__list> <li class=md-nav__item> <a href=#thread class=md-nav__link> <span class=md-ellipsis> Thread </span> </a> </li> <li class=md-nav__item> <a href=#thread-block class=md-nav__link> <span class=md-ellipsis> Thread Block </span> </a> </li> <li class=md-nav__item> <a href=#grid class=md-nav__link> <span class=md-ellipsis> Grid </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#kernel class=md-nav__link> <span class=md-ellipsis> Kernel 执行流程 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#roofline class=md-nav__link> <span class=md-ellipsis> Roofline 模型 </span> </a> </li> <li class=md-nav__item> <a href=#cuda-kernel class=md-nav__link> <span class=md-ellipsis> CUDA Kernel 性能调优 </span> </a> <nav class=md-nav aria-label="CUDA Kernel 性能调优"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#memory-bound class=md-nav__link> <span class=md-ellipsis> Memory-Bound </span> </a> <nav class=md-nav aria-label=Memory-Bound> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-instruction-issue-efficiency class=md-nav__link> <span class=md-ellipsis> 1. 指令层级：指令发射效率 (Instruction Issue Efficiency) </span> </a> </li> <li class=md-nav__item> <a href=#2-mlp-memory-level-parallelism class=md-nav__link> <span class=md-ellipsis> 2. 数据层级：内存级并行度 (MLP, Memory Level Parallelism) </span> </a> </li> <li class=md-nav__item> <a href=#3-transaction-utilization class=md-nav__link> <span class=md-ellipsis> 3. 硬件层级：传输粒度与利用率 (Transaction &amp; Utilization) </span> </a> </li> <li class=md-nav__item> <a href=#4-physical-conflict class=md-nav__link> <span class=md-ellipsis> 4. 架构层级：物理冲突 (Physical Conflict) </span> </a> </li> <li class=md-nav__item> <a href=#5-asynchronous-copy-cpasync class=md-nav__link> <span class=md-ellipsis> 5. Asynchronous Copy (cp.async) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#compute-bound class=md-nav__link> <span class=md-ellipsis> Compute-Bound </span> </a> <nav class=md-nav aria-label=Compute-Bound> <ul class=md-nav__list> <li class=md-nav__item> <a href=#1-tensor-cores-wmma-mma class=md-nav__link> <span class=md-ellipsis> 1. 使用 Tensor Cores (WMMA / MMA) </span> </a> </li> <li class=md-nav__item> <a href=#2-precision-reduction class=md-nav__link> <span class=md-ellipsis> 2. 降低精度 (Precision Reduction) </span> </a> </li> <li class=md-nav__item> <a href=#3-ilp-instruction-level-parallelism class=md-nav__link> <span class=md-ellipsis> 3. 指令级并行 (ILP, Instruction Level Parallelism) </span> </a> </li> <li class=md-nav__item> <a href=#4-dual-issue class=md-nav__link> <span class=md-ellipsis> 4. 双发射 (Dual Issue) / 并发执行 </span> </a> </li> <li class=md-nav__item> <a href=#5-fma-fused-multiply-add class=md-nav__link> <span class=md-ellipsis> 5. 使用 FMA (Fused Multiply-Add) </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#examples class=md-nav__link> <span class=md-ellipsis> Examples </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#cuda class=md-nav__link> <span class=md-ellipsis> CUDA 常用优化技巧 </span> </a> <nav class=md-nav aria-label="CUDA 常用优化技巧"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#maximize-compiler-computationgpu-miarch class=md-nav__link> <span class=md-ellipsis> Maximize Compiler Computation[^gpu-miarch] </span> </a> </li> <li class=md-nav__item> <a href=#coalescing-memory-access class=md-nav__link> <span class=md-ellipsis> Coalescing Memory Access </span> </a> </li> <li class=md-nav__item> <a href=#avoid-bank-conflicts-in-shared-memory class=md-nav__link> <span class=md-ellipsis> Avoid Bank Conflicts in Shared Memory </span> </a> <nav class=md-nav aria-label="Avoid Bank Conflicts in Shared Memory"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#solutionsbankconflict class=md-nav__link> <span class=md-ellipsis> Solutions[^bankconflict] </span> </a> </li> <li class=md-nav__item> <a href=#matmul-bank-conflict-avoidance-examplebankconflict2 class=md-nav__link> <span class=md-ellipsis> Matmul Bank Conflict Avoidance Example[^bankconflict2] </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#double-bufferingbankconflict2 class=md-nav__link> <span class=md-ellipsis> Double Buffering[^bankconflict2] </span> </a> </li> <li class=md-nav__item> <a href=#warp-shuffle-reduction class=md-nav__link> <span class=md-ellipsis> Warp Shuffle &amp; Reduction </span> </a> </li> <li class=md-nav__item> <a href=#tilematmul class=md-nav__link> <span class=md-ellipsis> Tile[^matmul] </span> </a> </li> <li class=md-nav__item> <a href=#wrap-tilematmul class=md-nav__link> <span class=md-ellipsis> Wrap Tile[^matmul] </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#transformer class=md-nav__link> <span class=md-ellipsis> Transformer 内部算子优化 </span> </a> <nav class=md-nav aria-label="Transformer 内部算子优化"> <ul class=md-nav__list> <li class=md-nav__item> <a href=#softmax-layernorm class=md-nav__link> <span class=md-ellipsis> Softmax &amp; LayerNorm 优化 </span> </a> </li> <li class=md-nav__item> <a href=#flash-attention class=md-nav__link> <span class=md-ellipsis> Flash Attention 优化 </span> </a> </li> </ul> </nav> </li> <li class=md-nav__item> <a href=#_2 class=md-nav__link> <span class=md-ellipsis> 参考资料 </span> </a> </li> </ul> </nav> </div> </div> </div> <div class=md-content data-md-component=content> <nav class=md-path aria-label=Navigation> <ol class=md-path__list> <li class=md-path__item> <a href=../../.. class=md-path__link> <span class=md-ellipsis> Home </span> </a> </li> <li class=md-path__item> <a href=../../../blogs/ class=md-path__link> <span class=md-ellipsis> Blogs </span> </a> </li> <li class=md-path__item> <a href=../%E4%BB%8EGEMM%E5%AE%9E%E8%B7%B5CUDA%E4%BC%98%E5%8C%96/ class=md-path__link> <span class=md-ellipsis> CUDA </span> </a> </li> </ol> </nav> <article class="md-content__inner md-typeset"> <div class="document-dates-plugin-wrapper document-dates-top"><div class=document-dates-plugin locale><div class=dd-left><span class=dd-item data-tippy-content data-tippy-raw=2025-12-02><span class=material-icons data-icon=doc_created></span><time datetime=2025-12-02T16:15:41+08:00>2025-12-02</time></span><span class=dd-item data-tippy-content data-tippy-raw=2025-12-18><span class=material-icons data-icon=doc_updated></span><time datetime=2025-12-18T14:11:12+00:00>2025-12-18</time></span></div><div class=dd-right><span class=material-icons data-icon=doc_author></span><div class=author-group><div class=avatar-wrapper data-name="Zhi Yiliu" data-tippy-content data-tippy-raw='<a href="mailto:2584074296@qq.com">Zhi Yiliu</a>'><span class=avatar-text></span><a class=glightbox data-type=image data-width=80% data-height=auto href="https://www.gravatar.com/avatar/c36589e7958d3f2a3c36a84bde756465?d=404&s=64" data-desc-position=bottom><img class=avatar src="https://www.gravatar.com/avatar/c36589e7958d3f2a3c36a84bde756465?d=404&s=64" onerror="this.style.display='none'"></a></div></div></div></div></div> <div class=md-typeset> <div class=blogging-tags-grid> <a href=https://tom-jerr.github.io/tags#CUDA class=blogging-tag><code>#CUDA</code></a> </div> </div> <style>
    .md-typeset .blogging-tags-grid {
        display: flex;
        flex-direction: row;
        flex-wrap: wrap;
        gap: 8px;
        margin-top: 5px;
    }

    .md-typeset .blogging-tag {
        color: var(--md-typeset-color);
        background-color: var(--md-typeset-code-color);
        white-space: nowrap;
        display: block;
    }

    .md-typeset .blogging-tag code {
        border-radius: 5px;
    }
</style> <h1 id=cuda-optimization-for-llm-inference>CUDA Optimization for LLM Inference<a class=headerlink href=#cuda-optimization-for-llm-inference title="Permanent link">¶</a></h1> <div style="margin-top: -30px; font-size: 0.75em; opacity: 0.7;"> <p><span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 2A10 10 0 0 0 2 12a10 10 0 0 0 10 10 10 10 0 0 0 10-10h-2a8 8 0 0 1-8 8 8 8 0 0 1-8-8 8 8 0 0 1 8-8zm6.78 1a.7.7 0 0 0-.48.2l-1.22 1.21 2.5 2.5L20.8 5.7c.26-.26.26-.7 0-.95L19.25 3.2c-.13-.13-.3-.2-.47-.2m-2.41 2.12L9 12.5V15h2.5l7.37-7.38z"></path></svg></span> 约 6647 个字 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M360.8 1.2c-17-4.9-34.7 5-39.6 22l-128 448c-4.9 17 5 34.7 22 39.6s34.7-5 39.6-22l128-448c4.9-17-5-34.7-22-39.6m64.6 136.1c-12.5 12.5-12.5 32.8 0 45.3l73.4 73.4-73.4 73.4c-12.5 12.5-12.5 32.8 0 45.3s32.8 12.5 45.3 0l96-96c12.5-12.5 12.5-32.8 0-45.3l-96-96c-12.5-12.5-32.8-12.5-45.3 0zm-274.7 0c-12.5-12.5-32.8-12.5-45.3 0l-96 96c-12.5 12.5-12.5 32.8 0 45.3l96 96c12.5 12.5 32.8 12.5 45.3 0s12.5-32.8 0-45.3L77.3 256l73.3-73.4c12.5-12.5 12.5-32.8 0-45.3z"></path></svg></span> 14 行代码 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M21 17H7V3h14m0-2H7a2 2 0 0 0-2 2v14a2 2 0 0 0 2 2h14a2 2 0 0 0 2-2V3a2 2 0 0 0-2-2M3 5H1v16a2 2 0 0 0 2 2h16v-2H3m12.96-10.71-2.75 3.54-1.96-2.36L8.5 15h11z"></path></svg></span> 15 张图片 <span class=twemoji><svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M12 20c4.42 0 8-3.58 8-8s-3.58-8-8-8-8 3.58-8 8 3.58 8 8 8m0-18c5.5 0 10 4.5 10 10s-4.5 10-10 10C6.47 22 2 17.5 2 12S6.5 2 12 2m.5 11H11V7h1.5v4.26l3.7-2.13.75 1.3z"></path></svg></span> 预计阅读时间 33 分钟</p> </div> <h2 id=overview>Overview<a class=headerlink href=#overview title="Permanent link">¶</a></h2> <p>在大规模语言模型（LLM）推理中，优化 CUDA 代码对于提升性能和效率至关重要。本文档介绍了一些关键的 CUDA 优化技术，帮助开发者更好地利用 GPU 资源进行 LLM 推理。这里我们将介绍 Transformer-based 自回归预训练模型推理用到的算子的优化方法。这些优化的方法也适用于其他算子。</p> <p>我们主要关注以下几个方面：</p> <ol> <li>GPU 硬件架构(Hirerarchy Memory, SM, Warp 等)</li> <li>CUDA 计算模型(Thread, Thread Block, Grid 等)</li> <li>CUDA Kernel 性能调优(Nsight Compute, Occupancy)</li> <li>CUDA 常用优化技巧(Double buffering, Memory Coalescing, Overcoming Bank Conflict 等)</li> <li>Transformer 内部算子优化<br> - Matrix Multiplication 优化<br> - Softmax 优化<br> - LayerNorm 优化<br> - Self-Attention 优化<br> - Flash Attention 优化</li> </ol> <h2 id=gpu>GPU 硬件架构<a class=headerlink href=#gpu title="Permanent link">¶</a></h2> <p>GPU 以 Throughput 为设计目标，和 CPU 有很大的不同。</p> <ul> <li>GPU 中虽有<strong>缓存结构但是数量</strong>少。 因为要减少指令访问缓存的次数。</li> <li>GPU 中<strong>控制单元非常简单</strong>。 控制单元中没有分支预测机制和数据转发机制，对于复杂的指令运算就会比较慢。</li> <li>GPU 的<strong>运算单元 (Core) 非常多</strong>，采用长延时流水线以实现高吞吐量。 每一行的运算单元的控制器只有一个，意味着每一行的运算单元使用的指令是相同的，不同的是它们的数据内容。那么这种整齐划一的运算方式使得 GPU 对于那些控制简单但运算高效的指令的效率显著增加。[^cuda1]</li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/gpu_hardware.jpg data-desc-position=bottom><img alt src=../img/gpu_hardware.jpg></a></p> <h3 id=streaming-multiprocessor-sm-based-on-h100-hopper-architecture>Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture)<a class=headerlink href=#streaming-multiprocessor-sm-based-on-h100-hopper-architecture title="Permanent link">¶</a></h3> <p>NVIDIA H100 GPU 基于 Hopper 架构，其 SM 设计为了极致的 AI 和 HPC 性能进行了重大升级。一个 H100 SM 包含以下核心组件：</p> <ul> <li> <p><strong>4 个处理块 (SMSP - SM Sub-Partitions):</strong></p> </li> <li> <p><strong>架构设计:</strong> 为了提高指令流水线的并行度和资源利用率，每个 SM 被物理划分为 4 个独立的处理块（SMSP）。</p> </li> <li><strong>资源隔离:</strong> 每个 SMSP 拥有自己独立的指令调度器、寄存器文件和计算单元。这意味着它们可以独立地调度和执行 Warp，互不干扰。</li> <li> <p><strong>组件详情:</strong></p> <ul> <li><strong>Warp Scheduler &amp; Dispatch Unit:</strong> 每个 SMSP 有一个 Warp 调度器。它负责从分配给该 SMSP 的 Warp 池中选择一个 Ready 的 Warp，并将其下一条指令分发到执行单元。这是 GPU <strong>延迟掩盖机制的核心</strong>。</li> <li><strong>Register File:</strong> 每个 SMSP 配备 64K 个 32-bit 寄存器（整个 SM 共 256K）。寄存器是 GPU 上速度最快的存储，用于保存线程的私有变量。巨大的寄存器文件支持了高并发线程数（Occupancy）。</li> <li><strong>L0 Instruction Cache:</strong> 缓存指令，减少取指延迟。</li> <li><strong>Math Units (计算单元):</strong></li> <li><strong>32 FP32 Cores (CUDA Cores):</strong> 负责标准的单精度浮点运算。</li> <li><strong>16 INT32 Cores:</strong> 负责整数运算，通常用于地址计算、循环控制等。Hopper 架构允许 FP32 和 INT32 指令并发执行，提高了流水线效率。</li> <li><strong>16 FP64 Cores:</strong> 双精度浮点单元。H100 的 FP64 性能是 A100 的 3 倍，这对科学计算（HPC）至关重要。</li> <li><strong>1 4<sup>th</sup> Gen Tensor Core:</strong> 这是 AI 推理和训练的动力源。<ul> <li><strong>多精度支持:</strong> 支持 FP8, FP16, BF16, TF32, FP64。FP8 是 Hopper 的一大亮点，吞吐量是 FP16 的两倍。</li> <li><strong>稀疏性 (Sparsity):</strong> 支持结构化稀疏（Structured Sparsity），可进一步提升 2 倍性能。</li> <li><strong>异步执行:</strong> 与 CUDA Cores 异步执行，极大提升了矩阵运算效率。</li> </ul> </li> </ul> </li> <li> <p><strong>Shared Memory / L1 Data Cache:</strong></p> </li> <li> <p><strong>统一架构:</strong> H100 SM 拥有 <strong>256 KB</strong> 的统一 L1/Shared Memory（相比 A100 的 192 KB 提升了 33%）。这块内存可以根据需要被配置为 L1 缓存或 Shared Memory。</p> </li> <li><strong>高带宽:</strong> 它是片上内存，带宽极高，延迟极低，是线程块内数据交换的桥梁。</li> <li> <p><strong>Distributed Shared Memory (DSMEM):</strong> 这是 Hopper 引入的革命性特性。</p> <ul> <li><strong>Thread Block Cluster:</strong> 允许将多个 Thread Block 组成一个 Cluster（例如 8 个 Block）。</li> <li><strong>直接访问:</strong> Cluster 内的所有线程可以直接访问其他 SM 上的 Shared Memory，就像访问本地 Shared Memory 一样（虽然延迟稍高一点点）。这打破了传统 CUDA 编程中 "Shared Memory 仅限 Block 内共享" 的限制，使得跨 Block 的数据复用和通信成为可能。</li> </ul> </li> <li> <p><strong>Tensor Memory Accelerator (TMA):</strong></p> </li> <li> <p><strong>定义:</strong> 一个专用的硬件直接内存访问 (DMA) 引擎。</p> </li> <li><strong>功能:</strong> 负责在 Global Memory 和 Shared Memory 之间高效、异步地搬运数据。</li> <li><strong>优势:</strong><ul> <li><strong>解放 SM:</strong> 传统的数据加载需要 CUDA Core 执行 Load 指令，占用寄存器和 ALU 资源。TMA 接管了这项工作，让 SM 专注于计算。</li> <li><strong>降低寄存器压力:</strong> 数据直接从 Global Memory 写入 Shared Memory，不经过寄存器文件。</li> <li><strong>简化编程:</strong> 只需要配置好 Copy Descriptor，TMA 就会自动处理复杂的地址计算（如矩阵块的 stride 访问）和越界检查。</li> <li><strong>支持 mbarrier:</strong> 与异步屏障 (mbarrier) 结合，实现高效的流水线并行。</li> </ul> </li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/h100sm.png data-desc-position=bottom><img alt src=../img/h100sm.png></a></p> <h4 id=functional-units-h100-specifics>Functional Units (H100 Specifics)<a class=headerlink href=#functional-units-h100-specifics title="Permanent link">¶</a></h4> <p>H100 的功能单元设计旨在最大化并发性和吞吐量：</p> <ul> <li><strong>并发流水线 (Concurrent Pipelines):</strong></li> <li>H100 的 SM 包含独立的 FP32, INT32 和 FP64 数据路径。</li> <li><strong>FP32 &amp; INT32 并发:</strong> 允许在同一个周期内同时发射 FP32 指令和 INT32 指令。这对于循环索引更新（INT32）和核心计算（FP32）并行执行非常有效。</li> <li><strong>FP64 性能:</strong> H100 的 FP64 吞吐量是 FP32 的 ½，相比 A100 大幅提升，适合科学计算。</li> <li><strong>Tensor Core (4<sup>th</sup> Gen):</strong></li> <li>独立于 CUDA Cores 的专用流水线。</li> <li>支持 <strong>异步执行</strong>，即 Tensor Core 在进行矩阵运算时，CUDA Cores 可以同时处理其他无关指令（如地址计算、逻辑判断）。</li> </ul> <h4 id=warp-contexts-occupancy>Warp Contexts &amp; Occupancy<a class=headerlink href=#warp-contexts-occupancy title="Permanent link">¶</a></h4> <ul> <li><strong>Warp Contexts:</strong> 每个 SM 维护着大量 Warp 的上下文（PC, 寄存器状态等）。H100 上每个 SM 最多支持 <strong>64 个 Warp</strong>（即 2048 个线程）。</li> <li><strong>Occupancy (占用率):</strong></li> <li>指当前 SM 上实际激活的 Warp 数量与最大支持数量的比值。</li> <li><strong>寄存器压力:</strong> H100 每个 SM 有 64K (32-bit) 寄存器。如果每个线程使用的寄存器过多，SM 就无法同时容纳足够多的 Warp，导致 Occupancy 下降，从而无法有效隐藏延迟。</li> <li><strong>Shared Memory 限制:</strong> 同样，如果 Block 消耗过多的 Shared Memory，也会限制 SM 上能并发运行的 Block 数量。</li> </ul> <h4 id=warp-scheduler-h100>Warp Scheduler (H100)<a class=headerlink href=#warp-scheduler-h100 title="Permanent link">¶</a></h4> <p>H100 的 Warp 调度器更加智能和高效：</p> <ul> <li><strong>独立调度器:</strong> 每个 SMSP (SM Sub-Partition) 都有自己的 Warp Scheduler。这意味着一个 SM 每个周期可以同时从 4 个 SMSP 中各发射指令，实现 4 路并发。</li> <li><strong>Latency Hiding:</strong></li> <li>调度器的核心任务是<strong>零开销上下文切换</strong>。</li> <li>当当前 Warp 遇到长延迟操作（如 Global Memory 读取或 Tensor Core 计算）而阻塞时，调度器会在<strong>下一个时钟周期</strong>立即切换到另一个“准备就绪”的 Warp 执行。</li> <li><strong>H100 优势:</strong> 配合 TMA (Tensor Memory Accelerator) 的异步拷贝，SM 可以挂起等待数据的 Warp，转而执行计算密集的 Warp，实现计算与访存的完美重叠。</li> <li><strong>Warp Divergence:</strong></li> <li>依然遵循 SIMT (Single Instruction, Multiple Threads) 模型。</li> <li>如果 Warp 内线程进入不同的 <code>if-else</code> 分支，硬件会串行执行不同路径，并使用 <strong>Active Mask</strong> 屏蔽不应执行的线程。</li> <li>尽量避免 Warp 内的分支分化，或者使用 <code>__shfl_sync</code> 等指令在 Warp 内交换数据以统一控制流。</li> </ul> <h3 id=gpu-memory-hierarchy-h100-enhanced>GPU Memory Hierarchy (H100 Enhanced)<a class=headerlink href=#gpu-memory-hierarchy-h100-enhanced title="Permanent link">¶</a></h3> <p>H100 的内存体系结构为了喂饱强大的 Tensor Cores 进行了全面升级，核心在于<strong>更高的带宽</strong>和<strong>更灵活的数据共享</strong>。</p> <ul> <li> <p><strong>Registers (寄存器):</strong></p> </li> <li> <p><strong>速度:</strong> 最快，单周期访问。</p> </li> <li><strong>容量:</strong> 每个 SM 拥有 256KB 的寄存器文件。</li> <li> <p><strong>优化:</strong> 编译器会尽可能将频繁访问的变量放入寄存器。如果寄存器溢出 (Spill)，数据会被放入 Local Memory (实际上是 Global Memory)，导致性能急剧下降。</p> </li> <li> <p><strong>L1 Data Cache / Shared Memory (Unified):</strong></p> </li> <li> <p><strong>统一架构:</strong> 每个 SM 拥有 <strong>256 KB</strong> 的高速片上内存。</p> </li> <li><strong>可配置:</strong> 可以根据 Kernel 需求灵活划分为 L1 Cache 和 Shared Memory</li> <li> <p><strong>DSMEM (Distributed Shared Memory):</strong> 允许一个 Thread Block Cluster 内的 SM 直接通过 SM-to-SM 网络访问彼此的 Shared Memory。这使得 Shared Memory 的有效容量在逻辑上扩大了数倍。</p> </li> <li> <p><strong>L2 Cache:</strong></p> </li> <li> <p><strong>容量:</strong> H100 拥有巨大的 <strong>50 MB</strong> L2 Cache。</p> </li> <li> <p><strong>作用:</strong> 所有 SM 共享。它是连接高速 SM 和低速 HBM 的关键缓冲。由于原子操作 (Atomics) 通常在 L2 上执行，更大的 L2 意味着更少的 DRAM 访问冲突。</p> </li> <li> <p><strong>HBM3 (Global Memory):</strong></p> </li> <li> <p><strong>带宽:</strong> H100 搭载 HBM3 显存，带宽高达 <strong>3.35 TB/s</strong>。</p> </li> <li> <p><strong>TMA:</strong> 配合 Tensor Memory Accelerator，数据可以“流”进 Shared Memory，而不需要 CUDA Cores 逐个指令搬运。</p> </li> <li> <p><strong>Constant / Texture Memory:</strong> 依然存在，用于只读数据的缓存优化，但在现代计算密集型 Kernel 中，其角色逐渐被 L1 Cache 和 Shared Memory 覆盖。</p> </li> </ul> <p><img alt=😆 class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f606.svg title=:laughing:><strong>Summary (H100 Perspective)</strong></p> <table> <thead> <tr> <th style="text-align: left;">层级</th> <th style="text-align: left;">类型</th> <th style="text-align: left;">H100 特性</th> <th style="text-align: left;">延迟 (Cycles)</th> </tr> </thead> <tbody> <tr> <td style="text-align: left;"><strong>Registers</strong></td> <td style="text-align: left;">私有</td> <td style="text-align: left;">256KB/SM, 极速</td> <td style="text-align: left;">~0</td> </tr> <tr> <td style="text-align: left;"><strong>Shared Memory</strong></td> <td style="text-align: left;">Block/Cluster 共享</td> <td style="text-align: left;"><strong>DSMEM (跨 SM 访问)</strong>, TMA 异步加载</td> <td style="text-align: left;">~20-30 (Local), ~50 (Remote)</td> </tr> <tr> <td style="text-align: left;"><strong>L1 Cache</strong></td> <td style="text-align: left;">SM 私有</td> <td style="text-align: left;">与 Shared Memory 统一配置</td> <td style="text-align: left;">~30</td> </tr> <tr> <td style="text-align: left;"><strong>L2 Cache</strong></td> <td style="text-align: left;">全局共享</td> <td style="text-align: left;"><strong>50MB 超大容量</strong>, 原子操作中心</td> <td style="text-align: left;">~200</td> </tr> <tr> <td style="text-align: left;"><strong>HBM3 (Global)</strong></td> <td style="text-align: left;">全局共享</td> <td style="text-align: left;"><strong>3TB/s+ 带宽</strong>, TMA 直通</td> <td style="text-align: left;">~400-600+</td> </tr> </tbody> </table> <h3 id=warp-h100-scheduling-execution>Warp (H100 Scheduling &amp; Execution)<a class=headerlink href=#warp-h100-scheduling-execution title="Permanent link">¶</a></h3> <p>虽然程序员编写的是单个线程的代码，但在硬件层面，H100 依然将线程组织成固定大小（32 个）的束，称为 <strong>Warp</strong>。Warp 是 SM 上调度和执行的真正基本单位。</p> <ul> <li> <p><strong>SIMT 与 独立线程调度:</strong></p> </li> <li> <p>H100 延续了 Volta 引入的独立线程调度机制。每个线程都有自己的程序计数器 (PC) 和调用栈。</p> </li> <li>尽管如此，为了性能，Warp 内的 32 个线程通常依然是同步执行相同指令的 (SIMT)。</li> <li> <p><strong>Warp Divergence:</strong> 如果 Warp 内线程进入不同的分支，硬件会串行执行不同路径。虽然独立调度避免了死锁风险，但<strong>性能惩罚依然存在</strong>，因为硬件单元在某一时刻只能服务于一种指令路径。</p> </li> <li> <p><strong>Dispatch Speed:</strong></p> </li> <li> <p>H100 的每个 SM 划分为 4 个 SMSP，每个 SMSP 都有独立的 Warp Scheduler。</p> </li> <li> <p>这意味着一个 SM 在每个时钟周期可以同时从 4 个不同的 Warp 中发射指令，极大地提高了指令吞吐量。</p> </li> <li> <p><strong>延迟隐藏:</strong></p> </li> <li> <p>传统的延迟隐藏依赖于“上下文切换”：当 Warp A 等待内存时，切换到 Warp B。</p> </li> <li> <p><strong>H100 版本:</strong> 结合 <strong>TMA</strong> 和 <strong>异步拷贝 (<code>cp.async</code>)</strong>，Warp 不再需要为了搬运数据而“停顿”在 Load 指令上。Warp 可以发出异步拷贝命令，然后继续执行其他计算任务（如 GEMM），直到真正需要数据时才在 <code>mbarrier</code> 上等待。这使得计算和访存的重叠 (Overlap) 达到了新的高度。</p> </li> <li> <p><strong>寄存器压力与 Occupancy:</strong></p> </li> <li> <p>H100 每个 SM 有 64K 寄存器。</p> </li> <li>为了维持足够的 Active Warps 来隐藏延迟，必须严格控制每个 Kernel 使用的寄存器数量。如果每个线程使用过多寄存器，SM 上能同时驻留的 Warp 就会减少，导致调度器“无 Warp 可切”，流水线出现气泡。</li> </ul> <h2 id=cuda-cuda2>CUDA 计算模型[^cuda2]<a class=headerlink href=#cuda-cuda2 title="Permanent link">¶</a></h2> <p>CUDA 将计算任务组织成一个三级层次结构 。这是一个由程序员创建的、用于组织问题的<strong>逻辑层次</strong>，而非硬件的直接体现 。</p> <ul> <li><strong>线程 (Thread)</strong>：最基本的执行单位。单个线程执行一个 Kernel 函数的实例 。每个线程在其所属的线程块内拥有一个唯一的 ID(threadIdx)</li> <li><strong>线程块(Thread Block)</strong>：一组可以相互协作的线程（在现代架构上最多 1024 个）。一个块内的所有线程可以通过高速的片上 &nbsp; 共享内存共享数据，并能通过<code>__syncthreads()</code>来协调它们的执行 。每个线程块在其所属的 Grid 内也拥有一个唯一的 ID(blockIdx)</li> <li><strong>网格 (Grid)</strong>：为执行单个 Kernel 而启动的所有线程块的集合 。一个 Grid 内的所有线程都可以访问同一个全局内存空间。Grid 内的线程块被假定为独立执行，且执行顺序不确定；它们之间没有直接的同步机制。</li> </ul> <p>线程块和网格可以被组织成一维、二维或三维的结构，这为将计算任务映射到向量、矩阵、体数据等数据结构上提供了极大的便利 。</p> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/grid.jpg data-desc-position=bottom><img alt src=../img/grid.jpg></a></p> <h3 id=_1>与硬件的对应<a class=headerlink href=#_1 title="Permanent link">¶</a></h3> <h4 id=thread>Thread<a class=headerlink href=#thread title="Permanent link">¶</a></h4> <ul> <li> <p>在某一时刻，一个 CUDA core 执行一个线程的指令。</p> </li> <li> <p><img alt=⚠ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/26a0.svg title=:warning:> 实际上同一个 CUDA core 会在多个 thread 之间分时复用，因为线程数远多于核心数</p> </li> </ul> <h4 id=thread-block>Thread Block<a class=headerlink href=#thread-block title="Permanent link">¶</a></h4> <ul> <li>一个 block 被分配到一个 SM (Streaming Multiprocessor) 上执行。</li> <li>一个 block 中的线程会被分成若干 warp。SM 内有多个资源（寄存器文件、共享内存），这些资源在不同 block 之间分配。</li> </ul> <h4 id=grid>Grid<a class=headerlink href=#grid title="Permanent link">¶</a></h4> <ul> <li>一个 grid 对应一次 kernel 启动（Launch）。</li> <li>Grid 中的 block 会被分配到不同的 SM 执行。</li> <li>GPU 上的 GPC（Graphics Processing Cluster） 或 TPC（Texture Processing Cluster） 层级会参与全局调度。</li> </ul> <h3 id=kernel>Kernel 执行流程<a class=headerlink href=#kernel title="Permanent link">¶</a></h3> <ul> <li>当一个 Kernel 被启动时，由硬件调度器将 Grid 中的所有线程块分配到 GPU 上可用的 SM 中</li> <li>一个 Thread Block 会被完整地分配给一个 SM，并在其上完整地执行。在其生命周期内，它不会被迁移到其他 SM</li> <li>一个 SM 可以并发地执行多个 Thread Block，前提是它拥有足够的资源（如寄存器、共享内存）来容纳这些线程块</li> <li>在 SM 内部，一个 Thread Block 的所有线程被划分为若干个 Warp（每组 32 个线程）。这些 Warp 才是被 SM 的 Warp 调度器实际调度执行的单元 。</li> </ul> <p>这个映射关系是层级化的：<code>Grid -&gt; GPU</code>，<code>Block -&gt; SM</code>，<code>Thread -&gt; Warp -&gt; CUDA核心</code></p> <h2 id=roofline>Roofline 模型<a class=headerlink href=#roofline title="Permanent link">¶</a></h2> <p>Roofline 模型（屋顶线模型）是一种用来<strong>分析程序性能瓶颈</strong>（计算受限还是带宽受限）的方法。\<br> 它把<strong>计算性能</strong>（FLOPs/s）和<strong>访存性能</strong>（Bytes/s）联系在一起，。以可视化的方式展示性能上限</p> <div class=arithmatex>\[ Achievable&nbsp;\\space FLOPs=min(AI×Memory\\space&nbsp;BW,Peak&nbsp;FLOPs) \]</div> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/roofline.png data-desc-position=bottom><img alt src=../img/roofline.png></a></p> <h2 id=cuda-kernel>CUDA Kernel 性能调优<a class=headerlink href=#cuda-kernel title="Permanent link">¶</a></h2> <p>根据 Roofline 模型，算子分为 Compute-Bound 和 Memory-Bound 型，所以我们需要分开讨论，但实际上在优化过程中，这两种瓶颈会交替出现😭</p> <h3 id=memory-bound>Memory-Bound<a class=headerlink href=#memory-bound title="Permanent link">¶</a></h3> <h4 id=1-instruction-issue-efficiency>1. 指令层级：指令发射效率 (Instruction Issue Efficiency)<a class=headerlink href=#1-instruction-issue-efficiency title="Permanent link">¶</a></h4> <ul> <li> <p><strong>LSU (Load Store Unit) 压力：</strong></p> </li> <li> <p><strong>Scalar (<code>float</code>):</strong> 搬运同样多的数据，需要发射 <strong>4 倍</strong> 的指令数。这会大量占用带宽，并增加 LSU 维护 In-flight 状态的开销。</p> </li> <li> <p><strong>Vectorized (<code>float4</code>):</strong> <strong>单指令高吞吐</strong>。一条指令即可搬运 128-bit 数据。LSU 队列占用少，更容易维持流水线饱和。</p> </li> <li> <p><strong>指令发射延迟掩盖 (Issue Latency Hiding)：</strong></p> </li> <li> <p><strong>气泡问题：</strong> 指令发射有固有延迟。如果每条指令搬运的数据量太小（如 4 Bytes），指令发射的速度可能跟不上内存总线的消耗速度，导致总线出现“空闲气泡”。</p> </li> </ul> <h4 id=2-mlp-memory-level-parallelism>2. 数据层级：内存级并行度 (MLP, Memory Level Parallelism)<a class=headerlink href=#2-mlp-memory-level-parallelism title="Permanent link">¶</a></h4> <p>这是决定带宽上限的关键软件策略。</p> <ul> <li> <p><strong>原理：</strong> HBM 延迟极高 (~600 cycles)。为了掩盖延迟，必须让总线上同时飞着足够多的请求 (In-flight Requests)。</p> </li> <li> <p><strong>优化手段：</strong> <strong>循环展开 (Loop Unrolling)</strong>。</p> </li> <li> <p><em>Bad:</em> <code>Load -&gt; Use -&gt; Load -&gt; Use</code> (串行依赖，延迟无法掩盖)。</p> </li> <li> <p><em>Good:</em> <code>Load1 -&gt; Load2 -&gt; Load3 -&gt; Load4 ... -&gt; Use1</code> (并行发射，一次等待，全部返回)。</p> </li> </ul> <h4 id=3-transaction-utilization>3. 硬件层级：传输粒度与利用率 (Transaction &amp; Utilization)<a class=headerlink href=#3-transaction-utilization title="Permanent link">¶</a></h4> <p>即使软件写得好，硬件机制也可能导致浪费。</p> <ul> <li> <p><strong>扇区利用率 (Sector Utilization):</strong></p> </li> <li> <p><strong>机制：</strong> DRAM 到 L2 的最小传输粒度是 <strong>32 Bytes (Sector)</strong>。</p> </li> <li> <p><strong>浪费：</strong> 如果只读 1 个 Byte (<code>char</code>) 且未打包，硬件也被迫搬运 32 Bytes。<strong>有效带宽 (Effective Bandwidth)</strong> 只有 1/32。</p> </li> <li> <p><strong>对策：</strong> 对于小数据类型（INT8/FP16），必须使用打包对齐（Pack Alignment）访问。</p> </li> <li> <p><strong>地址对齐 (Address Alignment):</strong></p> </li> <li> <p><strong>机制：</strong> 硬件要求访问地址按 32B 或 128B 对齐。</p> </li> <li> <p><strong>后果：</strong> 如果指针地址偏移（Misaligned），一次 128 Bytes 的读取可能会跨越两个 128B 块，导致硬件必须发起 <strong>2 个 Transactions</strong>。这会直接导致带宽性能减半。</p> </li> </ul> <h4 id=4-physical-conflict>4. 架构层级：物理冲突 (Physical Conflict)<a class=headerlink href=#4-physical-conflict title="Permanent link">¶</a></h4> <p>通常由硬件解决，但在极端优化时需注意。</p> <ul> <li> <p><strong>分区冲突 (Partition Camping / Channel Conflict):</strong></p> </li> <li> <p><strong>原理：</strong> 显存被划分为多个物理分区（Memory Controllers）。</p> </li> <li> <p><strong>现象：</strong> 特定的访问步长（Stride，通常是 2 的幂次）可能导致所有请求集中打向同一个 Controller，造成局部拥堵（Serialization），而其他 Controller 空闲。</p> </li> <li> <p><strong>现状：</strong> 现代 GPU (Pascal+) 已通过物理地址哈希（Address Swizzling）极大缓解了此问题，但在写极限 Kernel 时仍需避免完美的 2 的幂次跨度。</p> </li> </ul> <h4 id=5-asynchronous-copy-cpasync>5. Asynchronous Copy (cp.async)<a class=headerlink href=#5-asynchronous-copy-cpasync title="Permanent link">¶</a></h4> <p>在 Ampere (A100) 和 Hopper (H100) 架构上，为了极致的 Double/Multi-stage Buffering，我们使用异步拷贝指令。</p> <ul> <li><strong>机制：</strong> <code>cp.async</code> (PTX: <code>cp.async.ca.shared.global</code>) 允许数据直接从 Global Memory 传输到 Shared Memory。</li> <li><strong>优势：</strong></li> <li><strong>Bypass Register File:</strong> 数据不经过寄存器，直接进入 Shared Memory，极大降低寄存器压力 (Register Pressure)。</li> <li><strong>Non-blocking:</strong> 发出指令后，线程可以继续执行其他计算（如计算上一块数据的 GEMM），实现计算与访存的完美流水线掩盖。</li> <li><strong>流水线 (Pipeline):</strong></li> <li><code>cp.async.commit_group()</code>: 提交一组拷贝任务。</li> <li><code>cp.async.wait_group&lt;N&gt;()</code>: 等待直到只剩下 N 组未完成。</li> </ul> <h3 id=compute-bound>Compute-Bound<a class=headerlink href=#compute-bound title="Permanent link">¶</a></h3> <h4 id=1-tensor-cores-wmma-mma>1. 使用 Tensor Cores (WMMA / MMA)<a class=headerlink href=#1-tensor-cores-wmma-mma title="Permanent link">¶</a></h4> <p>这是现代 LLM 推理加速的<strong>绝对核心</strong>。在 LLM 推理中，99% 的 GEMM 都是 FP16/INT8/BF16，必须运行在 Tensor Cores 上才能获得 8-16 倍的加速。</p> <ul> <li> <p><strong>原理：</strong></p> </li> <li> <p><strong>CUDA Core (FP32):</strong> SIMT 模型，每个 Thread 处理一个标量计算。每个时钟周期执行 1 次 <code>FMA (a*b+c)</code>。</p> </li> <li><strong>Tensor Core (FP16/INT8):</strong> Warp-level 模型，一个 Warp (32 threads) 协同处理一个矩阵块（如 16x16x16）。每个时钟周期执行 1 次完整的矩阵乘加。</li> <li> <p><strong>差距：</strong> A100 上，Tensor Core 的吞吐量是 CUDA Core 的 <strong>16 倍</strong>以上。</p> </li> <li> <p><strong>关键概念：Fragment</strong></p> </li> <li> <p>数据不能随意放在寄存器里，必须按照 Tensor Core 要求的布局（Layout）存放。</p> </li> <li> <p><code>nvcuda::wmma::fragment</code>: 这是一个模板类，代表矩阵的一个块（Tile）。</p> </li> <li> <p><strong>API 层次：</strong></p> </li> </ul> <ol> <li><strong>WMMA API (<code>nvcuda::wmma</code>)</strong>: C++ 模板封装，易用性好。<br> <div class="language-cpp highlight"><span class=filename>C++</span><pre><span></span><code><span id=__span-0-1><a id=__codelineno-0-1 name=__codelineno-0-1></a><a href=#__codelineno-0-1><span class=linenos data-linenos="1 "></span></a><span class=k>using</span><span class=w> </span><span class=k>namespace</span><span class=w> </span><span class=nn>nvcuda</span><span class=p>;</span>
</span><span id=__span-0-2><a id=__codelineno-0-2 name=__codelineno-0-2></a><a href=#__codelineno-0-2><span class=linenos data-linenos="2 "></span></a><span class=n>wmma</span><span class=o>::</span><span class=n>fragment</span><span class=o>&lt;</span><span class=n>wmma</span><span class=o>::</span><span class=n>matrix_a</span><span class=p>,</span><span class=w> </span><span class=mi>16</span><span class=p>,</span><span class=w> </span><span class=mi>16</span><span class=p>,</span><span class=w> </span><span class=mi>16</span><span class=p>,</span><span class=w> </span><span class=n>half</span><span class=p>,</span><span class=w> </span><span class=n>wmma</span><span class=o>::</span><span class=n>row_major</span><span class=o>&gt;</span><span class=w> </span><span class=n>a_frag</span><span class=p>;</span>
</span><span id=__span-0-3><a id=__codelineno-0-3 name=__codelineno-0-3></a><a href=#__codelineno-0-3><span class=linenos data-linenos="3 "></span></a><span class=n>wmma</span><span class=o>::</span><span class=n>load_matrix_sync</span><span class=p>(</span><span class=n>a_frag</span><span class=p>,</span><span class=w> </span><span class=n>a_ptr</span><span class=p>,</span><span class=w> </span><span class=n>lda</span><span class=p>);</span>
</span><span id=__span-0-4><a id=__codelineno-0-4 name=__codelineno-0-4></a><a href=#__codelineno-0-4><span class=linenos data-linenos="4 "></span></a><span class=n>wmma</span><span class=o>::</span><span class=n>mma_sync</span><span class=p>(</span><span class=n>c_frag</span><span class=p>,</span><span class=w> </span><span class=n>a_frag</span><span class=p>,</span><span class=w> </span><span class=n>b_frag</span><span class=p>,</span><span class=w> </span><span class=n>c_frag</span><span class=p>);</span>
</span><span id=__span-0-5><a id=__codelineno-0-5 name=__codelineno-0-5></a><a href=#__codelineno-0-5><span class=linenos data-linenos="5 "></span></a><span class=n>wmma</span><span class=o>::</span><span class=n>store_matrix_sync</span><span class=p>(</span><span class=n>c_ptr</span><span class=p>,</span><span class=w> </span><span class=n>c_frag</span><span class=p>,</span><span class=w> </span><span class=n>ldc</span><span class=p>,</span><span class=w> </span><span class=n>wmma</span><span class=o>::</span><span class=n>mem_row_major</span><span class=p>);</span>
</span></code></pre></div></li> <li><strong>PTX MMA (<code>mma.sync</code>)</strong>: 更底层，控制力更强，Cutlass 常用。</li> </ol> <ul> <li> <p><strong>优化手段：</strong></p> </li> <li> <p>抛弃 <code>c = a * b + c</code> 的标量写法。</p> </li> <li>使用 <code>nvcuda::wmma</code> 或 Cutlass 库。</li> <li><strong>数据布局 (Layout):</strong> 必须处理好 Shared Memory 到 Register 的加载，避免 Bank Conflict。</li> </ul> <h4 id=2-precision-reduction>2. 降低精度 (Precision Reduction)<a class=headerlink href=#2-precision-reduction title="Permanent link">¶</a></h4> <p>计算吞吐量与数据类型的位宽成反比</p> <ul> <li><strong>FP32:</strong> 基准 (1x)</li> <li><strong>TF32 (Ampere+):</strong> Tensor Core 专用，19 bits，吞吐量通常是 FP32 的 high-speed 模式。</li> <li><strong>FP16 / BF16:</strong> 吞吐量是 FP32 的 <strong>2x</strong> (CUDA Core) 或更高 (Tensor Core)。</li> <li><strong>INT8:</strong> 吞吐量是 FP16 的 <strong>2x</strong>。</li> <li><strong>优化手段：</strong> 在 LLM 推理中，权重（Weight）和激活（Activation）通常量化为 <strong>FP16</strong> 或 <strong>INT8/W8A16</strong>。</li> </ul> <h4 id=3-ilp-instruction-level-parallelism>3. 指令级并行 (ILP, Instruction Level Parallelism)<a class=headerlink href=#3-ilp-instruction-level-parallelism title="Permanent link">¶</a></h4> <p><strong>循环展开 (Loop Unrolling)</strong>: 让每个线程同时维持多个“In-flight”的计算指令<br> </p><div class="language-cpp highlight"><span class=filename>C++</span><pre><span></span><code><span id=__span-1-1><a id=__codelineno-1-1 name=__codelineno-1-1></a><a href=#__codelineno-1-1><span class=linenos data-linenos="1 "></span></a><span class=c1>// 展开后，编译器会交错发射指令</span>
</span><span id=__span-1-2><a id=__codelineno-1-2 name=__codelineno-1-2></a><a href=#__codelineno-1-2><span class=linenos data-linenos="2 "></span></a><span class=n>a1</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>b1</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=n>c1</span><span class=p>;</span><span class=w> </span><span class=c1>// 发射</span>
</span><span id=__span-1-3><a id=__codelineno-1-3 name=__codelineno-1-3></a><a href=#__codelineno-1-3><span class=linenos data-linenos="3 "></span></a><span class=n>a2</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>b2</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=n>c2</span><span class=p>;</span><span class=w> </span><span class=c1>// 发射，不需要等 a1</span>
</span><span id=__span-1-4><a id=__codelineno-1-4 name=__codelineno-1-4></a><a href=#__codelineno-1-4><span class=linenos data-linenos="4 "></span></a><span class=n>a3</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>b3</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=n>c3</span><span class=p>;</span><span class=w> </span><span class=c1>// 发射</span>
</span><span id=__span-1-5><a id=__codelineno-1-5 name=__codelineno-1-5></a><a href=#__codelineno-1-5><span class=linenos data-linenos="5 "></span></a><span class=c1>// ... 此时 ALU 流水线被填满</span>
</span></code></pre></div><p></p> <h4 id=4-dual-issue>4. 双发射 (Dual Issue) / 并发执行<a class=headerlink href=#4-dual-issue title="Permanent link">¶</a></h4> <ul> <li> <p><strong>原理：</strong> 现代 SM（Ampere/Ada/Hopper）通常包含：</p> </li> <li> <p>FP32 计算单元</p> </li> <li>INT32 计算单元</li> <li> <p>这两者在某些架构上是<strong>独立</strong>的，可以<strong>同时执行</strong>。</p> </li> <li> <p><strong>场景：</strong></p> </li> <li> <p><code>val = val * x + y</code> (FP32 计算)</p> </li> <li> <p><code>ptr += stride</code> (INT32 指针计算/循环计数)</p> </li> <li> <p><strong>优化手段：</strong> 编写 Kernel 时，如果能让浮点计算（业务逻辑）和整数计算（地址索引）交织在一起，SM 可以<strong>在一个周期内同时发射这两条指令</strong>，从而掩盖掉索引计算的开销。</p> </li> </ul> <h4 id=5-fma-fused-multiply-add>5. 使用 FMA (Fused Multiply-Add)<a class=headerlink href=#5-fma-fused-multiply-add title="Permanent link">¶</a></h4> <ul> <li><strong>原理：</strong> <code>a * b + c</code>。</li> <li>如果不优化：<code>MUL</code> (乘法) + <code>ADD</code> (加法) = 2 条指令。</li> <li>优化后：<code>FFMA</code> (Fused FMA) = 1 条指令。</li> <li><strong>优化手段：</strong> 编译器通常会自动优化。但在手写 intrinsic 时，确保调用 <code>__fmaf_rn(a, b, c)</code> 而不是分开写。</li> </ul> <h3 id=examples>Examples<a class=headerlink href=#examples title="Permanent link">¶</a></h3> <ul> <li><strong>Memory-Bound Vector Add</strong>：<a href=https://tom-jerr.github.io/blogs/posts/Vector%20Add%20Optimization%20Example/ >一步步实现 CUDA Vector Add 优化</a></li> <li><strong>Compute-Bound GEMM</strong>: <a href=https://tom-jerr.github.io/notes/cuda/%E4%BB%8EGEMM%E5%AE%9E%E8%B7%B5CUDA%E4%BC%98%E5%8C%96>从 GEMM 实践 CUDA 优化</a></li> </ul> <h2 id=cuda>CUDA 常用优化技巧<a class=headerlink href=#cuda title="Permanent link">¶</a></h2> <h3 id=maximize-compiler-computationgpu-miarch>Maximize Compiler Computation[^gpu-miarch]<a class=headerlink href=#maximize-compiler-computationgpu-miarch title="Permanent link">¶</a></h3> <ul> <li>Unroll Loops<blockquote> <p>展开循环（loop unrolling），让循环体重复展开多次，<strong>减少循环控制开销</strong>（比如 i++、i\&lt;N 的判断），提高 GPU 的吞吐量。</p> </blockquote> </li> <li>Write code using compile-time constants (not same as constant registers)<blockquote> <p>在代码里用 编译期已知的常量来做索引、循环次数、数组大小等，而不是依赖 GPU 的常量寄存器</p> </blockquote> </li> </ul> <h3 id=coalescing-memory-access>Coalescing Memory Access<a class=headerlink href=#coalescing-memory-access title="Permanent link">¶</a></h3> <ul> <li> <p>当一个 Warp 中的<strong>所有 32 个线程访问全局内存中的连续位置时，硬件可以将这 32 个小的请求“合并”成一个单一、大型、高效的内存事务</strong></p> </li> <li> <p>Memory Access Patterns:</p> </li> <li> <p>合并访问（理想）：Warp 中的线程 i 访问内存地址 base + i。这在处理按行主序存储的矩阵的行时非常常见</p> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/coalesce-smem.png data-desc-position=bottom><img alt src=../img/coalesce-smem.png></a></p> </li> <li> <p>跨步访问（问题）：线程 i 访问 base + i * stride。如果步长（stride）很大，这将导致许多独立的、低效的内存事务。这在访问按行主序存储的矩阵的列时很常见</p> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=img/uncoalesce-smem.png data-desc-position=bottom><img alt src=img/uncoalesce-smem.png></a></p> </li> <li> <p>非对齐访问：Warp 访问的起始地址未与内存事务的大小对齐</p> </li> </ul> <h3 id=avoid-bank-conflicts-in-shared-memory>Avoid Bank Conflicts in Shared Memory<a class=headerlink href=#avoid-bank-conflicts-in-shared-memory title="Permanent link">¶</a></h3> <p><img alt=⚠ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/26a0.svg title=:warning:> Shared memory is organized into <strong>32 banks</strong>. Each bank is a slice of SRAM that can load or store <strong>4 bytes of data every cycle</strong>.</p> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/smem.jpg data-desc-position=bottom><img alt src=../img/smem.jpg></a></p> <ul> <li>当一个 Warp 中的所有 32 个线程访问全局内存中的连续位置时，硬件可以将这 32 个小的请求 <strong>“合并”成一个单一、大型、高效的内存事务</strong></li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/conflict-free.png data-desc-position=bottom><img alt src=../img/conflict-free.png></a></p> <ul> <li>当同一个 Warp 中的两个或更多线程试图访问位于同一个内存银行中的不同地址时，就会发生银行冲突 。此时，这些访问会被串行化处理，从而降低了共享内存的有效带宽</li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/bank-conflict.png data-desc-position=bottom><img alt src=../img/bank-conflict.png></a></p> <h4 id=solutionsbankconflict>Solutions[^bankconflict]<a class=headerlink href=#solutionsbankconflict title="Permanent link">¶</a></h4> <ul> <li>Padding: 在数据结构中插入填充元素，以改变数据在内存中的布局，避免多个线程访问同一银行</li> <li>可能降低 SM 的 occupancy</li> <li><strong>可能地址访问不对齐，无法使用向量化访问</strong></li> <li><strong>Swizzling (XOR Swizzling):</strong> 重新组织数据的存储方式，使得并行访问时更少冲突(更常用)<img alt=🚀 class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/1f680.svg title=:rocket:></li> <li> <p><strong>原理：</strong> 通过对地址位进行异或操作，打乱 Bank 的映射关系。</p> </li> <li> <p><strong>公式示例：</strong> <code>bank_id = ((addr / 4) / 8) ^ ((addr / 4) % 32)</code> (简化版)。</p> </li> <li> <p><strong>LDGSTS (Load Global Store Shared):</strong> 现代架构 (Ampere+) 提供了 <code>cp.async</code> 等指令，硬件自动处理 Swizzled 存储，不需要程序员手动计算复杂的地址映射。</p> </li> <li> <p>某些 swizzling 方法在从 shared memory 读数据到 register 时不能进行 float4 的合并读取</p> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/swizzling.jpg data-desc-position=bottom><img alt src=../img/swizzling.jpg></a></p> </li> <li> <p>逻辑位置表示元素在矩阵中的逻辑坐标。</p> </li> <li> <p>物理位置表示其对应元素在实际存储数据的 shared memory 中的位置坐标。</p> <blockquote> <p>当我们说读取矩阵的第 2 行第 3 列的元素，(2,3)就表示逻辑位置，而真正读取数据的时候，我们需要从实际存储数(2,1)的 shared memory 中对应的位置</p> </blockquote> </li> </ul> <h4 id=matmul-bank-conflict-avoidance-examplebankconflict2>Matmul Bank Conflict Avoidance Example[^bankconflict2]<a class=headerlink href=#matmul-bank-conflict-avoidance-examplebankconflict2 title="Permanent link">¶</a></h4> <div class="language-cpp highlight"><span class=filename>C++</span><pre><span></span><code><span id=__span-2-1><a id=__codelineno-2-1 name=__codelineno-2-1></a><a href=#__codelineno-2-1><span class=linenos data-linenos="1 "></span></a><span class=k>const</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>warp_id</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>tid</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=mi>32</span><span class=p>;</span>
</span><span id=__span-2-2><a id=__codelineno-2-2 name=__codelineno-2-2></a><a href=#__codelineno-2-2><span class=linenos data-linenos="2 "></span></a><span class=k>const</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>lane_id</span><span class=w> </span><span class=o>=</span><span class=w> </span><span class=n>tid</span><span class=w> </span><span class=o>%</span><span class=w> </span><span class=mi>32</span><span class=p>;</span>
</span><span id=__span-2-3><a id=__codelineno-2-3 name=__codelineno-2-3></a><a href=#__codelineno-2-3><span class=linenos data-linenos="3 "></span></a><span class=k>const</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>a_tile_index</span><span class=w> </span><span class=o>=</span><span class=w>  </span><span class=n>warp_id</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=mi>2</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=mi>16</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>lane_id</span><span class=w> </span><span class=o>/</span><span class=w> </span><span class=mi>8</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=mi>4</span>
</span><span id=__span-2-4><a id=__codelineno-2-4 name=__codelineno-2-4></a><a href=#__codelineno-2-4><span class=linenos data-linenos="4 "></span></a><span class=k>const</span><span class=w> </span><span class=kt>int</span><span class=w> </span><span class=n>b_tile_index</span><span class=w> </span><span class=o>=</span><span class=w>  </span><span class=n>warp_id</span><span class=w> </span><span class=o>%</span><span class=w> </span><span class=mi>2</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=mi>32</span><span class=w> </span><span class=o>+</span><span class=w> </span><span class=n>lane_id</span><span class=w> </span><span class=o>%</span><span class=w> </span><span class=mi>8</span><span class=w> </span><span class=o>*</span><span class=w> </span><span class=mi>4</span><span class=p>;</span>
</span></code></pre></div> <p>使用的是 4×2 的 warp 布局, warp 中的每个线程按照 4×8 进行排布, 每个 warp 对应 16×32 的数据</p> <ul> <li>每个 wrap 32 个线程只获取 As 的 4*4=16 个数据</li> <li>每个 wrap 32 个线程只获取 Bs 的 8*4=32 个数据</li> <li><img alt=⚠ class=twemoji src=https://cdn.jsdelivr.net/gh/jdecked/twemoji@16.0.1/assets/svg/26a0.svg title=:warning:> 广播 (Broadcast): 如果一个 Warp 中的所有线程访问同一个 bank 中的完全相同的地址，这是一种广播操作，不会产生冲突</li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/avoid-bank-conflict.png data-desc-position=bottom><img alt src=../img/avoid-bank-conflict.png></a></p> <h3 id=double-bufferingbankconflict2>Double Buffering[^bankconflict2]<a class=headerlink href=#double-bufferingbankconflict2 title="Permanent link">¶</a></h3> <p>在共享内存中分配两个缓冲区: 当 SM 正在对缓冲区 1 中的数据进行计算时，硬件可以异步地将下一块数据从全局内存预取到缓冲区 2 中。一旦计算完成，两个缓冲区的角色互换。这种方式有效地将全局内存的访问延迟隐藏在了计算的背后<br> <a class=glightbox data-type=image data-width=80% data-height=auto href=../img/double_buffer.png data-desc-position=bottom><img alt src=../img/double_buffer.png></a></p> <h3 id=warp-shuffle-reduction>Warp Shuffle &amp; Reduction<a class=headerlink href=#warp-shuffle-reduction title="Permanent link">¶</a></h3> <p>在 Softmax 和 LayerNorm 等 Memory-bound 算子中，瓶颈往往在于规约 (Reduction) 操作（如求 Max 或 Sum）。</p> <ul> <li><strong>传统方法：</strong> 使用 Shared Memory 进行树状规约。缺点是需要频繁读写 Shared Memory，且需要 <code>__syncthreads()</code> 同步。</li> <li><strong>优化方法：</strong> 使用 Warp Shuffle API 在寄存器之间直接通信。</li> <li><code>__shfl_down_sync(mask, val, delta)</code>: 将数据从当前线程向下传递给 ID 为 <code>tid + delta</code> 的线程。</li> <li><code>__shfl_xor_sync(mask, val, laneMask)</code>: 蝴蝶交换 (Butterfly Exchange)，用于全规约。</li> <li><strong>Block Reduce:</strong> 当一行数据超过 32 个（比如 hidden_size=4096）时，如何跨 Warp 进行规约？</li> <li>策略：Warp Reduce -&gt; 存入 Shared Memory -&gt; 最后一个 Warp 再读出来 Reduce。</li> </ul> <h3 id=tilematmul>Tile[^matmul]<a class=headerlink href=#tilematmul title="Permanent link">¶</a></h3> <ul> <li> <p>将原本一行 × 一列的计算进行分块，每次只计算一块</p> </li> <li> <p>一次性从全局内存中加载一小块 A (BM x BK) 和一小块 B (BK x BN) 到共享内存中</p> </li> <li> <p>一个线程块内的所有线程就可以在共享内存上快速地进行大量的计算，以完成对应的一小块 C (BM x BN) 的计算</p> </li> <li> <p>每个线程不再是只计算 C 块中的一个元素，而是负<strong>责计算一个更小的结果网格</strong>（图中是 2x2）。这样做可以进一步提升数据复用率和计算效率</p> </li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/tile2.png data-desc-position=bottom><img alt src=../img/tile2.png></a></p> <h3 id=wrap-tilematmul>Wrap Tile[^matmul]<a class=headerlink href=#wrap-tilematmul title="Permanent link">¶</a></h3> <ul> <li><strong>线程块分片(blocktiling)</strong>: 不同的线程块可以在不同的 SM 上并行执行.</li> <li><strong>warp 分片(warptiling)</strong>: 不同的 warp 可以在不同的 warp 调度器上并行执行, 也可以在同一个 warp 调度器上并发执行.</li> <li><strong>线程分片(threadtiling)</strong>: (数量非常有限的)指令可以在相同的 CUDA 内核上并行执行(=指令级并行, instruction-level parallelism, 即 ILP).</li> </ul> <p><a class=glightbox data-type=image data-width=80% data-height=auto href=../img/warp_tile.png data-desc-position=bottom><img alt src=../img/warp_tile.png></a></p> <div class="admonition note"> <p class=admonition-title>Note</p> <p>实际上， WMMA 就是 Warp Tile 的一种实现</p> </div> <h2 id=transformer>Transformer 内部算子优化<a class=headerlink href=#transformer title="Permanent link">¶</a></h2> <h3 id=softmax-layernorm>Softmax &amp; LayerNorm 优化<a class=headerlink href=#softmax-layernorm title="Permanent link">¶</a></h3> <p>这两个算子是典型的 Memory-bound 算子。</p> <ul> <li><strong>Softmax:</strong> <span class=arithmatex>\(Softmax(x_i) = \\frac{e^{x_i - x\_{max}}}{\\sum e^{x_j - x\_{max}}}\)</span></li> <li>需要两次规约：一次求 Max，一次求 Sum。</li> <li>优化核心：使用 Warp Shuffle 进行寄存器级规约，减少 Shared Memory 访问。</li> <li><strong>LayerNorm:</strong> 需要求 Mean 和 Variance。</li> <li>同样依赖高效的 Block Reduce。</li> </ul> <h3 id=flash-attention>Flash Attention 优化<a class=headerlink href=#flash-attention title="Permanent link">¶</a></h3> <p>Standard Attention 的痛点在于 <span class=arithmatex>\(N \\times N\)</span> 的 Attention Matrix 太大，写回 HBM 再读回来做 Softmax，带宽撑不住。</p> <ul> <li><strong>核心思想：</strong><br> 1. <strong>Tiling (分块):</strong> 将 Q, K, V 切分成小块，加载到 SRAM (Shared Memory) 中计算。<br> 1. <strong>Recomputation (重计算):</strong> 为了不存储 <span class=arithmatex>\(N \\times N\)</span> 的中间矩阵，反向传播时宁愿重新计算一遍前向过程。<br> 1. <strong>Online Softmax:</strong> 这是一个数学技巧，允许 Softmax 分块计算，不需要等看到全局最大值。<ul> <li>维护局部的 max 和 sum，当处理新的块时，更新全局的 max 和 sum。</li> </ul> </li> </ul> <h2 id=_2>参考资料<a class=headerlink href=#_2 title="Permanent link">¶</a></h2> <p>[^cuda1]: <a href=https://zhuanlan.zhihu.com/p/645330027>CUDA（一）：CUDA 编程基础</a><br> [^cuda2]: <a href=https://zhuanlan.zhihu.com/p/654027980>CUDA（二）：GPU 的内存体系及其优化指南</a><br> [^matmul]: <a href=https://blog.csdn.net/LostUnravel/article/details/138034380#t12>[CUDA 学习笔记] 如何优化 CUDA 矩阵乘内核以获得类似 cuBLAS 的性能: 工作日志</a><br> [^gpu-miarch]: <a href=https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf>EE 7722, Lecture Slides: NVIDIA GPU Microarchitecture</a><br> [^bankconflict]: <a href=https://zhuanlan.zhihu.com/p/4746910252>CUDA shared memory 避免 bank conflict 的 swizzling 机制解析</a><br> [^bankconflict2]: <a href=https://zhuanlan.zhihu.com/p/696844342>GEMM 优化: 双缓冲 (Prefetch) 和 Bank Conflict 解决</a></p> <form class=md-feedback name=feedback hidden> <fieldset> <legend class=md-feedback__title> Support Me! </legend> <div class=md-feedback__inner> <div class=md-feedback__list> <button class="md-feedback__icon md-icon" type=submit title="Support Me!" data-md-value=1> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 384 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M96 400c-17.7 0-32 14.3-32 32v48c0 17.7 14.3 32 32 32h224c17.7 0 32-14.3 32-32v-48c0-17.7-14.3-32-32-32zm-22.8-48h64.6l-79.5-88.3c-6.6-7.4-10.3-16.9-10.3-26.8V204c0-16.1 11.9-29.5 27.4-31.7 11.8-1.7 20.6-11.8 20.6-23.8V72c0-13.3 10.7-24 24-24 7.2 0 13.6 3.1 18 8.1 4.6 5.2 11.1 8.1 18 8.1s13.4-3 18-8.1c4.4-5 10.8-8.1 18-8.1 8.5 0 15.9 4.4 20.2 11.1 6.9 10.7 20.9 14.2 32 8 3.5-1.9 7.4-3.1 11.8-3.1 10.6 0 19.7 6.9 22.8 16.6 3.8 11.7 15.9 18.7 28 16 1.7-.4 3.4-.6 5.2-.6 13.3 0 24 10.7 24 24v92.2c0 14.4-3.5 28.5-10.2 41.2L273.6 352h54.3l40.3-76.2c10.4-19.6 15.8-41.5 15.8-63.6V120c0-38.4-30.1-69.8-68.1-71.9C303 28.8 281 16 256 16c-5.7 0-11.2.7-16.5 1.9C226.8 6.8 210.2 0 192 0c-13.1 0-25.4 3.5-36 9.6C145.4 3.5 133.1 0 120 0 80.2 0 48 32.2 48 72v58.7C19.7 143 0 171.2 0 204v32.9c0 21.7 8 42.7 22.6 58.9z"></path></svg> </button> </div> <div class=md-feedback__note> <div data-md-value=1 hidden> <center> 难道说……你愿意给我买一瓶快乐水吗！🫣 </center> <center> How about...buy me a coffee？😋 </center> <img src=/img/wechat.png width=200px> </div> </div> </div> </fieldset> </form> <!-- Insert generated snippet here --> <script src=https://giscus.app/client.js data-repo=tom-jerr/tom-jerr.github.io data-repo-id=R_kgDONZrjcA data-category=Announcements data-category-id=DIC_kwDONZrjcM4Ck-PT data-mapping=title data-strict=0 data-reactions-enabled=1 data-emit-metadata=1 data-input-position=top data-theme=preferred_color_scheme data-lang=en data-loading=lazy crossorigin=anonymous async>
    </script> <!-- Synchronize Giscus theme with palette --> <script>
    var giscus = document.querySelector("script[src*=giscus]")

    // Set palette on initial load
    var palette = __md_get("__palette")
    if (palette && typeof palette.color === "object") {
        var theme = palette.color.scheme === "slate"
            ? "transparent_dark"
            : "light"

        // Instruct Giscus to set theme
        giscus.setAttribute("data-theme", theme)
    }

    // Register event handlers after documented loaded
    document.addEventListener("DOMContentLoaded", function () {
        var ref = document.querySelector("[data-md-component=palette]")
        ref.addEventListener("change", function () {
            var palette = __md_get("__palette")
            if (palette && typeof palette.color === "object") {
                var theme = palette.color.scheme === "slate"
                    ? "transparent_dark"
                    : "light"

                // Instruct Giscus to change theme
                var frame = document.querySelector(".giscus-frame")
                frame.contentWindow.postMessage(
                    {giscus: {setConfig: {theme}}},
                    "https://giscus.app"
                )
            }
        })
    })
</script> </article> </div> <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script> <script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script> </div> <button type=button class="md-top md-icon" data-md-component=top hidden> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"></path></svg> Back to top </button> </main> <!-- Application footer --> <footer class=md-footer> <!-- Link to previous and/or next page --> <!-- Website statistics --> <div class=md-footer-stats> <div class="md-footer-stats__inner md-grid"> <div class=md-footer-stats__content> <span class=md-footer-stats__item> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 24 24" width=16 height=16 style="vertical-align: middle; margin-right: 4px;"> <path fill=currentColor d="M19,19H5V8H19M16,1V3H8V1H6V3H5C3.89,3 3,3.89 3,5V19A2,2 0 0,0 5,21H19A2,2 0 0,0 21,19V5C21,3.89 20.1,3 19,3H18V1M17,12H12V17H17V12Z"></path> </svg> <span>运行时间: <span id=web-time>加载中...</span></span> </span> </div> </div> </div> <!-- Further information --> <div class="md-footer-meta md-typeset"> <div class="md-footer-meta__inner md-grid"> <div class=md-footer-copyright> <div class=md-footer-copyright__highlight> Copyright © 2025 - Present <a href=https://github.com/tom-jerr/ target=_blank rel=noopener>tom-jerr</a> </div> Made with <a href=https://squidfunk.github.io/mkdocs-material/ target=_blank rel=noopener> Material for MkDocs </a> </div> <!-- Social links --> <div class=md-social> <a href=https://github.com/tom-jerr/ target=_blank rel=noopener title=GitHub class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 512 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M173.9 397.4c0 2-2.3 3.6-5.2 3.6-3.3.3-5.6-1.3-5.6-3.6 0-2 2.3-3.6 5.2-3.6 3-.3 5.6 1.3 5.6 3.6m-31.1-4.5c-.7 2 1.3 4.3 4.3 4.9 2.6 1 5.6 0 6.2-2s-1.3-4.3-4.3-5.2c-2.6-.7-5.5.3-6.2 2.3m44.2-1.7c-2.9.7-4.9 2.6-4.6 4.9.3 2 2.9 3.3 5.9 2.6 2.9-.7 4.9-2.6 4.6-4.6-.3-1.9-3-3.2-5.9-2.9M252.8 8C114.1 8 8 113.3 8 252c0 110.9 69.8 205.8 169.5 239.2 12.8 2.3 17.3-5.6 17.3-12.1 0-6.2-.3-40.4-.3-61.4 0 0-70 15-84.7-29.8 0 0-11.4-29.1-27.8-36.6 0 0-22.9-15.7 1.6-15.4 0 0 24.9 2 38.6 25.8 21.9 38.6 58.6 27.5 72.9 20.9 2.3-16 8.8-27.1 16-33.7-55.9-6.2-112.3-14.3-112.3-110.5 0-27.5 7.6-41.3 23.6-58.9-2.6-6.5-11.1-33.3 2.6-67.9 20.9-6.5 69 27 69 27 20-5.6 41.5-8.5 62.8-8.5s42.8 2.9 62.8 8.5c0 0 48.1-33.6 69-27 13.7 34.7 5.2 61.4 2.6 67.9 16 17.7 25.8 31.5 25.8 58.9 0 96.5-58.9 104.2-114.8 110.5 9.2 7.9 17 22.9 17 46.4 0 33.7-.3 75.4-.3 83.6 0 6.5 4.6 14.4 17.3 12.1C436.2 457.8 504 362.9 504 252 504 113.3 391.5 8 252.8 8M105.2 352.9c-1.3 1-1 3.3.7 5.2 1.6 1.6 3.9 2.3 5.2 1 1.3-1 1-3.3-.7-5.2-1.6-1.6-3.9-2.3-5.2-1m-10.8-8.1c-.7 1.3.3 2.9 2.3 3.9 1.6 1 3.6.7 4.3-.7.7-1.3-.3-2.9-2.3-3.9-2-.6-3.6-.3-4.3.7m32.4 35.6c-1.6 1.3-1 4.3 1.3 6.2 2.3 2.3 5.2 2.6 6.5 1 1.3-1.3.7-4.3-1.3-6.2-2.2-2.3-5.2-2.6-6.5-1m-11.4-14.7c-1.6 1-1.6 3.6 0 5.9s4.3 3.3 5.6 2.3c1.6-1.3 1.6-3.9 0-6.2-1.4-2.3-4-3.3-5.6-2"></path></svg> </a> <a href=/img/qq.png target=_blank rel=noopener title=QQ class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M434.1 420.4c-11.5 1.4-44.9-52.7-44.9-52.7 0 31.3-16.1 72.2-51 101.8 16.8 5.2 54.8 19.2 45.8 34.4-7.3 12.3-125.5 7.9-159.6 4-34.1 3.8-152.3 8.3-159.6-4-9-15.2 28.9-29.2 45.8-34.4-34.9-29.5-51.1-70.4-51.1-101.8 0 0-33.3 54.1-44.9 52.7-5.4-.6-12.4-29.6 9.3-99.7 10.3-33 22-60.5 40.1-105.8C60.9 98 109.2-.1 224.3-.1 338-.1 387.5 96 384.6 214.9c18.1 45.2 29.9 72.9 40.1 105.8 21.8 70.1 14.7 99.1 9.3 99.7z"></path></svg> </a> <a href=/img/wechat.png target=_blank rel=noopener title=微信 class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M385.2 167.6c6.4 0 12.6.3 18.8 1.1C387.4 90.3 303.3 32 207.7 32 100.5 32 13 104.8 13 197.4c0 53.4 29.3 97.5 77.9 131.6l-19.3 58.6 68-34.1c24.4 4.8 43.8 9.7 68.2 9.7 6.2 0 12.1-.3 18.3-.8-4-12.9-6.2-26.6-6.2-40.8-.1-84.9 72.9-154 165.3-154m-104.5-52.9c14.5 0 24.2 9.7 24.2 24.4 0 14.5-9.7 24.2-24.2 24.2-14.8 0-29.3-9.7-29.3-24.2.1-14.7 14.6-24.4 29.3-24.4m-136.4 48.6c-14.5 0-29.3-9.7-29.3-24.2 0-14.8 14.8-24.4 29.3-24.4 14.8 0 24.4 9.7 24.4 24.4 0 14.6-9.6 24.2-24.4 24.2M563 319.4c0-77.9-77.9-141.3-165.4-141.3-92.7 0-165.4 63.4-165.4 141.3S305 460.7 397.6 460.7c19.3 0 38.9-5.1 58.6-9.9l53.4 29.3-14.8-48.6C534 402.1 563 363.2 563 319.4m-219.1-24.5c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.8 0 24.4 9.7 24.4 19.3 0 10-9.7 19.6-24.4 19.6m107.1 0c-9.7 0-19.3-9.7-19.3-19.6 0-9.7 9.7-19.3 19.3-19.3 14.5 0 24.4 9.7 24.4 19.3.1 10-9.9 19.6-24.4 19.6"></path></svg> </a> <a href=https://x.com/tom_jerry_jack target=_blank rel=noopener title=X class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 448 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M357.2 48h70.6L273.6 224.2 455 464H313L201.7 318.6 74.5 464H3.8l164.9-188.5L-5.2 48h145.6l100.5 132.9zm-24.8 373.8h39.1L119.1 88h-42z"></path></svg> </a> <a href=https://www.zhihu.com/people/chen-wen-de-jian-ke target=_blank rel=noopener title=Zhihu class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 640 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M170.5 148.1v217.5h23.4l7.7 26.4 42-26.4h49.5V148.1H170.4zM268.3 342h-27.9l-27.9 17.5-5.1-17.5h-11.9V171.7h72.8zm-118.5-94.3H97.5c1.7-27.1 2.2-51.6 2.2-73.5h51.2s2-22.6-8.6-22.3H53.8c3.5-13.1 7.9-26.7 13.1-40.7 0 0-24.1 0-32.3 21.6-3.4 8.9-13.2 43.1-30.7 78.1 5.9-.6 25.4-1.2 36.8-22.2 2.1-5.9 2.5-6.7 5.1-14.5h28.9c0 10.5-1.2 66.9-1.7 73.4H20.7c-11.7 0-15.6 23.6-15.6 23.6h65.6c-4.4 49.9-28 91.9-70.8 125.1 20.5 5.9 40.9-.9 51-9.9 0 0 23-20.9 35.6-69.3l54 64.9s7.9-26.9-1.2-40c-7.6-8.9-28.1-33.1-36.8-41.8L87.9 312c4.4-14 7-27.6 7.9-40.7h61.6s-.1-23.6-7.6-23.6m412-1.6c20.8-25.6 45-58.6 45-58.6s-18.6-14.8-27.4-4.1c-6 8.2-36.8 48.2-36.8 48.2l19.2 14.4zm-150-59.1c-9-8.2-25.9 2.1-25.9 2.1s39.5 55 41.1 57.4l19.5-13.7s-25.7-37.6-34.7-45.9zM640 258.4c-19.8 0-130.9.9-131.1.9v-101q7.2 0 22.8-1.2c40.9-2.4 70.1-4 87.8-4.8 0 0 12.2-27.2-.6-33.4-3.1-1.2-23.2 4.6-23.2 4.6s-165.2 16.5-232.4 18c1.6 8.8 7.6 17.1 15.8 19.6 13.3 3.5 22.7 1.7 49.2.9 24.8-1.6 43.7-2.4 56.5-2.4v99.8H351.3s2.8 22.3 25.5 22.9h107.9v70.9c0 14-11.2 22-24.5 21.1-14.1.1-26.1-1.1-41.7-1.8 2 4 6.3 14.4 19.3 21.8 9.9 4.8 16.2 6.6 26 6.6 29.6 0 45.7-17.3 44.9-45.3v-73.3h122.4c9.7 0 8.7-23.8 8.7-23.8z"></path></svg> </a> <a href=mailto:2584074296@qq.com target=_blank rel=noopener title="send email to me!" class=md-social__link> <svg xmlns=http://www.w3.org/2000/svg viewbox="0 0 576 512"><!-- Font Awesome Free 7.1.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M536.4-26.3c9.8-3.5 20.6-1 28 6.3s9.8 18.2 6.3 28l-178 496.9c-5 13.9-18.1 23.1-32.8 23.1-14.2 0-27-8.6-32.3-21.7l-64.2-158c-4.5-11-2.5-23.6 5.2-32.6l94.5-112.4c5.1-6.1 4.7-15-.9-20.6s-14.6-6-20.6-.9l-112.4 94.3c-9.1 7.6-21.6 9.6-32.6 5.2L38.1 216.8c-13.1-5.3-21.7-18.1-21.7-32.3 0-14.7 9.2-27.8 23.1-32.8z"></path></svg> </a> </div> </div> </div> </footer> <!-- Website running time counter script --> <script>
  function updateWebTime() {
    var startDate = new Date("2025/05/04 20:00:00");
    var now = new Date();
    var diff = now.getTime() - startDate.getTime();
    
    var days = Math.floor(diff / (1000 * 60 * 60 * 24));
    var hours = Math.floor((diff % (1000 * 60 * 60 * 24)) / (1000 * 60 * 60));
    var minutes = Math.floor((diff % (1000 * 60 * 60)) / (1000 * 60));
    
    var timeStr = days + " 天 " + hours + " 小时 " + minutes + " 分钟";
    
    var webTimeElement = document.getElementById("web-time");
    if (webTimeElement) {
      webTimeElement.textContent = timeStr;
    }
    
    setTimeout(updateWebTime, 60000); // 每分钟更新一次
  }
  
  // 页面加载完成后启动计时器
  if (document.readyState === "loading") {
    document.addEventListener("DOMContentLoaded", updateWebTime);
  } else {
    updateWebTime();
  }
  

</script> </div> <div class=md-dialog data-md-component=dialog> <div class="md-dialog__inner md-typeset"></div> </div> <script id=__config type=application/json>{"annotate": null, "base": "../../..", "features": ["content.code.annotate", "content.code.copy", "content.code.select", "content.footnote.tooltips", "content.tabs.link", "header.autohide", "navigation.tracking", "navigation.tabs", "navigation.top", "navigation.path", "navigation.indexes", "search.highlight", "search.share", "search.suggest", "toc.follow"], "search": "../../../assets/javascripts/workers/search.2c215733.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script> <script src=../../../assets/javascripts/bundle.79ae519e.min.js></script> <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script> <script src=https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js></script> <script src=https://unpkg.com/mathjax@3/es5/tex-mml-chtml.js></script> <script src=../../../js/mathjax.js></script> <script src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js></script> <script src=https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js></script> <script src=../../../assets/document_dates/tippy/popper.min.js></script> <script src=../../../assets/document_dates/tippy/tippy.umd.min.js></script> <script src=../../../assets/document_dates/core/default.config.js></script> <script src=../../../assets/document_dates/user.config.js></script> <script src=../../../assets/document_dates/core/utils.js></script> <script src=../../../assets/document_dates/core/core.js></script> <script id=init-glightbox>const lightbox = GLightbox({"touchNavigation": true, "loop": false, "zoomable": true, "draggable": true, "openEffect": "zoom", "closeEffect": "zoom", "slideEffect": "slide"});
document$.subscribe(()=>{ lightbox.reload(); });
</script></body></html>