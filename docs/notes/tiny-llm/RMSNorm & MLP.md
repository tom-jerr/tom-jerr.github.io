---
title: RMSNorm & MLP
date: 2025/10/11 23:15
tags:
  - LLMInference
---

# RMSNorm & MLP

## RMSNorm

RMSNorm[^rmsnorm] çš„å®šä¹‰ï¼š

$$
	y = \frac{x}{\sqrt{mean(x^2) + \epsilon}} \cdot weight
$$

- `x`Â  æ˜¯è¾“å…¥å¼ é‡ã€‚
- `weight`Â  æ˜¯ä¸€ä¸ªå¯å­¦ä¹ çš„ç¼©æ”¾å‚æ•°ã€‚
- `epsilon(eps)` æ˜¯ä¸ºäº†æ•°å€¼ç¨³å®šæ€§è€Œæ·»åŠ çš„ä¸€ä¸ªå°å¸¸æ•°ï¼ˆä¾‹å¦‚ï¼Œ1e-5 æˆ– 1e-6ï¼‰ã€‚
- $mean(x^2)$ æ˜¯å¹³æ–¹å’Œç„¶åé™¤ä»¥å…ƒç´ çš„æ•°é‡
- LayerNorm æˆåŠŸçš„å…³é”®åœ¨äºå…¶ **â€œé‡æ–°ç¼©æ”¾â€ (re-scaling)** çš„ä¸å˜æ€§ï¼Œè€Œ **â€œé‡æ–°ä¸­å¿ƒåŒ–â€ (re-centeringï¼Œå³å‡å»å‡å€¼)** è¿™ä¸€æ­¥å¯èƒ½ä¸æ˜¯å¿…éœ€çš„

### å½’ä¸€åŒ–æ–¹æ³•

| **æ–¹æ³•**                 | **å½’ä¸€åŒ–ç»´åº¦**                                    | **æ˜¯å¦ä¾èµ–æ‰¹æ¬¡** | **ä¸»è¦åº”ç”¨é¢†åŸŸ**           |
| ------------------------ | ------------------------------------------------- | ---------------- | -------------------------- |
| **BatchNorm**            | è·¨æ‰¹æ¬¡ (`N`)ï¼Œåœ¨æ¯ä¸ªç‰¹å¾ (`D`) ä¸Šç‹¬ç«‹             | **æ˜¯**           | è®¡ç®—æœºè§†è§‰ (CNN)           |
| **LayerNormÂ Â  å±‚èŒƒæ•°**   | è·¨ç‰¹å¾ (`D`)ï¼Œåœ¨æ¯ä¸ªæ ·æœ¬ (`N, S`) ä¸Šç‹¬ç«‹          | **å¦**           | è‡ªç„¶è¯­è¨€å¤„ç† (Transformer) |
| **GroupNormÂ Â  ç¾¤ä½“è§„èŒƒ** | è·¨ç‰¹å¾åˆ†ç»„ï¼Œåœ¨æ¯ä¸ªæ ·æœ¬ä¸Šç‹¬ç«‹                      | **å¦**           | è®¡ç®—æœºè§†è§‰ (å°æ‰¹æ¬¡åœºæ™¯)    |
| **RMSNormÂ Â  å‡æ–¹æ ¹èŒƒæ•°** | è·¨ç‰¹å¾ (`D`)ï¼Œåœ¨æ¯ä¸ªæ ·æœ¬ä¸Šç‹¬ç«‹ (ç®€åŒ–ç‰ˆ LayerNorm) | **å¦**           | **å¤§è¯­è¨€æ¨¡å‹ (LLM)**       |

### ä¸ºä»€ä¹ˆ LLM ä½¿ç”¨ RMSNorm

- **æè‡´çš„æ•ˆç‡** ğŸš€ï¼šLLM å‚æ•°é‡æå¤§ï¼ŒRMSNorm å»æ‰ä¸€ä¸ªå¯è®­ç»ƒå‚æ•°å¯¹ FLOPs å‡å°‘æ•ˆæœéå¸¸æ˜æ˜¾ã€‚å¯ä»¥å®ç°æ›´å¿«çš„è®­ç»ƒé€Ÿåº¦å’Œæ›´ä½çš„æ¨ç†å»¶è¿Ÿã€‚
- **æ€§èƒ½ç›¸å½“**ï¼šè¿™ç§ç®€åŒ–å¹¶æ²¡æœ‰å¸¦æ¥æ˜æ˜¾çš„æ€§èƒ½æŸå¤±ã€‚åœ¨ Transformer è¿™ç§ç»“æ„ä¸­ï¼Œå»æ‰å‡å€¼ä¸­å¿ƒåŒ–è¿™ä¸€æ­¥ï¼Œæ¨¡å‹çš„æ€§èƒ½ä¾ç„¶éå¸¸ç¨³å®šå’Œå¼ºå¤§ã€‚
- **è®­ç»ƒç¨³å®šæ€§**ï¼šRMSNorm ä¿ç•™äº† LayerNorm æœ€é‡è¦çš„ç‰¹æ€§â€”â€”**é‡æ–°ç¼©æ”¾çš„ä¸å˜æ€§**ã€‚è¿™æ„å‘³ç€å®ƒä¾ç„¶èƒ½å¤Ÿæœ‰æ•ˆåœ°æ§åˆ¶å±‚ä¸å±‚ä¹‹é—´æ¿€æ´»å€¼çš„å°ºåº¦ï¼ˆscaleï¼‰ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸æˆ–æ¶ˆå¤±ï¼Œä¿è¯äº†æ·±åº¦ç½‘ç»œè®­ç»ƒçš„ç¨³å®šæ€§ã€‚
  RMSNorm æä¾›äº†ä¸€ä¸ª **æ€§ä»·æ¯”æé«˜** çš„æ–¹æ¡ˆï¼Œç”¨æœ€å°çš„æ”¹åŠ¨æ¢æ¥äº†æ˜¾è‘—çš„æ•ˆç‡æå‡ï¼ŒåŒæ—¶åˆæ²¡æœ‰ç‰ºç‰²æ¨¡å‹æ€§èƒ½ï¼Œå› æ­¤è¿…é€Ÿæˆä¸ºåƒ Llamaã€Qwen ç­‰ä¸»æµå¤§æ¨¡å‹çš„æ ‡é…ã€‚

## MLP

### FFN v1

åœ¨åŸå§‹ Transformerï¼ˆVaswani et al., 2017[^transformer]ï¼‰ä¸­ï¼Œå‰é¦ˆå±‚ï¼ˆFFNï¼‰é€šå¸¸æ˜¯ï¼š
$$\text{FFN}(x) = W_2 \cdot \text{ReLU}(W_1 x)$$
åæ¥å¾ˆå¤šæ¨¡å‹ï¼ˆå¦‚ BERTï¼‰æ”¹æˆäº† `GELU` æ¿€æ´»ï¼Œä½†æ•´ä½“ç»“æ„ä»ç„¶æ˜¯â€œçº¿æ€§ â†’ éçº¿æ€§ â†’ çº¿æ€§â€ï¼Œæ²¡æœ‰é—¨æ§æœºåˆ¶ã€‚

âš  **å­˜åœ¨é—®é¢˜**

> Transformer çš„ FFN å±‚åªä½¿ç”¨ä¸€ä¸ªå•ä¸€çš„éçº¿æ€§æ¿€æ´»ï¼ˆå¦‚ ReLU/GELUï¼‰ï¼Œ  
> é™åˆ¶äº†æ¨¡å‹çš„è¡¨è¾¾èƒ½åŠ›å’Œæ¢¯åº¦æµåŠ¨ã€‚

### FFN v2

å¯ä»¥å€Ÿé‰´å·ç§¯ç½‘ç»œä¸­ **é—¨æ§æœºåˆ¶ï¼ˆGated Linear Unit, GLUï¼‰** çš„æ€æƒ³ï¼Œ è®©å‰é¦ˆå±‚çš„è¾“å‡º**ç”±ä¸¤æ¡è·¯å¾„å…±åŒå†³å®šï¼šä¸€æ¡ä¸»è·¯å¾„ + ä¸€æ¡é—¨æ§è·¯å¾„**ã€‚

$$
GLU(X)=(xW_a + b_a)\odot \sigma(xW_b + b_b)
$$

$W_a$ æ˜¯ä¸»é€šé“ï¼Œ$W_b$ æ˜¯é—¨æ§é€šé“ï¼Œé—¨æ§è·¯å¾„æ§åˆ¶ä¸»è·¯å¾„ä¸­å“ªäº›ç‰¹å¾å¯ä»¥â€œé€šè¿‡â€

### FFN v3

Shazeer [^swiglu]ç³»ç»Ÿåœ°æµ‹è¯•äº† **å„ç§ GLU å˜ä½“** æ›¿æ¢ Transformer çš„ FFN å­å±‚ä¸­çš„æ¿€æ´»å‡½æ•°ã€‚

ä»–æŠŠä¼ ç»Ÿ FFNï¼š
$$W2\cdot ReLU(W1x)$$
æ›¿æ¢æˆï¼š
$$W_3 \cdot \big( f(W_1 x) \odot g(W_2 x) \big)$$
å…¶ä¸­ f å’Œ g æ˜¯ä¸åŒçš„æ¿€æ´»å‡½æ•°ç»„åˆï¼ˆsigmoid, ReLU, GELU, Swish...ï¼‰ã€‚

| åç§°       | å…¬å¼                | æ¿€æ´»å‡½æ•°     | ç‰¹ç‚¹               |
| ---------- | ------------------- | ------------ | ------------------ |
| **GLU**    | $a \odot \sigma(b)$ | sigmoid      | åŸå§‹ç‰ˆæœ¬           |
| **ReGLU**  | $a \odot ReLU(b)$   | ReLU         | ç¨€ç–æ¿€æ´»ï¼Œè®¡ç®—ç®€å• |
| **GeGLU**  | $a \odot GELU(b)$   | GELU         | å¹³æ»‘æ¢¯åº¦           |
| **SwiGLU** | $a \odot Swish(b)$  | Swish (SiLU) | æœ€å¹³æ»‘ã€æ•ˆæœæœ€å¥½   |

ğŸš€ **ç”¨é—¨æ§æ¿€æ´»æ›¿æ¢ä¼ ç»Ÿ FFN æ¿€æ´»èƒ½æ˜¾è‘—æå‡ Transformer æ€§èƒ½ã€‚**

| åç»­æ¨¡å‹                             | é‡‡ç”¨çš„å˜ä½“ | æ¥æº              |
| ------------------------------------ | ---------- | ----------------- |
| **T5**                               | GeGLU      | æ¥è‡ª[^swiglu]     |
| **PaLM / Qwen2 / LLaMA2-3 / Falcon** | SwiGLU     | æ¥è‡ª[^swiglu]ç»“è®º |
| **GPT-NeoX / MPT / Baichuan**        | SwiGLU     | åŒæºæ€æƒ³          |

#### æŸå¤±å‡½æ•°

- GELU: $\text{GELU}(x) = x \cdot \Phi(x)$ï¼Œ**æ˜¯ä¸€ä¸ªæ¦‚ç‡é—¨æ§ï¼Œå¯¹è¾“å…¥ä¹˜ä»¥ä¸€ä¸ªä»‹äº 0 ï½ 1 çš„æ¦‚ç‡ï¼ˆç”±é«˜æ–¯åˆ†å¸ƒå†³å®šï¼‰**
  å…¶ä¸­ï¼Œ$\Phi(x)$ æ˜¯æ ‡å‡†æ­£æ€åˆ†å¸ƒçš„ç´¯ç§¯åˆ†å¸ƒå‡½æ•° (CDF)ï¼š
  $$
  \Phi(x) = \frac{1}{2}\left[1 + \operatorname{erf}\left(\frac{x}{\sqrt{2}}\right)\right]
  $$
      ç­‰ä»·çš„è¿‘ä¼¼å½¢å¼ï¼ˆä¾¿äºè®¡ç®—ï¼‰ä¸ºï¼š
  $$
  \text{GELU}(x) \approx 0.5x \left(1 + \tanh\left[\sqrt{\frac{2}{\pi}}\left(x + 0.044715x^3\right)\right]\right)
  $$
- SiLU: $SiLU(x)=x \cdot \sigma(x)$ï¼Œ**å¹³æ»‘å¯å¯¼ï¼Œæ— é›¶æ¢¯åº¦æ­»åŒº**ï¼›
  - å¯¹å°è´Ÿå€¼ä»ç„¶æœ‰éƒ¨åˆ†å“åº”
  - å¯¹å¤§æ­£å€¼è¶‹è¿‘äºçº¿æ€§ï¼›
    - å¯¹å¤§è´Ÿå€¼è¶‹è¿‘äº 0ã€‚

| ç‰¹æ€§           | GELU                 | SiLU (Swish)                    |
| -------------- | -------------------- | ------------------------------- |
| å®šä¹‰           | $x \cdot \Phi(x)$    | $x \cdot \sigma(x)$             |
| æ¥æº           | BERT, GPT ç­‰         | EfficientNet, LLaMA, Qwen2      |
| å¹³æ»‘æ€§         | é«˜                   | é«˜                              |
| æ˜¯å¦å¯¹ç§°       | å¦                   | å¦                              |
| æ˜¯å¦æœ‰é—¨æ§å½¢å¼ | å¦                   | âœ… å¯ç›´æ¥é—¨æ§ï¼ˆGLU å˜ä½“ä¸­å¸¸ç”¨ï¼‰ |
| è®¡ç®—å¤æ‚åº¦     | ç¨é«˜ï¼ˆå« tanh/erfï¼‰  | ç•¥ä½                            |
| æ•ˆæœ           | Transformer ç»å…¸é€‰æ‹© | ç°ä»£ LLMï¼ˆå¦‚ Qwen2ã€LLaMAï¼‰åçˆ± |

### Qwen2 MLP

Qwen2 çš„ MLP ä¸­çš„æ‰€æœ‰çº¿æ€§æŠ•å½±é€šå¸¸éƒ½æ˜¯æ—  bias å®ç°çš„ã€‚

- A gate linear projection ($W_{gate}$â€‹).
- An up linear projection ($W_{up}$â€‹).
- A SiLU activation function applied to the output ofÂ $W_{gate}$
- An element-wise multiplication of the **SiLU-activatedÂ $W_{gate}$Â output and the$W_{up}$Â output. This forms the "gated" part**.
- A final down linear projection ($W_{down}$â€‹).
  $$
  MLP(x) = (SiLu(W_{gate}(x))\odot W_{up}(x))W_{down}
  $$

```python
N.. is zero or more dimensions for batches
E is hidden_size (embedding dimension of the model)
I is intermediate_size (dimension of the hidden layer in MLP)
L is the sequence length

input: N.. x L x E
w_gate: I x E
w_up: I x E
w_down: E x I
output: N.. x L x E
```

## Reference

[^transformer]: [Attention Is All You Need](https://arxiv.org/abs/1706.03762)
[^swiglu]: [GLU Variants Improve Transformer](https://arxiv.org/pdf/2002.05202v1)
[^rmsnorm]: [Root Mean Square Layer Normalization](https://arxiv.org/abs/1910.07467)
