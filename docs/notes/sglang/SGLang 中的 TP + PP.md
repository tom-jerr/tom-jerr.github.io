---
title: SGLang ä¸­çš„ TP + PP
tags:
  - LLMInference
date: 2025/11/22
---

# SGLang ä¸­çš„ TP+ PP

è¿™é‡Œæˆ‘ä»¬ä»¥ `Qwen2` æ¨¡å‹ä¸ºä¾‹ï¼Œå¼€å¯ PP + TP åˆ†æä¸€ä¸‹ SGLang æ˜¯å¦‚ä½•å®ç°æ¨¡å‹æ¨ç†çš„å¹¶è¡Œçš„

## ä¸åŒèŠ‚ç‚¹çš„èŒè´£

| èŠ‚ç‚¹       | å·¥ä½œå†…å®¹                                       |
| -------- | ------------------------------------------ |
| Rank 0   | tokenizerã€detokenizerã€HTTPæœåŠ¡ã€è°ƒåº¦å™¨ã€æ¨¡å‹ worker |
| Rank > 0 | **åªè¿è¡Œè°ƒåº¦å™¨ + workerï¼Œä¸å¤„ç†å‰ç«¯æœåŠ¡**                |

## Initiallize Server

1. `launch_server.py` ä¸­æ ¹æ® `grpc_mode` å‚æ•°å†³å®šæ‰§è¡Œ `serve_grpc()` æˆ–è€… `launch_server()`
	> åç»­ä»¥ `launch_server()` ä¸ºä¾‹è¿›è¡Œè®²è§£

2. è°ƒç”¨ `engine.py` ä¸­çš„ `_launch_subprocesses()`
	> è¿™é‡Œæ‰€æœ‰çš„å­è¿›ç¨‹ä»¥ **spawn** æ–¹å¼æ´¾ç”Ÿå…¨æ–° Python è¿›ç¨‹

	- æŒ‰ç…§ pp å’Œ tp é‡æ–°æ˜ å°„ GPU Idï¼Œç„¶åä¸ºæ¯ä¸ª GPU åˆ›å»ºä¸€ä¸ª`mp.Pipe()`
	- æ¥ç€å¯åŠ¨ä¸€ä¸ª Scheduler **å­è¿›ç¨‹**ï¼Œä¼ å…¥ pipe çš„å†™ç«¯ï¼ˆä½¿ç”¨ `run_scheduler_process()`ï¼‰ï¼Œç„¶å**çˆ¶è¿›ç¨‹ä¿ç•™å­è¿›ç¨‹å¼•ç”¨å’Œ pipe çš„è¯»ç«¯**
	- å¤šæœºåœºæ™¯ä¸‹é 0 èŠ‚ç‚¹ä¸å‚ä¸å‰ç«¯æœåŠ¡ï¼Œä»…è´Ÿè´£å¯åŠ¨ scheduler è¿›ç¨‹å¹¶**ä¿æŒèŠ‚ç‚¹å¥åº·**ï¼Œé¿å…é‡å¤è¿è¡Œ tokenizer / detokenizer / æ¥å…¥æœåŠ¡ã€‚
	- 0 å·èŠ‚ç‚¹å¯åŠ¨ detokenizer å­è¿›ç¨‹ by `run_detokenizer_process()`ï¼Œ
	- å¯åŠ¨ TokenizerManagerï¼Œç­‰å¾…æ‰€æœ‰çš„ GPU éƒ½åŠ è½½äº† model å¹¶æ‹¥æœ‰ç›¸åŒçš„ `scheduler_info`(By `mp.Pipe()`)
3. ä» `_launch_subprocesses()` ä¸­è·å¾— `tokenizer_manager`  å’Œ `scheduler_info`ï¼Œç›´æ¥åœ¨ä¸»è¿›ç¨‹å¯åŠ¨ HTTP æœåŠ¡ TokenizerManager è¿›è¡Œè¯·æ±‚çš„æ¥æ”¶

ğŸ˜†ç°åœ¨ï¼ŒTokenizer è¿›ç¨‹ï¼ŒScheduler è¿›ç¨‹ï¼ŒDetokenizer è¿›ç¨‹å¯ä»¥é€šè¿‡äº‹ä»¶å¾ªç¯ä¸åœé©±åŠ¨ï¼Œå®ç°ç”¨æˆ·è¯·æ±‚çš„å¤„ç†

### Scheduler 

å…ˆåˆ›å»º Scheduler å¯¹è±¡ï¼Œåœ¨ `__init__` ä¸­è¿›è¡Œ TpModelWorker åˆå§‹åŒ–ï¼ŒDraftWorker åˆå§‹åŒ–ï¼Œmemory pool å’Œ memory cache åˆå§‹åŒ–

ç„¶åæ ¹æ® `server_args` ä¸åŒï¼Œå¯åŠ¨ä¸åŒçš„äº‹ä»¶å¾ªç¯

```python
if disaggregation_mode == DisaggregationMode.NULL:
	if scheduler.enable_pdmux:
		scheduler.event_loop_pdmux()
	elif server_args.pp_size > 1:
		scheduler.event_loop_pp()
	elif scheduler.enable_overlap:
		scheduler.event_loop_overlap()
	else:
		scheduler.event_loop_normal()
```

### TpModelWorker & ModelRunner

TpModelWoker çš„ `__init__()` ä¸­è¿›è¡Œäº† ModelRunner çš„åˆå§‹åŒ–

ModelRunner çš„ `__init__()` è°ƒç”¨äº† `self.init_torch_distributed()`
- ç¡®è®¤ä½¿ç”¨çš„é€šä¿¡åç«¯ï¼Œè¿™é‡Œä»¥ NCCL ä¸ºä¾‹
- æœ€ç»ˆè°ƒç”¨Â parallel_state.pyÂ ä¸­çš„Â `initialize_model_parallel()`ã€‚
- åˆ›å»ºå…¨å±€çš„Â `_TP`Â (Tensor Parallelism) è¿›ç¨‹ç»„ã€‚å‡è®¾ TP=4ï¼ŒGPU 0-3 ä¼šè¢«åˆ’å…¥åŒä¸€ä¸ª NCCL é€šä¿¡ç»„ã€‚
- åˆ›å»ºå…¨å±€çš„ `_PP`(Pipeline Parallelism) è¿›ç¨‹ç»„ï¼Œä¸ºæ¯ä¸ªæµæ°´çº¿ stage åˆ›å»º 1 ä¸ªç‹¬ç«‹çš„é€šä¿¡ group

| i   | ranks = range(i, 8, 4) | PP Stage |
| --- | ---------------------- | -------- |
| 0   | 0,4                    | [0,4]    |
| 1   | 1,5                    | [1,5]    |
| 2   | 2,6                    | [2,6]    |
| 3   | 3,7                    | [3,7]    |
|     |                        |          |
### Detokenizer

å®é™…ä¸Š detokenizer ä¼šä¸€ç›´äº‹ä»¶å¾ªç¯ï¼Œä» Scheduler å¾—åˆ° TODOï¼Œè§£ç æˆ `BatchTokenIDOutput` ä¼ é€’ç»™ Tokenizer å­è¿›ç¨‹

```python
def event_loop(self):
"""The event loop that handles requests"""
	while True:
		recv_obj = self.recv_from_scheduler.recv_pyobj()
		output = self._request_dispatcher(recv_obj)
		if output is not None:
			self.send_to_tokenizer.send_pyobj(output)
```

### Parallel Linear in SGLang

åœ¨ SGLangä¸­ï¼Œ`Parallel Linear`Â æ˜¯å®ç° Tensor Parallel (TP) çš„æ ¸å¿ƒç»„ä»¶ã€‚å®ƒé€šè¿‡å°†å·¨å¤§çš„æƒé‡çŸ©é˜µåˆ‡åˆ†åˆ°å¤šä¸ª GPU ä¸Šï¼Œä½¿å¾—æ¯ä¸ª GPU åªéœ€è®¡ç®—ä¸€éƒ¨åˆ†æ•°æ®ï¼Œä»è€Œå‡å°‘æ˜¾å­˜å ç”¨å¹¶åŠ é€Ÿè®¡ç®—ã€‚

ä¸»è¦æœ‰ä¸¤ç§å¹¶è¡Œæ–¹å¼ï¼š**Column Parallel (åˆ—å¹¶è¡Œ)**Â å’ŒÂ **Row Parallel (è¡Œå¹¶è¡Œ)**ã€‚é€šå¸¸å®ƒä»¬ä¼šæˆå¯¹å‡ºç°ï¼ˆä¾‹å¦‚åœ¨ MLP ä¸­ï¼šå…ˆ Column å Rowï¼‰ï¼Œä»¥æœ€å°åŒ–é€šä¿¡å¼€é”€ã€‚

#### ColumnParallelLinear

**æ•°å­¦åŸç†**ï¼š
å‡è®¾çº¿æ€§å±‚è¿ç®—ä¸º $Y =XA$ 
æˆ‘ä»¬å°†æƒé‡çŸ©é˜µÂ $A$Â æŒ‰**åˆ—**åˆ‡åˆ†ä¸ºÂ $[A_1â€‹,A_2â€‹,...,A_pâ€‹]$ã€‚  
æ¯ä¸ª GPU æŒæœ‰Â $A_i$â€‹ã€‚  
è¾“å…¥Â $X$Â æ˜¯å®Œæ•´çš„ï¼ˆå¤åˆ¶åœ¨æ‰€æœ‰ GPU ä¸Šï¼‰ã€‚  
æ¯ä¸ª GPU è®¡ç®—Â $Y_i=XA_i$â€‹ã€‚  
è¾“å‡ºÂ $Y$è¢« åˆ‡åˆ†ä¸º $[Y_1,Y_2,...,Y_p]$ï¼Œå³æ¯ä¸ª GPU å¾—åˆ°è¾“å‡ºå‘é‡çš„ä¸€éƒ¨åˆ†ç‰¹å¾ã€‚

**å…¸å‹åº”ç”¨**:

- Attention çš„ QKV Projection (`QKVParallelLinear`)ã€‚
- MLP çš„ Gate / Up Projection (`MergedColumnParallelLinear`)ã€‚

#### RowParallelLinear

**æ•°å­¦åŸç†**ï¼š
å‡è®¾çº¿æ€§å±‚è¿ç®—ä¸º $Y =XA$ ã€‚  
æˆ‘ä»¬å°†æƒé‡çŸ©é˜µÂ $A$Â æŒ‰**è¡Œ**åˆ‡åˆ†ä¸º $$\begin{bmatrix} A_1 \\ A_2 \\ \dots \\ A_p \end{bmatrix}  $$â€‹â€‹â€‹  
ä¸ºäº†åŒ¹é…çŸ©é˜µä¹˜æ³•è§„åˆ™ï¼Œè¾“å…¥Â $X$Â ä¹Ÿå¿…é¡»æŒ‰**åˆ—**åˆ‡åˆ†ä¸ºÂ $[X_1â€‹,X_2â€‹,...,X_pâ€‹]$ï¼ˆè¿™æ­£å¥½æ˜¯ ColumnParallelLinear çš„è¾“å‡ºæ ¼å¼ï¼‰ã€‚  
æ¯ä¸ª GPU è®¡ç®—Â $Y_i=XA_i$â€‹ã€‚æ³¨æ„ï¼Œ$Y_i$ çš„å½¢çŠ¶ä¸æœ€ç»ˆè¾“å‡ºÂ $Y$Â ç›¸åŒï¼Œä½†å®ƒåªæ˜¯éƒ¨åˆ†å’Œã€‚  
æœ€ç»ˆè¾“å‡ºÂ $Y = \sum Y_i$â€‹ ï¼Œéœ€è¦ä¸€æ¬¡Â **All-Reduce (Sum)**Â æ“ä½œ

**å…¸å‹åº”ç”¨**ï¼š
- Attention çš„ Output Projection (`o_proj`)ã€‚
- MLP çš„ Down Projection (`down_proj`)ã€‚

#### ColumnParallelLinear + RowParallelLinear

 **è®¡ç®—é‡å‡åŠï¼Œå¤šäº†ä¸€æ¬¡é›†ç¾¤é€šä¿¡ï¼ˆallReduceï¼‰ï¼Œä¸­é—´å€¼çš„å­˜å‚¨å¤§å°å‡åŠï¼ŒInput, Weight å‡åŠ**

![](img/llm_dp.jpg)
### Qwen2 Model


æ¯ä¸ª GPU è¿›ç¨‹éƒ½ä¼šå®ä¾‹åŒ–ä¸€ä¸ªÂ `Qwen2ForCausalLM`Â å¯¹è±¡ï¼Œä½†æ ¹æ®å…¶æ‰€åœ¨çš„Â **PP Rank**Â å’ŒÂ **TP Rank**ï¼ŒåŠ è½½çš„å†…å®¹ä¸åŒ

#### Embedding å±‚     
- åªæœ‰Â **PP Rank 0**Â çš„è¿›ç¨‹ä¼šåˆå§‹åŒ– `VocabParallelEmbedding`(**Row Parallel**)ã€‚
- **TP å¤„ç†**: è¯è¡¨ (Vocab) è¢«åˆ‡åˆ†åˆ° TP ç»„çš„å„ä¸ª GPU ä¸Šã€‚æ¯ä¸ª GPU åªæŒæœ‰Â `VocabSize / TP_Size`Â å¤§å°çš„æƒé‡ã€‚
- **å…¶ä»– PP Rank**: åˆå§‹åŒ–ä¸º PPMissingLayerÂ (å ä½ç¬¦ï¼Œä¸å ç”¨æ˜¾å­˜)ã€‚

```python
# perform weight tying for PP
if self.pp_group.world_size > 1 and config.tie_word_embeddings:
	if self.pp_group.is_first_rank:
		self.pp_group.send(
			self.model.embed_tokens.weight, dst=self.pp_group.last_rank
		)
	elif self.pp_group.is_last_rank:
		emb_token_weight = self.pp_group.recv(
			size=(config.vocab_size, config.hidden_size),
			dtype=next(self.model.parameters()).dtype,
			src=self.pp_group.first_rank,
		)
		self.lm_head.weight.copy_(emb_token_weight)
```

#### Transformer Layers (æ‰€æœ‰ PP Rank)
- ä½¿ç”¨ `make_layers` è¿›è¡Œæ„å»ºã€‚**æ‰€æœ‰å±‚éƒ½ä¼šæ„å»ºï¼Œåªæœ‰æœ¬åœ°å±‚ä¼šåŠ è½½æƒé‡(å³å ç”¨æ˜¾å­˜)**
- **PP åˆ‡åˆ†**: æ€»å±‚æ•°ï¼ˆä¾‹å¦‚ 32 å±‚ï¼‰ä¼šè¢«å‡åŒ€åˆ†é…ç»™ PP ç»„çš„å„ä¸ª Rankã€‚
	- ä¾‹å¦‚ PP=4ï¼ŒRank 0 è´Ÿè´£ 0-7 å±‚ï¼ŒRank 1 è´Ÿè´£ 8-15 å±‚ï¼Œä»¥æ­¤ç±»æ¨ã€‚
- **æœ¬åœ°å±‚**: å½“å‰ Rank åªå®é™…åˆå§‹åŒ–å®ƒè´Ÿè´£çš„é‚£éƒ¨åˆ†Â `Qwen2DecoderLayer`ã€‚
- **ç¼ºå¤±å±‚**: ä¸å±äºå½“å‰ Rank çš„å±‚è¢«åˆå§‹åŒ–ä¸ºÂ `PPMissingLayer`ã€‚

```python
modules = torch.nn.ModuleList(
	[PPMissingLayer(return_tuple=return_tuple) for _ in range(start_layer)]
	+ get_offloader().wrap_modules(
		(
			layer_fn(idx=idx, prefix=add_prefix(idx, prefix))
			for idx in range(start_layer, end_layer)
		),
		**(offloader_kwargs or {}),
	)
	+ [
		PPMissingLayer(return_tuple=return_tuple)
		for _ in range(end_layer, num_hidden_layers)
	]
)
```

Qwen2DecodeLayer å®é™…ä¸Šä¸»è¦æ˜¯ç”± Qwen2Attentionï¼ŒQwen2MLPï¼ŒRMSNorm ç»„æˆçš„
1. **Qwen2Attention**:

	- **è¾“å…¥ -> ä¸­é—´**: ä½¿ç”¨Â QKVParallelLinearÂ (ç»§æ‰¿è‡ªÂ **ColumnParallelLinear**)ã€‚å°†è¾“å…¥æŠ•å½±åˆ° Q, K, Vã€‚
	- **ä¸­é—´ -> è¾“å‡º**: ä½¿ç”¨Â RowParallelLinearã€‚å°† Attention çš„è¾“å‡ºæŠ•å½±å› hidden sizeã€‚
2. **Qwen2MLP**:
    - **è¾“å…¥ -> ä¸­é—´**: ä½¿ç”¨Â MergedColumnParallelLinearÂ (ç»§æ‰¿è‡ªÂ **ColumnParallelLinear**)ã€‚å°†è¾“å…¥æŠ•å½±åˆ° Gate å’Œ Up çŠ¶æ€ã€‚
    - **ä¸­é—´ -> è¾“å‡º**: ä½¿ç”¨Â RowParallelLinearã€‚å°†æ¿€æ´»åçš„çŠ¶æ€æŠ•å½±å› hidden sizeã€‚
#### LMHead å±‚ 
æ˜¯ä¸€ä¸ªÂ **æŒ‰è¯è¡¨ç»´åº¦åˆ‡åˆ† (Column Parallel)**Â çš„çº¿æ€§å±‚
- åªæœ‰Â **æœ€åä¸€ä¸ª PP Rank**Â ä¼šåˆå§‹åŒ–Â `RMSNorm`Â å’ŒÂ `ParallelLMHead`ã€‚
- **å…¶ä»– PP Rank**: åˆå§‹åŒ–ä¸ºÂ PPMissingLayerã€‚

---

### Inference Process

å½“ä¸€ä¸ª Batch çš„è¯·æ±‚åˆ°æ¥æ—¶ï¼Œæ•°æ®æµä¼šåœ¨ GPU ä¹‹é—´é€šè¿‡æµæ°´çº¿ä¼ é€’ã€‚

#### A. è¾“å…¥å¤„ç†

- **è¯·æ±‚åˆ†å‘**ï¼šå½“è¯·æ±‚è¿›å…¥ç³»ç»Ÿæ—¶ï¼ŒScheduler ä¼šå°†è¯·æ±‚å¯¹è±¡ï¼ˆåŒ…å«Â `input_ids`ï¼‰å¹¿æ’­ç»™æ‰€æœ‰çš„ Rankï¼ˆåŒ…æ‹¬ä¸åŒçš„ PP Rank å’Œ TP Rankï¼‰ã€‚
- **Batch æ„å»º**ï¼šæ¯ä¸ª Rank ä¸Šçš„ Scheduler éƒ½ä¼šç‹¬ç«‹æ„å»ºÂ `ForwardBatch`Â å¯¹è±¡ã€‚

åªæœ‰ PP Rank 0 ä½¿ç”¨Â `input_ids`Â è¿›è¡Œè®¡ç®—ï¼Œå¯¹äº PP Rank > 0 çš„èŠ‚ç‚¹ï¼Œå®ƒä»¬çš„è¾“å…¥æ•°æ®ä¸æ˜¯æ¥è‡ªÂ `input_ids`ï¼Œè€Œæ˜¯æ¥è‡ª**ä¸Šä¸€ä¸ª Rank é€šè¿‡ P2P é€šä¿¡å‘é€çš„ä¸­é—´ç»“æœ**ã€‚
```python
if self.pp_group.is_first_rank:
	# Rank 0: çœŸæ­£ä½¿ç”¨ input_ids è¿›è¡Œ Embedding æŸ¥æ‰¾
	if input_embeds is None:
		hidden_states = self.embed_tokens(input_ids)
	else:
		hidden_states = input_embeds
	residual = None
else:
	# Rank > 0: å¿½ç•¥ input_idsï¼Œç›´æ¥ä½¿ç”¨ä¸Šä¸€ä¸ªé˜¶æ®µä¼ è¿‡æ¥çš„ hidden_states
	assert pp_proxy_tensors is not None
	hidden_states = pp_proxy_tensors["hidden_states"]
	residual = pp_proxy_tensors["residual"]
```

#### B. æµæ°´çº¿ä¼ é€’ (Pipeline Forward)

æµç¨‹æŒ‰ PP Rank é¡ºåºä¾æ¬¡æ‰§è¡Œï¼š

1. **PP Rank 0 (èµ·å§‹é˜¶æ®µ)**:
    
    - **Embedding**:
        - è¾“å…¥Â `input_ids`ã€‚
        - æ‰§è¡ŒÂ `VocabParallelEmbedding`ã€‚
	        -  æ¯ä¸ª GPU éƒ½åœ¨å…¶æœ¬åœ°çš„å°æƒé‡çŸ©é˜µä¸Šè¿›è¡ŒæŸ¥è¡¨æ“ä½œã€‚
			- å¯¹äºå±äºè¯¥ GPU çš„ Tokenï¼ŒæŸ¥å‡ºçš„å‘é‡æ˜¯æ­£ç¡®çš„ã€‚
			- å¯¹äºä¸å±äºè¯¥ GPU çš„ Tokenï¼ŒæŸ¥å‡ºçš„å‘é‡æ˜¯æ— æ„ä¹‰çš„ï¼ˆåƒåœ¾å€¼ï¼‰ã€‚
        - **TP åŠ¨ä½œ**: å„ TP Rank è®¡ç®—éƒ¨åˆ† Embeddingï¼Œç„¶åé€šè¿‡Â **AllReduce**Â èšåˆï¼Œä½¿å¾—æ¯ä¸ª TP Rank è·å¾—å®Œæ•´çš„ Embedding å‘é‡ã€‚
	        ```python
	        output_parallel = tensor_model_parallel_all_reduce(output_parallel)
	        ```
    - **Layers (0 ~ N)**:
        - é¡ºåºæ‰§è¡Œåˆ†é…ç»™è¯¥ Rank çš„ Transformer å±‚ã€‚
    - **è¾“å‡º**: å°†è®¡ç®—å‡ºçš„Â `hidden_states`Â å‘é€ç»™ PP Rank 1ã€‚
1. **PP Rank i (ä¸­é—´é˜¶æ®µ)**:
    
    - **è¾“å…¥**: é€šè¿‡ P2PÂ `recv`Â æ¥æ”¶ä¸Šä¸€çº§ (Rank i-1) å‘æ¥çš„Â `hidden_states`ã€‚
    - **Layers (M ~ K)**:
        - é¡ºåºæ‰§è¡Œåˆ†é…ç»™è¯¥ Rank çš„ Transformer å±‚ã€‚
    - **è¾“å‡º**: å°†æ›´æ–°åçš„Â `hidden_states`Â å‘é€ç»™ä¸‹ä¸€çº§ (Rank i+1)ã€‚
2. **PP Rank Last (æœ€ç»ˆé˜¶æ®µ)**:
    
    - **è¾“å…¥**: æ¥æ”¶ä¸Šä¸€çº§å‘æ¥çš„Â `hidden_states`ã€‚
    - **Layers (X ~ End)**: æ‰§è¡Œå‰©ä½™çš„ Transformer å±‚ã€‚
    - **Final Norm**: æ‰§è¡ŒÂ `RMSNorm`ã€‚
    - **Logits**:
        - æ‰§è¡ŒÂ `ParallelLMHead`ã€‚
        - **TP åŠ¨ä½œ**: ç±»ä¼¼äº Embeddingï¼Œè¾“å‡º Logits é€šå¸¸éœ€è¦èšåˆï¼ˆæˆ–è€…åœ¨é‡‡æ ·æ—¶å¤„ç†ï¼‰ã€‚
    - **é‡‡æ ·**: è®¡ç®—æ¦‚ç‡å¹¶é‡‡æ ·ä¸‹ä¸€ä¸ª Tokenã€‚

#### C. å±‚å†…å¹¶è¡Œç»†èŠ‚ (Inside a Layer with TP)

åœ¨æ¯ä¸€ä¸ª Transformer å±‚å†…éƒ¨ï¼ŒTP æ˜¯è¿™æ ·å·¥ä½œçš„ï¼š

1. **Qwen2Attention**:
    
    - **è¾“å…¥**: å®Œæ•´çš„Â `hidden_states`Â (æ‰€æœ‰ TP Rank éƒ½æœ‰å‰¯æœ¬)ã€‚
    - **QKV Proj (Column Parallel)**: æ¯ä¸ª TP Rank åªè®¡ç®—ä¸€éƒ¨åˆ† Head çš„ Q/K/Vã€‚
    - **Attention è®¡ç®—**: æ¯ä¸ª Rank ç‹¬ç«‹è®¡ç®—è‡ªå·±é‚£éƒ¨åˆ† Head çš„ Attentionã€‚
    - **Output Proj (Row Parallel)**: æ¯ä¸ª Rank è®¡ç®—éƒ¨åˆ†è¾“å‡ºã€‚
    - **AllReduce**: åœ¨ Output Proj ä¹‹åï¼Œæ‰§è¡Œä¸€æ¬¡Â **AllReduce (Sum)**ï¼Œè®©æ‰€æœ‰ TP Rank é‡æ–°è·å¾—å®Œæ•´çš„ Attention è¾“å‡ºï¼Œå¹¶åŠ åˆ° Residual ä¸Šã€‚
2. **Qwen2MLP**:
    
    - **Gate/Up Proj (Column Parallel)**: è¾“å…¥å®Œæ•´ï¼Œè¾“å‡ºè¢«åˆ‡åˆ†ï¼ˆæ¯ä¸ª Rank è®¡ç®—ä¸€éƒ¨åˆ†ä¸­é—´ç‰¹å¾ï¼‰ã€‚
    - **Activation**: åœ¨åˆ‡åˆ†çš„æ•°æ®ä¸Šç‹¬ç«‹æ‰§è¡Œ (å¦‚ SiLU)ã€‚
    - **Down Proj (Row Parallel)**: è¾“å…¥æ˜¯åˆ—åˆ‡åˆ†çš„ï¼ˆæ¥è‡ª Activation çš„è¾“å‡ºï¼‰ï¼Œæƒé‡æŒ‰è¡Œåˆ‡åˆ†ã€‚æ¯ä¸ª Rank è®¡ç®—å‡ºéƒ¨åˆ†è¾“å‡ºã€‚
    - **AllReduce**: æ‰§è¡ŒÂ **AllReduce (Sum)**ï¼Œèšåˆæœ€ç»ˆç»“æœã€‚
![](img/llm_dp.jpg)