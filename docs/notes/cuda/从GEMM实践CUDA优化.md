---
title: ä» GEMM å®è·µ CUDA ä¼˜åŒ–
created: 2025-12-8
tags:
  - LLMInference
---

# ä» GEMM å®è·µ CUDA ä¼˜åŒ–

ğŸ“š æœ¬æ–‡å°†ä»æœ€ naive çš„ GEMM å®ç°å¼€å§‹ï¼Œä½¿ç”¨ nsight compute å·¥å…·è¿›è¡Œæ€§èƒ½åˆ†æå¯»æ‰¾ç“¶é¢ˆå¹¶ä¸€æ­¥æ­¥è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼æ¥å®è·µ CUDA ä¸­çš„å„ç§ä¼˜åŒ–æŠ€å·§ã€‚

## GEMM ç®€ä»‹

General Matrix-Matrix Multiplicationï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰ã€‚å®ƒæ˜¯ BLAS (Basic Linear Algebra Subprograms) æ ‡å‡†åº“ä¸­å®šä¹‰çš„â€œç¬¬ä¸‰çº§â€ï¼ˆLevel 3ï¼‰è¿ç®—ã€‚å…¶æ ‡å‡†æ•°å­¦å®šä¹‰å¦‚ä¸‹ï¼š
$$C \leftarrow \alpha AB + \beta C$$
å…¶ä¸­ï¼š
- $A, B, C$ æ˜¯çŸ©é˜µã€‚
- $\alpha, \beta$ æ˜¯æ ‡é‡ï¼ˆScalarï¼‰ã€‚
- é€šå¸¸å‡è®¾ $A$ çš„ç»´åº¦ä¸º $M \times K$ï¼Œ$B$ çš„ç»´åº¦ä¸º $K \times N$ï¼Œåˆ™ $C$ çš„ç»´åº¦ä¸º $M \times N$ã€‚

ä¸ºä»€ä¹ˆ GEMM å¦‚æ­¤é‡è¦ï¼Ÿ
- æ·±åº¦å­¦ä¹ çš„æ ¸å¿ƒï¼š å…¨è¿æ¥å±‚ï¼ˆDense/Linearï¼‰ç›´æ¥å°±æ˜¯çŸ©é˜µä¹˜æ³•ï¼›å·ç§¯å±‚ï¼ˆConvolutionï¼‰é€šè¿‡ Im2Col ç­‰å˜æ¢åï¼Œæœ¬è´¨ä¸Šä¹Ÿæ˜¯ GEMMã€‚
- è®¡ç®—å¯†é›†å‹ï¼š å®ƒæ˜¯**å…¸å‹çš„è®¡ç®—å¯†é›†å‹**ä»»åŠ¡ï¼ˆCompute Boundï¼‰ï¼Œè¿ç®—å¤æ‚åº¦ä¸º $O(N^3)$ï¼Œè€Œæ•°æ®æ¬è¿å¤æ‚åº¦ä¸º $O(N^2)$ã€‚è¿™æ„å‘³ç€ä¼˜åŒ–å¾—å½“ï¼Œå¯ä»¥æé«˜åœ°åˆ©ç”¨ç¡¬ä»¶å³°å€¼ç®—åŠ›ã€‚



## ä¼˜åŒ–æ–¹æ³•
### Naive GEMM
Naive GEMM ä¸€å…±ä½¿ç”¨M * Nä¸ªçº¿ç¨‹å®Œæˆæ•´ä¸ªçŸ©é˜µä¹˜æ³•ï¼Œæ¯ä¸ªçº¿ç¨‹è®¡ç®—ç»“æœçŸ©é˜µ $C$ çš„ä¸€ä¸ªå…ƒç´ ã€‚ä¸‹é¢æ˜¯ä¸€ä¸ªç®€å•çš„ FP32 GEMM CUDA kernel å®ç°ï¼š
```cpp
// FP32
// GEMM naive: compute one c[i,j]
// element per threads, all row major
__global__ void gemm_fp32_kernel(const float *a, const float *b, float *c, int M, int N, int K) {
  int n = blockIdx.x * blockDim.x + threadIdx.x;
  int m = blockIdx.y * blockDim.y + threadIdx.y;

  if (m < M && n < N) {
    float psum = 0.0;
#pragma unroll
    for (int k = 0; k < K; k++) {
      // m row in a matrix, n col in b matrix
      psum += a[m * K + k] * b[k * N + n];
    }
    c[m * N + n] = psum;  // c[m,n]
  }
}
```
![](img/naivegemm.jpg)

### Tiling ä¼˜åŒ–

- **Block Tile**: æ¯ä¸ªBlockè´Ÿè´£è®¡ç®—çŸ©é˜µCä¸­çš„$BM \times BN$ä¸ªå…ƒç´ ï¼›
- **K Tile**: å¾ªç¯$K / BK$æ¬¡ï¼Œä¸€ä¸ªBlockçš„æ‰€æœ‰çº¿ç¨‹æ¯æ¬¡ä¸€èµ·ä»Global Memoryä¸­load $BM \times BK$ä¸ªçŸ©é˜µAçš„å…ƒç´ å’Œ$BK \times BN$ä¸ªçŸ©é˜µBçš„å…ƒç´ ï¼›
- **Thread Tile**: æ¯ä¸ªThreadè´Ÿè´£è®¡ç®—çŸ©é˜µCä¸­çš„$TM \times TN$ä¸ªå…ƒç´ ã€‚
- **Shared Memory**: æ¯ä¸ªBlockåˆ†é…Shared Memoryæ¥å­˜å‚¨ä»Global MemoryåŠ è½½çš„çŸ©é˜µAå’ŒçŸ©é˜µBçš„Tileæ•°æ®ï¼Œå‡å°‘å¯¹Global Memoryçš„è®¿é—®æ¬¡æ•°ï¼Œæé«˜æ•°æ®é‡ç”¨ç‡ã€‚


![](img/tile_gemm.jpg)

#### åˆ†æ

- Block Level (çº¿ç¨‹å—çº§):
  - $BM \times BN$: ä¸€ä¸ª CUDA Block è´Ÿè´£è®¡ç®— $C$ çŸ©é˜µä¸­å¤§å°ä¸º $BM \times BN$ çš„ä¸€ä¸ªå­å—ã€‚
  - $BK$: æ¯æ¬¡å¾ªç¯ï¼ˆKç»´åº¦ï¼‰åŠ è½½åˆ° Shared Memory ä¸­çš„æ­¥é•¿ã€‚
  - **ç›®çš„**: åˆ©ç”¨ Shared Memory å¤ç”¨æ•°æ®ï¼Œå‡å°‘ Global Memory è®¿é—®ã€‚
- Thread Level (çº¿ç¨‹çº§):
  - $TM \times TN$: ä¸€ä¸ª CUDA Thread è´Ÿè´£è®¡ç®— Block å­å—ä¸­å¤§å°ä¸º $TM \times TN$ çš„å¾®å°å—ã€‚
  - **ç›®çš„**: åˆ©ç”¨ Register (å¯„å­˜å™¨) å¤ç”¨æ•°æ®ã€‚å¯„å­˜å™¨æ¯” Shared Memory æ›´å¿«ï¼Œä¸”æ˜¯é›¶å»¶è¿Ÿçš„

è®¡ç®—è®¿å­˜æ¯”(AI)
$$Loads = ( \frac{1}{BN} + \frac{1}{BM} ) \times MNK$$
- è®¡ç®—é‡: $2MNK$ (æµ®ç‚¹è¿ç®—æ¬¡æ•°) æ˜¯å›ºå®šçš„ã€‚
- è®¿å­˜é‡: éšç€ $BM$ å’Œ $BN$ å¢å¤§ï¼Œæ€»è®¿å­˜æ¬¡æ•°ä¸‹é™ã€‚

æˆ‘ä»¬å¸Œæœ›å°½å¯èƒ½å¢å¤§ $BM$ å’Œ $BN$ï¼Œä»¥æé«˜è®¡ç®—è®¿å­˜æ¯” (AI)ï¼Œä»è€Œæå‡æ€§èƒ½ï¼›ä¹Ÿå¸Œæœ›å¢å¤§ $TM$ å’Œ $TN$ï¼Œä»¥æé«˜å¯„å­˜å™¨æ•°æ®å¤ç”¨ç‡ã€‚ä½†æ˜¯ï¼Œå¢å¤§è¿™äº›å‚æ•°ä¼šå¢åŠ  Shared Memory å’Œå¯„å­˜å™¨çš„ä½¿ç”¨é‡ï¼Œä»è€Œé™ä½å¹¶å‘åº¦ (Occupancy)ã€‚å› æ­¤ï¼Œéœ€è¦åœ¨è®¡ç®—è®¿å­˜æ¯”å’Œå¹¶å‘åº¦ä¹‹é—´è¿›è¡Œæƒè¡¡ã€‚

Occupancy å—ä»¥ä¸‹å› ç´ å½±å“ï¼š
- **Shared Memory å®¹é‡** (é™åˆ¶ BM, BN, BK)
   - å…¬å¼ï¼š$Bytes = BK \times (BM + BN) \times 4 \text{ (float size)}$
   - Double Buffering: ä¸ºäº†æ©ç›–è®¿å­˜å»¶è¿Ÿï¼Œé€šå¸¸ä½¿ç”¨åŒç¼“å†²ï¼ˆåœ¨è®¡ç®—å½“å‰å—æ—¶åŠ è½½ä¸‹ä¸€å—ï¼‰ï¼Œè¿™ä¼šå¯¼è‡´éœ€æ±‚ç¿»å€ï¼š$$Bytes_{Total} = 2 \times BK \times (BM + BN) \times 4$$
- **å¯„å­˜å™¨å‹åŠ›** (Register Pressure) (é™åˆ¶ TM, TN)
  - ç´¯åŠ å™¨æ¶ˆè€—: æ¯ä¸ªçº¿ç¨‹å¿…é¡»ç‹¬è‡ªç»´æŠ¤ $TM \times TN$ ä¸ª $C$ çŸ©é˜µçš„å…ƒç´ ã€‚è¿™äº›å…ƒç´ å¿…é¡»ä¸€ç›´é©»ç•™åœ¨å¯„å­˜å™¨ä¸­ç›´åˆ°è®¡ç®—ç»“æŸã€‚
  - å¦‚æœ $TM=8, TN=8$ï¼Œä»… $C$ çš„éƒ¨åˆ†å’Œå°±éœ€è¦ 64ä¸ªå¯„å­˜å™¨ã€‚
  - åŠ ä¸ŠåŠ è½½ $A, B$ çš„ä¸´æ—¶å¯„å­˜å™¨ã€ç´¢å¼•è®¡ç®—ã€å¾ªç¯å˜é‡ï¼Œä¸€ä¸ªçº¿ç¨‹å¯èƒ½æ¶ˆè€— 80-100ä¸ªå¯„å­˜å™¨ã€‚
- **çº¿ç¨‹å¹¶è¡Œåº¦ (Thread Level Parallelism)**   
  - å…¬å¼ï¼š$ThreadsPerBlock = \frac{BM \times BN}{TM \times TN}$
  - ç¡¬é™åˆ¶: CUDA è§„å®šä¸€ä¸ª Block æœ€å¤š 1024 ä¸ªçº¿ç¨‹ã€‚
  - è½¯é™åˆ¶: çº¿ç¨‹æ•°æœ€å¥½æ˜¯ 32 (Warp Size) çš„å€æ•°ã€‚

```cpp
// gemm: Block Tile + Thread Tile + K Tile + Vec4, with smem
// BK:TILE_K=8 BM=BN=128
// TM=TN=8 å¢åŠ è®¡ç®—å¯†åº¦ BM/TM=16 BN/TN=16
// dim3 blockDim(BN/TN, BM/TM);
// dim3 gridDim((N + BN - 1) / BN, (M + BM - 1) / BM)
template <const int BM = 128, const int BN = 128, const int BK = 8, const int TM = 8,
          const int TN = 8>
__global__ void gemm_t_8x8_sliced_k_f32x4_kernel(float *a, float *b, float *c, int M, int N,
                                                 int K) {
  // [1]  Block Tile: ä¸€ä¸ª16x16çš„blockå¤„ç†Cä¸Šå¤§å°ä¸º128X128çš„ä¸€ä¸ªç›®æ ‡å—
  // [2] Thread Tile: æ¯ä¸ªthreadè´Ÿè´£è®¡ç®—TM*TN(8*8)ä¸ªå…ƒç´ ï¼Œå¢åŠ è®¡ç®—å¯†åº¦
  // [3]      K Tile: å°†Kåˆ†å—ï¼Œæ¯å—BKå¤§å°ï¼Œè¿­ä»£(K+BK-1/BK)æ¬¡ï¼Œ
  //                  æ¯æ¬¡è®¡ç®—TM*TNä¸ªå…ƒç´ å„è‡ªçš„éƒ¨åˆ†ä¹˜ç´¯åŠ 
  // [4]   Vectorize: å‡å°‘loadå’ŒstoreæŒ‡ä»¤ï¼Œä½¿ç”¨float4
  int bx = blockIdx.x;
  int by = blockIdx.y;
  int tx = threadIdx.x;
  int ty = threadIdx.y;
  int tid = threadIdx.y * blockDim.x + tx;  // tid within the block
  __shared__ float s_a[BM][BK], s_b[BK][BN];

  // 0. å…ˆè®¡ç®—shared memoryä¸­çš„ç´¢å¼•
  // 1024 ä¸ªå…ƒç´ ï¼Œæ¯ä¸ªçº¿ç¨‹è¯»4ä¸ªå…ƒç´ ï¼Œéœ€è¦256ä¸ªçº¿ç¨‹
  int load_smem_a_m = tid / 2;                 
  int load_smem_a_k = (tid % 2 == 0) ? 0 : 4;  
  int load_smem_b_k = tid / 32;         
  int load_smem_b_n = (tid % 32) * 4;  
  // 1. å†è®¡ç®—å…¨å±€å†…å­˜ä¸­çš„ç´¢å¼•
  int load_gmem_a_m = by * BM + load_smem_a_m;  // global row of a and c
  int load_gmem_b_n = bx * BN + load_smem_b_n;  // global col of b and c

  float r_c[TM][TN];
  for (int bk = 0; bk < (K + BK - 1) / BK; ++bk) {
    // 2. è®¡ç®—æ¯æ¬¡åŠ è½½åˆ°smemçš„Aå’ŒBçŸ©é˜µå…ƒç´ åœ¨å…¨å±€å†…å­˜ä¸­çš„åˆ—æ•°å’Œè¡Œæ•°
    int load_gmem_a_k = bk * BK + load_smem_a_k;
    int load_gmem_a_addr = load_gmem_a_m * K + load_gmem_a_k;
    FLOAT4(s_a[load_smem_a_m][load_smem_a_k]) =
        FLOAT4(a[load_gmem_a_addr]);  // load A to shared memory

    int load_gmem_b_k = bk * BK + load_smem_b_k;
    int load_gmem_b_addr = load_gmem_b_k * N + load_gmem_b_n;
    FLOAT4(s_b[load_smem_b_k][load_smem_b_n]) =
        FLOAT4(b[load_gmem_b_addr]);  // load B to shared memory
    __syncthreads();

    // 3. è®¡ç®—çº¿ç¨‹è´Ÿè´£çš„TMxTNä¸ªå…ƒç´ çš„éƒ¨åˆ†ä¹˜åŠ 
#pragma unroll
    for (int k = 0; k < BK; ++k) {
#pragma unroll
      for (int m = 0; m < TM; ++m) {
#pragma unroll
        for (int n = 0; n < TN; ++n) {
          int comp_smem_a_m = ty * TM + m;
          int comp_smem_b_n = tx * TN + n;
          r_c[m][n] += s_a[comp_smem_a_m][k] * s_b[k][comp_smem_b_n];
        }
      }
    }
    __syncthreads();
  }

// 4. store output
#pragma unroll
  for (int m = 0; m < TM; ++m) {
    int store_gmem_c_m =
        by * BM + ty * TM +
        m;  // å¤§åˆ†å—çš„èµ·å§‹è¡Œå·by * BM + å°åˆ†å—çš„èµ·å§‹è¡Œå·ty * TM + å°åˆ†å—å†…éƒ¨çš„ç›¸å¯¹è¡Œå· m
#pragma unroll
    for (int n = 0; n < TN; ++n) {
      int store_gmem_c_n =
          bx * BN + tx * TN +
          n;  // å¤§åˆ†å—çš„èµ·å§‹åˆ—å·bx * BN + å°åˆ†å—çš„èµ·å§‹åˆ—å·tx * TN + å°åˆ†å—å†…éƒ¨çš„ç›¸å¯¹åˆ—å· n
      int store_gmem_c_addr = store_gmem_c_m * N + store_gmem_c_n;
      c[store_gmem_c_addr] = r_c[m][n];
    }
  }
}
```

### Free Bank Conflict ä¼˜åŒ–
#### Bank Conflict ä»‹ç»
Cuda shared memoryæŒ‰ç…§4å­—èŠ‚ä¸€ä¸ªbankï¼Œæ€»å…±32ä¸ªbankï¼ˆ128å­—èŠ‚ï¼‰æ¥ç»„ç»‡ï¼Œå…¶storeå’Œloadæ“ä½œåœ¨ä¸€å®šæƒ…å†µä¸‹å­˜åœ¨bank conflictçš„æƒ…å†µï¼š

- ä¸åŒçš„çº¿ç¨‹è®¿é—®åŒä¸€bankçš„ä¸åŒaddressæ—¶å°±ä¼šå‡ºç°bank conflictã€‚
- bank conflictåªå‘ç”Ÿåœ¨**åŒä¸€ä¸ªwarpçš„ä¸åŒçº¿ç¨‹**é—´ã€‚
- å¦‚æœå¤šä¸ªçº¿ç¨‹è®¿é—®shared memoryçš„ç›¸åŒbankçš„ç›¸åŒaddressï¼Œå®é™…æ•ˆæœæ˜¯broadcastï¼Œébank conflictã€‚
- bank conflictåªå‘ç”Ÿåœ¨shared memoryçš„è¯»å†™æ“ä½œä¸Šï¼Œglobal memoryçš„è¯»å†™æ“ä½œä¸ä¼šæœ‰bank conflictäº§ç”Ÿã€‚

> [!NOTE]
> åœ¨æŸäº›æƒ…å†µä¸‹ï¼Œbank conflictçš„å‘ç”Ÿæ˜¯åœ¨warpä¸­å¤„äºåŒä¸€ä¸ªphaseçš„ä¸åŒthreadsä¹‹é—´ï¼Œè¿™é‡Œçš„phaseæ˜¯æŒ‡warpçš„32ä¸ªçº¿ç¨‹æ“ä½œshared memoryæ—¶ï¼Œåˆ†å¤šä¸ªphaseï¼Œæ¯ä¸ªphaseçš„å‚ä¸çº¿ç¨‹ä¸ä¸€æ ·ã€‚
>
> æ¯”å¦‚ldmatrixæŒ‡ä»¤ldmatrix.sync.aligned.x4.m8n8.shared.b16ï¼Œè¯¥æŒ‡ä»¤ä»å…±äº«å†…å­˜åŠ è½½æ•°æ®åˆ°çº¿ç¨‹å¯„å­˜å™¨ï¼Œæ“ä½œåˆ†4ä¸ªphaseï¼Œåœ¨phase0é˜¶æ®µï¼Œthread0~7æ“ä½œå…±äº«å†…å­˜ï¼Œåœ¨phase1é˜¶æ®µï¼Œthread8~15æ“ä½œå…±äº«å†…å­˜ï¼Œä»¥æ­¤ç±»æ¨ã€‚

- bank conflictä¼šå¯¼è‡´warpè¢«stallï¼Œå†²çªè¾ƒå¤šä¼šå¯¹æ•´ä¸ªpipelineçš„è€—æ—¶ä¼šæœ‰è¾ƒå¤§çš„å½±å“ã€‚

> [!WARNING]
> å½“å‘ç”Ÿbank conflictæ—¶ï¼Œwarpéœ€è¦é¢å¤–çš„ä¸€ä¸ªcycleæ¥é‡æ–°æäº¤shared memoryçš„è®¿é—®æŒ‡ä»¤åˆ°LSUå•å…ƒï¼Œè¯¥æŒ‡ä»¤éœ€è¦åœ¨MIOä¸­æ’é˜Ÿï¼Œè¿™ç§æ’é˜Ÿä¼šå¯¼è‡´è®¿é—®å»¶è¿Ÿå¢åŠ ï¼Œæ­¤æ—¶warpå¯èƒ½å¤„äºç­‰å¾…æ•°æ®è¿”å›çš„çŠ¶æ€ï¼Œwarp stateæ ‡è¯†ä¸ºStall Short Scoreboardã€‚
>
> å¦‚æœMIOé˜Ÿåˆ—æ»¡ï¼Œæ­¤æ—¶warpå…ˆéœ€è¦ç­‰å¾…MIOé˜Ÿåˆ—å¤„äºéç©ºçš„çŠ¶æ€ï¼Œæ­¤æ—¶warp stateæ ‡è¯†ä¸ºStall MIO Throttleã€‚

#### Free Bank Conflict ä¼˜åŒ–æ–¹æ³•
- **padding**: åœ¨shared memoryçš„äºŒç»´æ•°ç»„ä¸­å¢åŠ paddingåˆ—ï¼Œé¿å…ä¸åŒçº¿ç¨‹è®¿é—®åŒä¸€bank
- **è½¬ç½®å­˜å‚¨**: å°†ä»global memoryè¯»å–çš„æ•°æ®è½¬ç½®å­˜å‚¨åˆ°shared memoryä¸­
- **swizzlingæœºåˆ¶**: ä¸æ”¹å˜ç‰©ç†å†…å­˜å¤§å°ï¼Œè€Œæ˜¯æ”¹å˜åœ°å€æ˜ å°„é€»è¾‘ã€‚åœ¨å­˜å…¥ Shared Memory æ—¶ï¼Œå°†åˆ—åœ°å€ä¸è¡Œåœ°å€è¿›è¡Œå¼‚æˆ–ï¼ˆXORï¼‰ã€‚

#### ä»£ç ç¤ºä¾‹
è¿™é‡Œä½¿ç”¨äº†è½¬ç½®å­˜å‚¨çš„æ–¹æ³•ï¼›

å¯¹äº Paddding å¤§å°çš„é€‰æ‹©ï¼Œæˆ‘ä»¬éœ€è¦åŒæ—¶æ»¡è¶³ä¸¤ä¸ªç¡¬æ€§æ¡ä»¶ï¼š

- **æ¶ˆé™¤ Bank Conflict**ï¼š è·¨è¡Œè®¿é—®æ—¶çš„ Stride ä¸èƒ½æ˜¯ 32 çš„å€æ•°ã€‚

- **æ»¡è¶³å‘é‡åŒ–å¯¹é½**ï¼š æ¯ä¸€è¡Œçš„èµ·å§‹åœ°å€å¿…é¡»æ˜¯ 16 Byte å¯¹é½çš„ï¼ˆå› ä¸ºæˆ‘ä»¬è¦ç”¨ float4 è¿›è¡Œ Global -> Shared çš„å†™å…¥ï¼‰ã€‚

```cpp
template <const int BM = 128, const int BN = 128, const int BK = 8, const int TM = 8,
          const int TN = 8, const int OFFSET = 0>
__global__ void gemm_t_8x8_sliced_k_f32x4_bcf_kernel(float *a, float *b, float *c, const int M,
                                                     const int N, const int K) {
  const int bx = blockIdx.x;
  const int by = blockIdx.y;
  const int tx = threadIdx.x;
  const int ty = threadIdx.y;
  const int tid = ty * blockDim.x + tx;

  __shared__ float s_a[BK][BM + OFFSET];  //æˆ‘ä»¬éœ€è¦çš„æ˜¯ A[0][0], A[1][0], A[2][0],
                                          // A[3][0]ï¼ˆåŒä¸€åˆ—çš„ 4 ä¸ª M å€¼ï¼‰ã€‚åœ¨ s_a[BM][BK] ä¸­ï¼Œè¿™ 4
                                          //ä¸ªæ•°çš„å†…å­˜åœ°å€ä¸è¿ç»­ï¼ˆç›¸éš” BK ä¸ª floatï¼‰ã€‚
  __shared__ float s_b[BK][BN + OFFSET];

  float r_load_a[TM / 2];  // 4
  float r_load_b[TN / 2];  // 4
  float r_comp_a[TM];
  float r_comp_b[TN];
  float r_c[TM][TN] = {0.0};

  int load_a_smem_m = tid / 2;  // tid / 2ï¼Œ(0,1,2,...,128)
  int load_a_smem_k = (tid & 1) << 2;
  int load_b_smem_k = tid / 32;         // 0~8
  int load_b_smem_n = (tid & 31) << 2;  // (0,4,8,12,...,124)
  int load_a_gmem_m = by * BM + load_a_smem_m;
  int load_b_gmem_n = bx * BN + load_b_smem_n;

  for (int bk = 0; bk < (K + BK - 1) / BK; bk++) {
    int load_a_gmem_k = bk * BK + load_a_smem_k;
    int load_a_gmem_addr = load_a_gmem_m * K + load_a_gmem_k;
    int load_b_gmem_k = bk * BK + load_b_smem_k;
    int load_b_gmem_addr = load_b_gmem_k * N + load_b_gmem_n;
    FLOAT4(r_load_a[0]) = FLOAT4(a[load_a_gmem_addr]);
    FLOAT4(r_load_b[0]) = FLOAT4(b[load_b_gmem_addr]);

    s_a[load_a_smem_k][load_a_smem_m] = r_load_a[0];      // e.g layer_0  b0
    s_a[load_a_smem_k + 1][load_a_smem_m] = r_load_a[1];  // e.g layer_4  b0
    s_a[load_a_smem_k + 2][load_a_smem_m] = r_load_a[2];  // e.g layer_8  b0
    s_a[load_a_smem_k + 3][load_a_smem_m] = r_load_a[3];  // e.g layer_12 b0

    FLOAT4(s_b[load_b_smem_k][load_b_smem_n]) = FLOAT4(r_load_b[0]);

    __syncthreads();

#pragma unroll
    for (int tk = 0; tk < BK; tk++) {
      FLOAT4(r_comp_a[0]) = FLOAT4(s_a[tk][ty * TM / 2]);
      FLOAT4(r_comp_a[4]) = FLOAT4(s_a[tk][ty * TM / 2 + BM / 2]);

      FLOAT4(r_comp_b[0]) = FLOAT4(s_b[tk][tx * TN / 2]);
      FLOAT4(r_comp_b[4]) = FLOAT4(s_b[tk][tx * TN / 2 + BN / 2]);
      // conclusion: still have some bank conflicts, need 4 memory issues.

#pragma unroll
      for (int tm = 0; tm < TM; tm++) {
#pragma unroll
        for (int tn = 0; tn < TN; tn++) {
          // r_c[tm][tn] += r_comp_a[tm] * r_comp_b[tn];
          r_c[tm][tn] = __fmaf_rn(r_comp_a[tm], r_comp_b[tn], r_c[tm][tn]);
        }
      }
    }
    // sync per BK.
    __syncthreads();
  }

#pragma unroll
  for (int i = 0; i < TM / 2; i++) {
    int store_c_gmem_m = by * BM + ty * TM / 2 + i;
    int store_c_gmem_n = bx * BN + tx * TN / 2;
    int store_c_gmem_addr = store_c_gmem_m * N + store_c_gmem_n;
    FLOAT4(c[store_c_gmem_addr]) = FLOAT4(r_c[i][0]);
    FLOAT4(c[store_c_gmem_addr + BN / 2]) = FLOAT4(r_c[i][4]);
  }
#pragma unroll
  for (int i = 0; i < TM / 2; i++) {
    int store_c_gmem_m = by * BM + BM / 2 + ty * TM / 2 + i;
    int store_c_gmem_n = bx * BN + tx * TN / 2;
    int store_c_gmem_addr = store_c_gmem_m * N + store_c_gmem_n;
    FLOAT4(c[store_c_gmem_addr]) = FLOAT4(r_c[i + TM / 2][0]);
    FLOAT4(c[store_c_gmem_addr + BN / 2]) = FLOAT4(r_c[i + TM / 2][4]);
  }
}
```
##### Why Transpose?
**ä¸ä½¿ç”¨è½¬ç½®ï¼š**
- Bank Conflictï¼šå¯ä»¥é€šè¿‡è°ƒèŠ‚ OFFSET æ¶ˆé™¤ã€‚
- å‘é‡åŒ– (LDS.128)ï¼šè¿™é‡Œå–åˆ°çš„ 4 ä¸ªæ•°åœ¨ç‰©ç†å†…å­˜é‡Œæ˜¯åˆ†æ•£çš„ï¼ˆStrided Accessï¼‰ã€‚
- åæœï¼šå¿…é¡»å‘å°„ 4 æ¡ LDS.32 æŒ‡ä»¤æ¥åˆ†åˆ«è¯»å–è¿™ 4 ä¸ªæ•°ã€‚
- æ€§èƒ½ä»£ä»·ï¼š**æŒ‡ä»¤æ•°å¢åŠ äº† 4 å€ï¼Œå‘å°„å¸¦å®½è¢«æµªè´¹**ã€‚

**ç‰©ç†è½¬ç½® (s_a[BK][BM + OFFSET])**
- Bank Conflictï¼šå¯ä»¥é€šè¿‡è°ƒèŠ‚ OFFSET æ¶ˆé™¤ã€‚
- å‘é‡åŒ– (LDS.128)ï¼šæ”¯æŒã€‚å¯ä»¥ç”¨ 1 æ¡ LDS.128 æŒ‡ä»¤æŠŠè®¡ç®—æ‰€éœ€çš„ 4 ä¸ªæ•°åŠ è½½åˆ°å¯„å­˜å™¨ã€‚

##### Global -> Shared å˜æ…¢ï¼Œä¸ºä»€ä¹ˆè¦åšè½¬ç½®ï¼Ÿ
æ‰§è¡Œé¢‘ç‡çš„å¯¹æ¯”ï¼š
- å¤–å±‚å¾ªç¯ (Global $\rightarrow$ Shared)ï¼šæ‰§è¡Œæ¬¡æ•°ï¼š$K / BK$ æ¬¡ã€‚è¿™æ˜¯ä¸€ä¸ªç›¸å¯¹ä½é¢‘çš„æ“ä½œã€‚å¯¹æ€»æ—¶é—´å½±å“è¾ƒå°ã€‚
- æœ€å†…å±‚å¾ªç¯ (Shared $\rightarrow$ Register $\rightarrow$ FMA)ï¼šæ‰§è¡Œæ¬¡æ•°ï¼š$(K / BK) \times BK = K$ æ¬¡ã€‚è¿™æ˜¯ Kernel ä¸­**æœ€çƒ­ï¼ˆHot Spotï¼‰** çš„åœ°æ–¹ã€‚åœ¨è¿™ä¸ªå¾ªç¯é‡Œï¼Œä»»ä½•å¤šä½™çš„æŒ‡ä»¤éƒ½ä¼šè¢«æ”¾å¤§æ•°ç™¾ä¸‡å€ã€‚å¦‚æœåœ¨è¿™é‡Œèƒ½ç”¨ 1 æ¡æŒ‡ä»¤ä»£æ›¿ 4 æ¡æŒ‡ä»¤ï¼Œæ€§èƒ½æ”¶ç›Šæ˜¯å·¨å¤§çš„ã€‚

### Double Buffer ä¼˜åŒ–(Multi-Stage Pipeline)
Double Buffering æœ¬è´¨ä¸Šæ˜¯ä¸€ç§è½¯ä»¶æµæ°´çº¿ (Software Pipelining) ç­–ç•¥ã€‚å…¶æ ¸å¿ƒç›®æ ‡æ˜¯é€šè¿‡æŒ‡ä»¤è°ƒåº¦ï¼Œæ‰“ç ´å†¯Â·è¯ºä¾æ›¼æ¶æ„ä¸­â€œå–æŒ‡-æ‰§è¡Œâ€çš„ä¸²è¡Œä¾èµ–ï¼Œå®ç°è®¡ç®—èµ„æºï¼ˆALU/FPU/Tensor Coreï¼‰ä¸å­˜å‚¨èµ„æºï¼ˆDMA/Memory Controllerï¼‰çš„æ—¶é—´é‡å  ã€‚

- é¢„å– (Prefetching): åœ¨å¤„ç†å½“å‰è¿­ä»£ Step $K$ çš„åŒæ—¶ï¼Œæå‰å‘å‡º Step $K+1$ çš„å†…å­˜åŠ è½½è¯·æ±‚ã€‚
- å»¶è¿Ÿæ©ç›– (Latency Masking): åˆ©ç”¨è®¡ç®—æŒ‡ä»¤ï¼ˆCompute Kernelï¼‰çš„é«˜ååé‡æ‰§è¡Œæ—¶é—´ï¼Œå»å¡«è¡¥å†…å­˜åŠ è½½æŒ‡ä»¤ï¼ˆLoad Kernelï¼‰çš„é«˜å»¶è¿Ÿç©ºçª—æœŸã€‚
```shell
Time -------------------------------------------------->
[Load K0] ...latency...
                        [Compute K0]
                        [Load K1 (Async/Background)] ...latency...
                                    <---- è¢«è®¡ç®—æ—¶é—´æ©ç›– ---->
                                                     [Compute K1]
                                                     [Load K2] ...
```

#### Cost Model åˆ†æ
å®æ–½ $N$-stage buffering (å…¶ä¸­ $N=2$ ä¸ºåŒç¼“å†²) æ‰€éœ€çš„ Shared Memory å®¹é‡ $S_{mem}$ å¯ç”±ä¸‹å¼ç»™å‡ºï¼š
$$S_{mem} = N \times [BK \times (BM + BN)] \times \text{sizeof(dtype)}$$
å…¶ä¸­ï¼š
- $N$: ç¼“å†²çº§æ•° (Double Bufferingæ—¶ $N=2$, Multi-stage å¯èƒ½ä¸º 3, 4, 5)ã€‚
- $BM, BN$: åˆ†å—çŸ©é˜µåœ¨ M, N ç»´åº¦çš„å°ºå¯¸ã€‚
- $BK$: ç´¯åŠ ç»´åº¦ K çš„æ­¥é•¿ (Unroll factor)ã€‚
- $\text{sizeof(dtype)}$: æ•°æ®ç±»å‹å¤§å° (å¦‚ float ä¸º 4 Bytes)ã€‚

è¿™ä¸€å…¬å¼æ­ç¤ºäº†ç®—æœ¯å¼ºåº¦ (Arithmetic Intensity) ä¸ ç‰‡ä¸Šå­˜å‚¨å®¹é‡ (On-chip Memory Capacity) ä¹‹é—´çš„ç›´æ¥çŸ›ç›¾ã€‚
#### Occupancy vs. Parallelism Trade-off
æˆ‘ä»¬éœ€è¦æƒè¡¡ä¸¤ç§å¹¶è¡Œæ¨¡å¼ï¼š
- çº¿ç¨‹çº§å¹¶è¡Œ (TLP - Thread Level Parallelism)
  - å®šä¹‰ï¼š GPU é€šè¿‡å¿«é€Ÿä¸Šä¸‹æ–‡åˆ‡æ¢ (Context Switching) åœ¨ä¸åŒçš„ Warps ä¹‹é—´è½®è½¬æ‰§è¡Œï¼Œä»¥æ©ç›–å»¶è¿Ÿã€‚- åº¦é‡æŒ‡æ ‡ï¼šå ç”¨ç‡ (Occupancy)ã€‚å³æ´»è·ƒ Warp æ•°é‡ä¸ SM ç‰©ç†æœ€å¤§æ”¯æŒ Warp æ•°é‡çš„æ¯”å€¼ã€‚
  - èµ„æºé™åˆ¶ï¼šShared Memory æ˜¯é™åˆ¶ Occupancy çš„å…³é”®ç“¶é¢ˆã€‚
- æŒ‡ä»¤çº§å¹¶è¡Œ (ILP - Instruction Level Parallelism)
  - å®šä¹‰ï¼šå•ä¸ªçº¿ç¨‹å†…éƒ¨ï¼Œåˆ©ç”¨æµæ°´çº¿æŠ€æœ¯ï¼Œè®©è®¡ç®—æŒ‡ä»¤å’Œè®¿å­˜æŒ‡ä»¤åŒæ—¶åœ¨é£è¡Œä¸­ (In-flight)ã€‚
  - Double Buffering å®é™…ä¸Šæ˜¯æå¤§åœ°æé«˜äº† ILPã€‚

å¦‚æœæˆ‘ä»¬ä½¿ç”¨ Double Bufferingï¼Œè®© Shared Memory è¿‡å¤§ä¼šè§¦å‘ä»¥ä¸‹è¿é”ååº”ï¼š
- **Active Blocks ä¸‹é™**ï¼šè®¾ $C_{SM}$ ä¸ºå•ä¸ª SM çš„ Shared Memory æ€»å®¹é‡ (e.g., A100 164KB)ã€‚å•ä¸ª Block çš„éœ€æ±‚ä¸º $S_{block}$ã€‚åˆ™æ¯ä¸ª SM èƒ½åŒæ—¶é©»ç•™çš„æœ€å¤§ Block æ•°é‡ $N_{blocks}$ ä¸ºï¼š$$N_{blocks} = \lfloor \frac{C_{SM}}{S_{block}} \rfloor$$
- **Occupancy å´©å¡Œ**ï¼šå¦‚æœ $S_{block}$ è¶…è¿‡äº† $C_{SM} / 2$ï¼ˆå³å¤§äº 82KBï¼‰ï¼Œé‚£ä¹ˆ $N_{blocks}$ å¼ºåˆ¶å˜ä¸º 1ã€‚è¿™æ„å‘³ç€æ•´ä¸ª SM ä¸Šåªæœ‰ä¸€ä¸ª Block åœ¨è¿è¡Œã€‚
- **å°¾éƒ¨æ•ˆåº”ä¸å»¶è¿Ÿæš´éœ²**ï¼š
  - **ç¼ºä¹ TLP(Thread Level Parallelism, TLP) è¡¥å¿**ï¼š å½“ SM ä¸Šåªæœ‰ä¸€ä¸ª Block æ—¶ï¼Œå¦‚æœè¯¥ Block å†…çš„æ‰€æœ‰ Warps éƒ½å› ä¸ºåŒæ­¥æŒ‡ä»¤ (__syncthreads()) æˆ– é•¿å»¶è¿ŸæŒ‡ä»¤ï¼ˆå¦‚æœªè¢«å®Œå…¨æ©ç›–çš„ Global Memory è®¿é—®ï¼‰è€Œé˜»å¡ï¼ŒSM å°†æ²¡æœ‰ä»»ä½•å…¶ä»– Block çš„ Warps å¯ä»¥è°ƒåº¦æ¥å¡«è¡¥ç©ºé—²æ—¶é—´ã€‚
  - **æµæ°´çº¿æ°”æ³¡ (Pipeline Stalls)**ï¼š ä¸€æ—¦æµæ°´çº¿æ–­æµï¼ŒGPU çš„æµ·é‡è®¡ç®—å•å…ƒå°†ç¬é—´ç©ºè½¬ï¼Œæ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚
### wmma æŒ‡ä»¤ä¼˜åŒ–
#### Tensor Core
é€šè¿‡WMMA APIï¼Œå¼€å‘è€…å¯å°†D = A Ã— B + Cå½“ä½œwarpæ“ä½œï¼Œå…¶ä¸­çš„Aã€Bã€Cã€Déƒ½æ˜¯æ›´å¤§çŸ©é˜µçš„tileã€‚é€šè¿‡WMMA APIï¼Œwarpçš„æ‰€æœ‰çº¿ç¨‹å¯ä»¥åˆä½œå®Œæˆåœ¨è¿™äº›tileä¸Šçš„çŸ©é˜µä¹˜åŠ æ“ä½œ

æ¯ä¸ªtileå¯ä»¥è¿›ä¸€æ­¥åˆ†å‰²ä¸ºfragmentï¼Œæ¯ä¸ªfragmentæ˜¯æ˜ å°„åˆ°çº¿ç¨‹å¯„å­˜å™¨çš„ä¸€ç»„tileå…ƒç´ ã€‚å› æ­¤ï¼Œè¾“å…¥çŸ©é˜µçš„åˆ†å¸ƒæ˜¯è·¨çº¿ç¨‹çš„ï¼Œæ¯ä¸ªçº¿ç¨‹åªåŒ…å«ä¸€éƒ¨åˆ†tileã€‚ä¸€ä¸ª16Ã—16çš„tileåŒ…å«256ä¸ªå…ƒç´ ã€‚warpï¼ˆåŒ…æ‹¬32ä¸ªçº¿ç¨‹ï¼‰ä¸­çš„æ¯ä¸ªçº¿ç¨‹åœ¨8ä¸ªGPRï¼ˆGeneral-Purpose Registerï¼‰ä¸­ä¿å­˜ä¸€ä¸ª8ï¼ˆ256/32=8ï¼‰å…ƒç´ çš„fragmentã€‚

```cpp
nvcuda::wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> frag_a;
nvcuda::wmma::fragment<wmma::matrix_b, 16, 16, 16, half, wmma::row_major> frag_a;
nvcuda::wmma::fragment<wmma::accumulator, 16, 16, 16, half> frag_c;

nvcuda::wmma::fill_fragment(frag_c, 0.0);
nvcuda::wmma::load_matrix_sync(frag_a, (shared memory or global memory pointer), (stride_a));
nvcuda::wmma::load_matrix_sync(frag_b, (shared memory or global memory pointer), (stride_b));
nvcuda::wmma::mma_sync(frag_c, frag_a, frag_b, frag_c);
nvcuda::wmma::store_matrix_sync((shared memory or global memory pointer), frag_c, (stride_c), wmma::mem_row_major);
```

#### wmma.load vs ldmatrix
åœ¨ä½¿ç”¨ Tensor Core è¿›è¡ŒçŸ©é˜µä¹˜æ³•æ—¶ï¼Œæ•°æ®åŠ è½½æ˜¯ä¸€ä¸ªå…³é”®ç¯èŠ‚ï¼Œè¿™ä¸¤ä¸ª API çš„æ ¸å¿ƒåŒºåˆ«åœ¨äºï¼šå¯»å€é€»è¾‘çš„ç²’åº¦ã€‚

- wmma.load: æ˜¯ Block/Tile çº§ çš„å¯»å€ã€‚ä½ ç»™å®ƒä¸€ä¸ªåŸºåœ°å€ (ptr) å’Œä¸€ä¸ªè·¨åº¦ (stride)ï¼Œå®ƒå‡è®¾æ•°æ®æ˜¯â€œè§„çŸ©â€åœ°æŒ‰è¡Œæˆ–åˆ—æ’åˆ—çš„ã€‚
- ldmatrix: æ˜¯ Thread/Warp çº§ çš„å¯»å€ã€‚Warp é‡Œçš„æ¯ä¸ªçº¿ç¨‹éƒ½å¯ä»¥æä¾›ä¸€ä¸ªç‹¬ç«‹çš„ Shared Memory åœ°å€ã€‚
1. wmma.load çš„å±€é™æ€§ï¼šæ­»æ¿çš„çº¿æ€§æ˜ å°„
   - åœ¨ CUDA C++ API ä¸­ï¼Œwmma::load_matrix_sync çš„å‡½æ•°ç­¾åé€šå¸¸é•¿è¿™æ ·ï¼š`wmma::load_matrix_sync(frag, base_ptr, stride_dm)`;
  - éšå«å‡è®¾ï¼š å®ƒå‡è®¾çŸ©é˜µåœ¨å†…å­˜ä¸­æ˜¯è¿ç»­çš„æˆ–è€…å…·æœ‰å›ºå®š stride çš„ã€‚
  - å¯»å€å…¬å¼ï¼š å¯¹äºçŸ©é˜µä¸­çš„ç¬¬ $(row, col)$ ä¸ªå…ƒç´ ï¼Œå®ƒå¼ºåˆ¶è®¤ä¸ºåœ°å€æ˜¯ï¼š$$Addr = base\_ptr + row \times stride + col$$
  - é—®é¢˜æ‰€åœ¨ï¼šå¦‚æœä½ ä¸ºäº†é¿å… Shared Memory Bank Conflictï¼Œå¯¹æ•°æ®å¸ƒå±€åšäº† XOR Swizzlingï¼ˆå¼‚æˆ–æ··æ´—ï¼Œå³æ‰“ä¹±äº†æ¯è¡Œçš„èµ·å§‹åç§»é‡ï¼‰ï¼Œé‚£ä¹ˆæ•°æ®åœ¨å†…å­˜ä¸­çš„ç‰©ç†ä½ç½®å°±ä¸å†ç¬¦åˆ row * stride è¿™ç§ç®€å•çš„çº¿æ€§å…³ç³»äº†ã€‚ã€‚wmma.load æ— æ³•ç†è§£è¿™ç§é€»è¾‘ï¼Œ
2. ldmatrix çš„çµæ´»æ€§ï¼šPer-Thread å¯»å€
   - ldmatrix æ˜¯ Ampere å¼•å…¥çš„ PTX æŒ‡ä»¤ï¼Œå®ƒæŠŠâ€œåŠ è½½ä»€ä¹ˆæ•°æ®â€çš„æ§åˆ¶æƒå®Œå…¨äº¤ç»™äº†æ¯ä¸€ä¸ªçº¿ç¨‹ã€‚
   - æŒ‡ä»¤åŸå‹ï¼ˆPTXï¼‰ï¼š`ldmatrix.sync.aligned.m8n8.x4.shared.b16 {r0, r1, r2, r3}, [addr]`;
   - è¿™é‡Œçš„ [addr] æ˜¯ä¸€ä¸ªå¯„å­˜å™¨ä¸­çš„å€¼ã€‚
   - Warp ä¸­çš„ 32 ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ªçº¿ç¨‹éƒ½æŒæœ‰è‡ªå·±çš„ addr å¯„å­˜å™¨ã€‚
   - åœ¨ .x4 æ¨¡å¼ï¼ˆåŠ è½½ 4 ä¸ªå¯„å­˜å™¨ï¼Œå¯¹åº” $16 \times 16$ çŸ©é˜µ Aï¼‰ä¸‹ï¼Œç¡¬ä»¶é€šå¸¸å°† Warp åˆ†ä¸º 8 ç»„ï¼Œæ¯ç»„ 4 ä¸ªçº¿ç¨‹ã€‚è¿™ 4 ä¸ªçº¿ç¨‹åä½œåŠ è½½ä¸€éƒ¨åˆ†æ•°æ®ã€‚
#### warp tile
Warp Tile æ˜¯ Block -> Warp -> Thread çš„ä¸­é—´å±‚ã€‚

- åœ¨FP32 ä¸Šï¼šä¼˜åŒ– Cache å±€éƒ¨æ€§ï¼Œæ–¹ä¾¿æ’å¸ƒã€‚
- åœ¨Tensor Core ä¸Šï¼šå¿…é¡»å­˜åœ¨ï¼Œè¿™æ˜¯é©±åŠ¨ Tensor Core çš„æ•°æ®å¸ƒå±€åŸºç¡€ã€‚

##### ä¸ºä»€ä¹ˆ CUDA Core é‡Œé¢å¯¹ warp tile ä¸æ˜¯å¿…é¡»çš„ï¼Ÿ

ç¡¬ä»¶ç‰¹æ€§ï¼š CUDA Core æ˜¯æ ‡é‡/å‘é‡å•å…ƒï¼ŒåŸæœ¬å°±æ˜¯æŒ‰çº¿ç¨‹è°ƒåº¦çš„ã€‚

æ€§èƒ½ç“¶é¢ˆï¼š åœ¨ FP32 SGEMM ä¸­ï¼Œæ€§èƒ½ç“¶é¢ˆé€šå¸¸åœ¨äº Global Memory å¸¦å®½ å’Œ Shared Memory å†²çªã€‚

#### Block Swizzle
é€šè¿‡æ”¹å˜ Grid çš„éå†é¡ºåºï¼Œæ¬ºéª— GPU ç¡¬ä»¶è°ƒåº¦å™¨ï¼Œä»¥æœ€å¤§é™åº¦åœ°æé«˜ L2 Cache çš„å‘½ä¸­ç‡ã€‚

CUDA é»˜è®¤æŒ‰ç…§ blockIdx.x å¢åŠ çš„æ–¹å‘è°ƒåº¦ï¼Œå¡«æ»¡ä¸€è¡Œåå†æ¢ä¸‹ä¸€è¡Œï¼ˆblockIdx.y å¢åŠ ï¼‰ã€‚RTX 3090 åŒä¸€æ—¶åˆ»åªèƒ½è·‘ 82 ä¸ª Blockã€‚

**ä¸åŠ  Swizzleï¼š**

ä¸€è¡Œæœ‰ 64 ä¸ª Blockï¼ˆBX=64ï¼‰ã€‚GPU ä¸€æ¬¡èƒ½â€œåƒâ€æ‰ç¬¬ä¸€è¡Œå…¨è¡Œ (64ä¸ª) + ç¬¬äºŒè¡Œå‰ 18 ä¸ªã€‚å½“ç¬¬ä¸€æ‰¹ Block è·‘å®Œï¼ŒSM é‡Šæ”¾ï¼Œå¼€å§‹è·‘åç»­ Block æ—¶ï¼Œå¦‚æœæŒ‰ç…§å…‰æ …é¡ºåºï¼Œå®ƒä¼šç»§ç»­å¾€å³è·‘æˆ–è€…æ¢è¡Œã€‚å¦‚æœä¸æ§åˆ¶é¡ºåºï¼ŒSM å¯èƒ½ä¼šéšæœºåœ°ä» DRAM æ‹‰å–éå¸¸åˆ†æ•£çš„ B çŸ©é˜µ Tileï¼Œå¯¼è‡´å¸¦å®½ç“¶é¢ˆã€‚

> åœ¨å¤„ç†ä¸­é—´é‚£ 127 ä¸ª Block çš„è¿‡ç¨‹ä¸­ï¼ŒåŠ è½½äº†å¤§é‡æ–°çš„ B çŸ©é˜µæ•°æ®ã€‚ç”±äº L2 Cache å®¹é‡æœ‰é™ï¼Œ$B_{tile}[0]$ å¾ˆå¯èƒ½æ—©å°±è¢«åç»­çš„ $B_{tile}[1 \dots 127]$ æŒ¤å‡ºå»äº†ï¼ˆEvictedï¼‰ã€‚è¿™å°±å¯¼è‡´äº† B çŸ©é˜µçš„ Cache Thrashingï¼ˆé¢ ç°¸ï¼‰ï¼Œæ¯æ¬¡æ¢è¡Œéƒ½è¦é‡æ–°ä» DRAM è¯»å– Bï¼Œæå¤§åœ°æµªè´¹äº†å¸¦å®½

**åŠ ä¸Š Swizzleï¼š**
GPU ç¡¬ä»¶è°ƒåº¦å™¨é€šå¸¸ä¼˜å…ˆè°ƒåº¦ xï¼Œç„¶å yï¼Œæœ€å zï¼ˆæˆ–è€…è¯´åœ¨ z å›ºå®šçš„æƒ…å†µä¸‹è·‘å®Œ x, yï¼‰ã€‚ç°åœ¨çš„ gridDim.x = 16ã€‚

GPU æ‰§è¡Œçš„ Block å¤§æ¦‚ç‡æ˜¯ï¼š

- Strip 0 çš„ Row 0 (16ä¸ª)
- Strip 0 çš„ Row 1 (16ä¸ª)
- Strip 0 çš„ Row 2 (16ä¸ª)
- Strip 0 çš„ Row 3 (16ä¸ª)
- Strip 0 çš„ Row 4 (16ä¸ª)
- Strip 0 çš„ Row 5 (2ä¸ª)

è¿™ 82 ä¸ªæ­£åœ¨è¿è¡Œçš„ Blockï¼Œç»å¤§å¤šæ•°éƒ½é›†ä¸­åœ¨ B çŸ©é˜µçš„å‰ 16 ä¸ª Tile ä¸Šã€‚å®ƒä»¬å…±äº« B çŸ©é˜µçš„è¯»è¯·æ±‚ï¼ŒL2 Cache å‘½ä¸­ç‡æé«˜ï¼ŒDRAM å¸¦å®½å‹åŠ›éª¤é™ã€‚
#### Example

- warp_id èŒƒå›´ 0~7 (8ä¸ª warps)ã€‚
- warp_m = warp_id / 2 (0~3) -> M ç»´åº¦æœ‰ 4 ä¸ª Warpã€‚
- warp_n = warp_id % 2 (0~1) -> N ç»´åº¦æœ‰ 2 ä¸ª Warpã€‚
- æ¯ä¸ª Warp è´Ÿè´£ $32 \times 64$ã€‚
- Block æ€»å¤§å°æ˜¯ M: $4 \times 32 = 128$, N: $2 \times 64 = 128$ã€‚
- è¿™é‡Œç®€å•ä½¿ç”¨ Padding é¿å… shared memory åœ¨ load/store æ—¶çš„ Bank Conflictã€‚



## æ€»ç»“

![](img/gemm_benchmark_line.png)

è¿™å¼ å›¾è¡¨éå¸¸ç›´è§‚åœ°å±•ç¤ºäº†ä¸åŒ GEMM å®ç°ç­–ç•¥åœ¨ä¸åŒçŸ©é˜µè§„æ¨¡ä¸‹çš„æ€§èƒ½è¡¨ç°ï¼ˆTFLOPSï¼‰ã€‚

### 1. å°çŸ©é˜µä¸‹ï¼ˆ< 1024ï¼‰ï¼ŒPyTorch (cuBLAS) æ•ˆæœæœ€å¥½

**åŸå› ï¼š**

- å¯åŠ¨å¼€é”€ï¼ˆLaunch Overheadï¼‰ï¼šå°çŸ©é˜µè®¡ç®—æ—¶é—´çŸ­ï¼ŒKernel å¯åŠ¨å’Œ CPU ä¾§çš„å¼€é”€å æ¯”å¤§ã€‚PyTorch/cuBLASåœ¨è¿™æ–¹é¢åšäº†æè‡´ä¼˜åŒ–ã€‚

- å¯å‘å¼é€‰æ‹©ï¼šcuBLAS å†…éƒ¨é’ˆå¯¹å°å°ºå¯¸æœ‰ä¸“é—¨çš„â€œç¡¬ç¼–ç â€ç­–ç•¥ï¼Œå¹¶ä¸æ˜¯å•çº¯èµ°é€šç”¨çš„åˆ†å—é€»è¾‘ï¼Œå¯èƒ½ç›´æ¥ç”¨å¯„å­˜å™¨æåº¦ä¼˜åŒ–çš„ Micro-kernelã€‚

- æ‰‹å†™ Kernelï¼ˆç‰¹åˆ«æ˜¯ FP32x4 ç³»åˆ—ï¼‰åœ¨è¿™é‡Œæ€§èƒ½è¾ƒä½ï¼Œè¯´æ˜å•çº¯çš„ Tiling å’Œå‘é‡åŒ–åœ¨å°å°ºå¯¸ä¸Šæ— æ³•æˆ˜èƒœ cuBLAS çš„ç­–ç•¥ã€‚

### 2. ä¸­å¤§çŸ©é˜µä¸‹ï¼ŒTF32 Tensor Core æ•ˆæœæ˜æ˜¾ä¼˜äº FP32
- ä» 2048 å¼€å§‹ï¼Œå¸¦æœ‰ tf32 å‰ç¼€çš„æ›²çº¿ï¼ˆçº¢ã€ç´«ã€ç²‰ã€é’è‰²ï¼‰å¼€å§‹åè¶… f32_thã€‚åœ¨ 4096 å’Œ 8192 è¾¾åˆ°å³°å€¼ï¼Œæœ€é«˜æ¥è¿‘ 28 TFLOPSã€‚

**åŸå› ï¼š**
- Tensor Core ä¼˜åŠ¿ï¼šPyTorch çš„ f32_th é»˜è®¤å¯èƒ½è¿˜æ˜¯èµ°çš„ FP32 SIMT è·¯å¾„ï¼ˆæˆ–è€…ç­–ç•¥ç›¸å¯¹ä¿å®ˆï¼‰ï¼Œè™½ç„¶ç¨³å®šåœ¨ 23 TFLOPS å·¦å³ï¼Œä½†æ— æ³•è¾¾åˆ° Tensor Core çš„ç†è®ºæé™ã€‚

- æµæ°´çº¿æ©ç›–ï¼šé«˜æ€§èƒ½çš„ tf32 æ›²çº¿é€šå¸¸å¸¦æœ‰ stage2 æˆ– stage3ï¼Œè¯´æ˜å¤šçº§æµæ°´çº¿æˆåŠŸæ©ç›–äº† Global Memory çš„å»¶è¿Ÿã€‚

### 3. Grid Swizzling ä¸ Pipeline çš„ trade-off
- åœ¨ 16384 å¤„ä¾ç„¶ä¿æŒåœ¨ ~26 TFLOPS çš„é«˜ä½ï¼Œå®Œèƒœ PyTorchã€‚

**åŸå› ï¼š**

- Stage 3ï¼šä¸‰çº§æµæ°´çº¿æ¯”äºŒçº§æä¾›äº†æ›´æ·±çš„é¢„å–æ·±åº¦ï¼Œæ›´èƒ½å®¹å¿å¤§çŸ©é˜µä¸‹çš„ DRAM å»¶è¿ŸæŠ–åŠ¨ã€‚

- Warp Tile ä¼˜åŒ–ï¼šwarp2x4 å¯èƒ½æä¾›äº†æ›´å¥½çš„å¯„å­˜å™¨å’Œ Shared Memory å¸ƒå±€ï¼Œé…åˆ Stage 3 å½¢æˆäº†æ›´ç¨³å¥çš„è®¿å­˜æ¨¡å¼ã€‚


