---

title: CUDA Optimization for LLM Inference
tags:

- CUDA

created: 2025-11-3

---

# CUDA Optimization for LLM Inference

## Overview

åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­ï¼Œä¼˜åŒ– CUDA ä»£ç å¯¹äºæå‡æ€§èƒ½å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¡£ä»‹ç»äº†ä¸€äº›å…³é”®çš„ CUDA ä¼˜åŒ–æŠ€æœ¯ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°åˆ©ç”¨ GPU èµ„æºè¿›è¡Œ LLM æ¨ç†ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä»‹ç» Transformer-based è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹æ¨ç†ç”¨åˆ°çš„ç®—å­çš„ä¼˜åŒ–æ–¹æ³•ã€‚è¿™äº›ä¼˜åŒ–çš„æ–¹æ³•ä¹Ÿé€‚ç”¨äºå…¶ä»–ç®—å­ã€‚

æˆ‘ä»¬ä¸»è¦å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. GPU ç¡¬ä»¶æ¶æ„(Hirerarchy Memory, SM, Warp ç­‰)
1. CUDA è®¡ç®—æ¨¡å‹(Thread, Thread Block, Grid ç­‰)
1. CUDA Kernel æ€§èƒ½è°ƒä¼˜(Nsight Compute, Occupancy)
1. CUDA å¸¸ç”¨ä¼˜åŒ–æŠ€å·§(Double buffering, Memory Coalescing, Overcoming Bank Conflict ç­‰)
1. Transformer å†…éƒ¨ç®—å­ä¼˜åŒ–
   - Matrix Multiplication ä¼˜åŒ–
   - Softmax ä¼˜åŒ–
   - LayerNorm ä¼˜åŒ–
   - Self-Attention ä¼˜åŒ–
   - Flash Attention ä¼˜åŒ–

## GPU ç¡¬ä»¶æ¶æ„

GPU ä»¥ Throughput ä¸ºè®¾è®¡ç›®æ ‡ï¼Œå’Œ CPU æœ‰å¾ˆå¤§çš„ä¸åŒã€‚

- GPU ä¸­è™½æœ‰**ç¼“å­˜ç»“æ„ä½†æ˜¯æ•°é‡**å°‘ã€‚ å› ä¸ºè¦å‡å°‘æŒ‡ä»¤è®¿é—®ç¼“å­˜çš„æ¬¡æ•°ã€‚
- GPU ä¸­**æ§åˆ¶å•å…ƒéå¸¸ç®€å•**ã€‚ æ§åˆ¶å•å…ƒä¸­æ²¡æœ‰åˆ†æ”¯é¢„æµ‹æœºåˆ¶å’Œæ•°æ®è½¬å‘æœºåˆ¶ï¼Œå¯¹äºå¤æ‚çš„æŒ‡ä»¤è¿ç®—å°±ä¼šæ¯”è¾ƒæ…¢ã€‚
- GPU çš„**è¿ç®—å•å…ƒ (Core) éå¸¸å¤š**ï¼Œé‡‡ç”¨é•¿å»¶æ—¶æµæ°´çº¿ä»¥å®ç°é«˜ååé‡ã€‚ æ¯ä¸€è¡Œçš„è¿ç®—å•å…ƒçš„æ§åˆ¶å™¨åªæœ‰ä¸€ä¸ªï¼Œæ„å‘³ç€æ¯ä¸€è¡Œçš„è¿ç®—å•å…ƒä½¿ç”¨çš„æŒ‡ä»¤æ˜¯ç›¸åŒçš„ï¼Œä¸åŒçš„æ˜¯å®ƒä»¬çš„æ•°æ®å†…å®¹ã€‚é‚£ä¹ˆè¿™ç§æ•´é½åˆ’ä¸€çš„è¿ç®—æ–¹å¼ä½¿å¾— GPU å¯¹äºé‚£äº›æ§åˆ¶ç®€å•ä½†è¿ç®—é«˜æ•ˆçš„æŒ‡ä»¤çš„æ•ˆç‡æ˜¾è‘—å¢åŠ ã€‚[^cuda1]

![](img/gpu_hardware.jpg)

### Streaming Multiprocessor (SM) (Based on H100 Hopper Architecture)

NVIDIA H100 GPU åŸºäº Hopper æ¶æ„ï¼Œå…¶ SM è®¾è®¡ä¸ºäº†æè‡´çš„ AI å’Œ HPC æ€§èƒ½è¿›è¡Œäº†é‡å¤§å‡çº§ã€‚ä¸€ä¸ª H100 SM åŒ…å«ä»¥ä¸‹æ ¸å¿ƒç»„ä»¶ï¼š

- **4 ä¸ªå¤„ç†å— (SMSP - SM Sub-Partitions):**

  - **æ¶æ„è®¾è®¡:** ä¸ºäº†æé«˜æŒ‡ä»¤æµæ°´çº¿çš„å¹¶è¡Œåº¦å’Œèµ„æºåˆ©ç”¨ç‡ï¼Œæ¯ä¸ª SM è¢«ç‰©ç†åˆ’åˆ†ä¸º 4 ä¸ªç‹¬ç«‹çš„å¤„ç†å—ï¼ˆSMSPï¼‰ã€‚
  - **èµ„æºéš”ç¦»:** æ¯ä¸ª SMSP æ‹¥æœ‰è‡ªå·±ç‹¬ç«‹çš„æŒ‡ä»¤è°ƒåº¦å™¨ã€å¯„å­˜å™¨æ–‡ä»¶å’Œè®¡ç®—å•å…ƒã€‚è¿™æ„å‘³ç€å®ƒä»¬å¯ä»¥ç‹¬ç«‹åœ°è°ƒåº¦å’Œæ‰§è¡Œ Warpï¼Œäº’ä¸å¹²æ‰°ã€‚
  - **ç»„ä»¶è¯¦æƒ…:**
    - **Warp Scheduler & Dispatch Unit:** æ¯ä¸ª SMSP æœ‰ä¸€ä¸ª Warp è°ƒåº¦å™¨ã€‚å®ƒè´Ÿè´£ä»åˆ†é…ç»™è¯¥ SMSP çš„ Warp æ± ä¸­é€‰æ‹©ä¸€ä¸ªâ€œå‡†å¤‡å°±ç»ªâ€ï¼ˆReadyï¼‰çš„ Warpï¼Œå¹¶å°†å…¶ä¸‹ä¸€æ¡æŒ‡ä»¤åˆ†å‘åˆ°æ‰§è¡Œå•å…ƒã€‚è¿™æ˜¯ GPU "Latency Hiding" æœºåˆ¶çš„æ ¸å¿ƒã€‚
    - **Register File:** æ¯ä¸ª SMSP é…å¤‡ 64K ä¸ª 32-bit å¯„å­˜å™¨ï¼ˆæ•´ä¸ª SM å…± 256Kï¼‰ã€‚å¯„å­˜å™¨æ˜¯ GPU ä¸Šé€Ÿåº¦æœ€å¿«çš„å­˜å‚¨ï¼Œç”¨äºä¿å­˜çº¿ç¨‹çš„ç§æœ‰å˜é‡ã€‚å·¨å¤§çš„å¯„å­˜å™¨æ–‡ä»¶æ”¯æŒäº†é«˜å¹¶å‘çº¿ç¨‹æ•°ï¼ˆOccupancyï¼‰ã€‚
    - **L0 Instruction Cache:** ç¼“å­˜æŒ‡ä»¤ï¼Œå‡å°‘å–æŒ‡å»¶è¿Ÿã€‚
    - **Math Units (è®¡ç®—å•å…ƒ):**
      - **32 FP32 Cores (CUDA Cores):** è´Ÿè´£æ ‡å‡†çš„å•ç²¾åº¦æµ®ç‚¹è¿ç®—ã€‚
      - **16 INT32 Cores:** è´Ÿè´£æ•´æ•°è¿ç®—ï¼Œé€šå¸¸ç”¨äºåœ°å€è®¡ç®—ã€å¾ªç¯æ§åˆ¶ç­‰ã€‚Hopper æ¶æ„å…è®¸ FP32 å’Œ INT32 æŒ‡ä»¤å¹¶å‘æ‰§è¡Œï¼Œæé«˜äº†æµæ°´çº¿æ•ˆç‡ã€‚
      - **16 FP64 Cores:** åŒç²¾åº¦æµ®ç‚¹å•å…ƒã€‚H100 çš„ FP64 æ€§èƒ½æ˜¯ A100 çš„ 3 å€ï¼Œè¿™å¯¹ç§‘å­¦è®¡ç®—ï¼ˆHPCï¼‰è‡³å…³é‡è¦ã€‚
      - **1 4th Gen Tensor Core:** è¿™æ˜¯ AI æ¨ç†å’Œè®­ç»ƒçš„åŠ¨åŠ›æºã€‚
        - **å¤šç²¾åº¦æ”¯æŒ:** æ”¯æŒ FP8, FP16, BF16, TF32, FP64ã€‚FP8 æ˜¯ Hopper çš„ä¸€å¤§äº®ç‚¹ï¼Œååé‡æ˜¯ FP16 çš„ä¸¤å€ã€‚
        - **ç¨€ç–æ€§ (Sparsity):** æ”¯æŒç»“æ„åŒ–ç¨€ç–ï¼ˆStructured Sparsityï¼‰ï¼Œå¯è¿›ä¸€æ­¥æå‡ 2 å€æ€§èƒ½ã€‚
        - **å¼‚æ­¥æ‰§è¡Œ:** ä¸ CUDA Cores å¼‚æ­¥æ‰§è¡Œï¼Œæå¤§æå‡äº†çŸ©é˜µè¿ç®—æ•ˆç‡ã€‚

- **Shared Memory / L1 Data Cache:**

  - **ç»Ÿä¸€æ¶æ„:** H100 SM æ‹¥æœ‰ **256 KB** çš„ç»Ÿä¸€ L1/Shared Memoryï¼ˆç›¸æ¯” A100 çš„ 192 KB æå‡äº† 33%ï¼‰ã€‚è¿™å—å†…å­˜å¯ä»¥æ ¹æ®éœ€è¦è¢«é…ç½®ä¸º L1 ç¼“å­˜æˆ– Shared Memoryã€‚
  - **é«˜å¸¦å®½:** å®ƒæ˜¯ç‰‡ä¸Šå†…å­˜ï¼Œå¸¦å®½æé«˜ï¼Œå»¶è¿Ÿæä½ï¼Œæ˜¯çº¿ç¨‹å—å†…æ•°æ®äº¤æ¢çš„æ¡¥æ¢ã€‚
  - **Distributed Shared Memory (DSMEM):** è¿™æ˜¯ Hopper å¼•å…¥çš„é©å‘½æ€§ç‰¹æ€§ã€‚
    - **Thread Block Cluster:** å…è®¸å°†å¤šä¸ª Thread Block ç»„æˆä¸€ä¸ª Clusterï¼ˆä¾‹å¦‚ 8 ä¸ª Blockï¼‰ã€‚
    - **ç›´æ¥è®¿é—®:** Cluster å†…çš„æ‰€æœ‰çº¿ç¨‹å¯ä»¥ç›´æ¥è®¿é—®å…¶ä»– SM ä¸Šçš„ Shared Memoryï¼Œå°±åƒè®¿é—®æœ¬åœ° Shared Memory ä¸€æ ·ï¼ˆè™½ç„¶å»¶è¿Ÿç¨é«˜ä¸€ç‚¹ç‚¹ï¼‰ã€‚è¿™æ‰“ç ´äº†ä¼ ç»Ÿ CUDA ç¼–ç¨‹ä¸­ "Shared Memory ä»…é™ Block å†…å…±äº«" çš„é™åˆ¶ï¼Œä½¿å¾—è·¨ Block çš„æ•°æ®å¤ç”¨å’Œé€šä¿¡æˆä¸ºå¯èƒ½ã€‚

- **Tensor Memory Accelerator (TMA):**

  - **å®šä¹‰:** ä¸€ä¸ªä¸“ç”¨çš„ç¡¬ä»¶ç›´æ¥å†…å­˜è®¿é—® (DMA) å¼•æ“ã€‚
  - **åŠŸèƒ½:** è´Ÿè´£åœ¨ Global Memory å’Œ Shared Memory ä¹‹é—´é«˜æ•ˆã€å¼‚æ­¥åœ°æ¬è¿æ•°æ®ã€‚
  - **ä¼˜åŠ¿:**
    - **è§£æ”¾ SM:** ä¼ ç»Ÿçš„æ•°æ®åŠ è½½éœ€è¦ CUDA Core æ‰§è¡Œ Load æŒ‡ä»¤ï¼Œå ç”¨å¯„å­˜å™¨å’Œ ALU èµ„æºã€‚TMA æ¥ç®¡äº†è¿™é¡¹å·¥ä½œï¼Œè®© SM ä¸“æ³¨äºè®¡ç®—ã€‚
    - **é™ä½å¯„å­˜å™¨å‹åŠ›:** æ•°æ®ç›´æ¥ä» Global Memory å†™å…¥ Shared Memoryï¼Œä¸ç»è¿‡å¯„å­˜å™¨æ–‡ä»¶ã€‚
    - **ç®€åŒ–ç¼–ç¨‹:** åªéœ€è¦é…ç½®å¥½ Copy Descriptorï¼ŒTMA å°±ä¼šè‡ªåŠ¨å¤„ç†å¤æ‚çš„åœ°å€è®¡ç®—ï¼ˆå¦‚çŸ©é˜µå—çš„ stride è®¿é—®ï¼‰å’Œè¶Šç•Œæ£€æŸ¥ã€‚
    - **æ”¯æŒ mbarrier:** ä¸å¼‚æ­¥å±éšœ (mbarrier) ç»“åˆï¼Œå®ç°é«˜æ•ˆçš„æµæ°´çº¿å¹¶è¡Œã€‚

![](img/h100sm.png)

#### Functional Units (H100 Specifics)

H100 çš„åŠŸèƒ½å•å…ƒè®¾è®¡æ—¨åœ¨æœ€å¤§åŒ–å¹¶å‘æ€§å’Œååé‡ï¼š

- **å¹¶å‘æµæ°´çº¿ (Concurrent Pipelines):**
  - H100 çš„ SM åŒ…å«ç‹¬ç«‹çš„ FP32, INT32 å’Œ FP64 æ•°æ®è·¯å¾„ã€‚
  - **FP32 & INT32 å¹¶å‘:** å…è®¸åœ¨åŒä¸€ä¸ªå‘¨æœŸå†…åŒæ—¶å‘å°„ FP32 æŒ‡ä»¤å’Œ INT32 æŒ‡ä»¤ã€‚è¿™å¯¹äºå¾ªç¯ç´¢å¼•æ›´æ–°ï¼ˆINT32ï¼‰å’Œæ ¸å¿ƒè®¡ç®—ï¼ˆFP32ï¼‰å¹¶è¡Œæ‰§è¡Œéå¸¸æœ‰æ•ˆã€‚
  - **FP64 æ€§èƒ½:** H100 çš„ FP64 ååé‡æ˜¯ FP32 çš„ 1/2ï¼ˆåœ¨ GH100 æ ¸å¿ƒä¸Šï¼‰ï¼Œç›¸æ¯” A100 å¤§å¹…æå‡ï¼Œé€‚åˆç§‘å­¦è®¡ç®—ã€‚
- **Tensor Core (4th Gen):**
  - ç‹¬ç«‹äº CUDA Cores çš„ä¸“ç”¨æµæ°´çº¿ã€‚
  - æ”¯æŒ **å¼‚æ­¥æ‰§è¡Œ**ï¼Œå³ Tensor Core åœ¨è¿›è¡ŒçŸ©é˜µè¿ç®—æ—¶ï¼ŒCUDA Cores å¯ä»¥åŒæ—¶å¤„ç†å…¶ä»–æ— å…³æŒ‡ä»¤ï¼ˆå¦‚åœ°å€è®¡ç®—ã€é€»è¾‘åˆ¤æ–­ï¼‰ã€‚

#### Warp Contexts & Occupancy

- **Warp Contexts:** æ¯ä¸ª SM ç»´æŠ¤ç€å¤§é‡ Warp çš„ä¸Šä¸‹æ–‡ï¼ˆPC, å¯„å­˜å™¨çŠ¶æ€ç­‰ï¼‰ã€‚H100 ä¸Šæ¯ä¸ª SM æœ€å¤šæ”¯æŒ **64 ä¸ª Warp**ï¼ˆå³ 2048 ä¸ªçº¿ç¨‹ï¼‰ã€‚
- **Occupancy (å ç”¨ç‡):**
  - æŒ‡å½“å‰ SM ä¸Šå®é™…æ¿€æ´»çš„ Warp æ•°é‡ä¸æœ€å¤§æ”¯æŒæ•°é‡çš„æ¯”å€¼ã€‚
  - **å¯„å­˜å™¨å‹åŠ›:** H100 æ¯ä¸ª SM æœ‰ 64K (32-bit) å¯„å­˜å™¨ã€‚å¦‚æœæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨çš„å¯„å­˜å™¨è¿‡å¤šï¼ŒSM å°±æ— æ³•åŒæ—¶å®¹çº³è¶³å¤Ÿå¤šçš„ Warpï¼Œå¯¼è‡´ Occupancy ä¸‹é™ï¼Œä»è€Œæ— æ³•æœ‰æ•ˆéšè—å»¶è¿Ÿã€‚
  - **Shared Memory é™åˆ¶:** åŒæ ·ï¼Œå¦‚æœ Block æ¶ˆè€—è¿‡å¤šçš„ Shared Memoryï¼Œä¹Ÿä¼šé™åˆ¶ SM ä¸Šèƒ½å¹¶å‘è¿è¡Œçš„ Block æ•°é‡ã€‚

#### Warp Scheduler (H100)

H100 çš„ Warp è°ƒåº¦å™¨æ›´åŠ æ™ºèƒ½å’Œé«˜æ•ˆï¼š

- **ç‹¬ç«‹è°ƒåº¦å™¨:** æ¯ä¸ª SMSP (SM Sub-Partition) éƒ½æœ‰è‡ªå·±çš„ Warp Schedulerã€‚è¿™æ„å‘³ç€ä¸€ä¸ª SM æ¯ä¸ªå‘¨æœŸå¯ä»¥åŒæ—¶ä» 4 ä¸ª SMSP ä¸­å„å‘å°„æŒ‡ä»¤ï¼Œå®ç° 4 è·¯å¹¶å‘ã€‚
- **Latency Hiding (å»¶è¿Ÿéšè—):**
  - è°ƒåº¦å™¨çš„æ ¸å¿ƒä»»åŠ¡æ˜¯**é›¶å¼€é”€ä¸Šä¸‹æ–‡åˆ‡æ¢**ã€‚
  - å½“å½“å‰ Warp é‡åˆ°é•¿å»¶è¿Ÿæ“ä½œï¼ˆå¦‚ Global Memory è¯»å–æˆ– Tensor Core è®¡ç®—ï¼‰è€Œé˜»å¡æ—¶ï¼Œè°ƒåº¦å™¨ä¼šåœ¨**ä¸‹ä¸€ä¸ªæ—¶é’Ÿå‘¨æœŸ**ç«‹å³åˆ‡æ¢åˆ°å¦ä¸€ä¸ªâ€œå‡†å¤‡å°±ç»ªâ€çš„ Warp æ‰§è¡Œã€‚
  - **H100 ä¼˜åŠ¿:** é…åˆ TMA (Tensor Memory Accelerator) çš„å¼‚æ­¥æ‹·è´ï¼ŒSM å¯ä»¥æŒ‚èµ·ç­‰å¾…æ•°æ®çš„ Warpï¼Œè½¬è€Œæ‰§è¡Œè®¡ç®—å¯†é›†çš„ Warpï¼Œå®ç°è®¡ç®—ä¸è®¿å­˜çš„å®Œç¾é‡å ã€‚
- **Warp Divergence (åˆ†æ”¯åˆ†åŒ–):**
  - ä¾ç„¶éµå¾ª SIMT (Single Instruction, Multiple Threads) æ¨¡å‹ã€‚
  - å¦‚æœ Warp å†…çº¿ç¨‹è¿›å…¥ä¸åŒçš„ `if-else` åˆ†æ”¯ï¼Œç¡¬ä»¶ä¼šä¸²è¡Œæ‰§è¡Œä¸åŒè·¯å¾„ï¼Œå¹¶ä½¿ç”¨ **Active Mask** å±è”½ä¸åº”æ‰§è¡Œçš„çº¿ç¨‹ã€‚
  - **ä¼˜åŒ–å»ºè®®:** å°½é‡é¿å… Warp å†…çš„åˆ†æ”¯åˆ†åŒ–ï¼Œæˆ–è€…ä½¿ç”¨ `__shfl_sync` ç­‰æŒ‡ä»¤åœ¨ Warp å†…äº¤æ¢æ•°æ®ä»¥ç»Ÿä¸€æ§åˆ¶æµã€‚

### GPU Memory Hierarchy (H100 Enhanced)

H100 çš„å†…å­˜ä½“ç³»ç»“æ„ä¸ºäº†å–‚é¥±å¼ºå¤§çš„ Tensor Cores è¿›è¡Œäº†å…¨é¢å‡çº§ï¼Œæ ¸å¿ƒåœ¨äº**æ›´é«˜çš„å¸¦å®½**å’Œ**æ›´çµæ´»çš„æ•°æ®å…±äº«**ã€‚

- **Registers (å¯„å­˜å™¨):**

  - **é€Ÿåº¦:** æœ€å¿«ï¼Œå•å‘¨æœŸè®¿é—®ã€‚
  - **å®¹é‡:** æ¯ä¸ª SM æ‹¥æœ‰ 256KB (64K x 32-bit x 4 SMSP) çš„å¯„å­˜å™¨æ–‡ä»¶ã€‚
  - **ä¼˜åŒ–:** ç¼–è¯‘å™¨ä¼šå°½å¯èƒ½å°†é¢‘ç¹è®¿é—®çš„å˜é‡æ”¾å…¥å¯„å­˜å™¨ã€‚å¦‚æœå¯„å­˜å™¨æº¢å‡º (Spill)ï¼Œæ•°æ®ä¼šè¢«æ”¾å…¥ Local Memory (å®é™…ä¸Šæ˜¯ Global Memory)ï¼Œå¯¼è‡´æ€§èƒ½æ€¥å‰§ä¸‹é™ã€‚

- **L1 Data Cache / Shared Memory (Unified):**

  - **ç»Ÿä¸€æ¶æ„:** æ¯ä¸ª SM æ‹¥æœ‰ **256 KB** çš„é«˜é€Ÿç‰‡ä¸Šå†…å­˜ã€‚
  - **å¯é…ç½®:** å¯ä»¥æ ¹æ® Kernel éœ€æ±‚çµæ´»åˆ’åˆ†ä¸º L1 Cache å’Œ Shared Memoryï¼ˆä¾‹å¦‚ 228KB Shared + 28KB L1ï¼‰ã€‚
  - **DSMEM (Distributed Shared Memory):** H100 çš„æ€æ‰‹é”ã€‚å…è®¸ä¸€ä¸ª Thread Block Cluster å†…çš„ SM ç›´æ¥é€šè¿‡ SM-to-SM ç½‘ç»œè®¿é—®å½¼æ­¤çš„ Shared Memoryã€‚è¿™ä½¿å¾— Shared Memory çš„æœ‰æ•ˆå®¹é‡åœ¨é€»è¾‘ä¸Šæ‰©å¤§äº†æ•°å€ã€‚

- **L2 Cache:**

  - **å®¹é‡:** H100 æ‹¥æœ‰å·¨å¤§çš„ **50 MB** (GH100) L2 Cacheï¼ˆç›¸æ¯” A100 çš„ 40 MB æå‡ï¼‰ã€‚
  - **ä½œç”¨:** æ‰€æœ‰ SM å…±äº«ã€‚å®ƒæ˜¯è¿æ¥é«˜é€Ÿ SM å’Œä½é€Ÿ HBM çš„å…³é”®ç¼“å†²ã€‚ç”±äºåŸå­æ“ä½œ (Atomics) é€šå¸¸åœ¨ L2 ä¸Šæ‰§è¡Œï¼Œæ›´å¤§çš„ L2 æ„å‘³ç€æ›´å°‘çš„ DRAM è®¿é—®å†²çªã€‚

- **HBM3 (Global Memory):**

  - **å¸¦å®½:** H100 æ­è½½ HBM3 æ˜¾å­˜ï¼Œå¸¦å®½é«˜è¾¾ **3.35 TB/s** (ç›¸æ¯” A100 çš„ 1.5-2 TB/s æå‡å·¨å¤§)ã€‚
  - **TMA:** é…åˆ Tensor Memory Acceleratorï¼Œæ•°æ®å¯ä»¥â€œæµâ€è¿› Shared Memoryï¼Œè€Œä¸éœ€è¦ CUDA Cores é€ä¸ªæŒ‡ä»¤æ¬è¿ã€‚

- **Constant / Texture Memory:** ä¾ç„¶å­˜åœ¨ï¼Œç”¨äºåªè¯»æ•°æ®çš„ç¼“å­˜ä¼˜åŒ–ï¼Œä½†åœ¨ç°ä»£è®¡ç®—å¯†é›†å‹ Kernel ä¸­ï¼Œå…¶è§’è‰²é€æ¸è¢« L1 Cache å’Œ Shared Memory è¦†ç›–ã€‚

![](img/gpu_hardware.jpg)

:laughing:**Summary (H100 Perspective)**

| å±‚çº§              | ç±»å‹               | H100 ç‰¹æ€§                            | å»¶è¿Ÿ (Cycles)                |
| :---------------- | :----------------- | :----------------------------------- | :--------------------------- |
| **Registers**     | ç§æœ‰               | 256KB/SM, æé€Ÿ                       | ~0                           |
| **Shared Memory** | Block/Cluster å…±äº« | **DSMEM (è·¨ SM è®¿é—®)**, TMA å¼‚æ­¥åŠ è½½ | ~20-30 (Local), ~50 (Remote) |
| **L1 Cache**      | SM ç§æœ‰            | ä¸ Shared Memory ç»Ÿä¸€é…ç½®            | ~30                          |
| **L2 Cache**      | å…¨å±€å…±äº«           | **50MB è¶…å¤§å®¹é‡**, åŸå­æ“ä½œä¸­å¿ƒ      | ~200                         |
| **HBM3 (Global)** | å…¨å±€å…±äº«           | **3TB/s+ å¸¦å®½**, TMA ç›´é€š            | ~400-600+                    |

### Warp (H100 Scheduling & Execution)

è™½ç„¶ç¨‹åºå‘˜ç¼–å†™çš„æ˜¯å•ä¸ªçº¿ç¨‹çš„ä»£ç ï¼Œä½†åœ¨ç¡¬ä»¶å±‚é¢ï¼ŒH100 ä¾ç„¶å°†çº¿ç¨‹ç»„ç»‡æˆå›ºå®šå¤§å°ï¼ˆ32 ä¸ªï¼‰çš„æŸï¼Œç§°ä¸º **Warp**ã€‚Warp æ˜¯ SM ä¸Šè°ƒåº¦å’Œæ‰§è¡Œçš„çœŸæ­£åŸºæœ¬å•ä½ã€‚

- **SIMT ä¸ ç‹¬ç«‹çº¿ç¨‹è°ƒåº¦ (Independent Thread Scheduling):**

  - H100 å»¶ç»­äº† Volta å¼•å…¥çš„ç‹¬ç«‹çº¿ç¨‹è°ƒåº¦æœºåˆ¶ã€‚æ¯ä¸ªçº¿ç¨‹éƒ½æœ‰è‡ªå·±çš„ç¨‹åºè®¡æ•°å™¨ (PC) å’Œè°ƒç”¨æ ˆã€‚
  - å°½ç®¡å¦‚æ­¤ï¼Œä¸ºäº†æ€§èƒ½ï¼ŒWarp å†…çš„ 32 ä¸ªçº¿ç¨‹é€šå¸¸ä¾ç„¶æ˜¯åŒæ­¥æ‰§è¡Œç›¸åŒæŒ‡ä»¤çš„ (SIMT)ã€‚
  - **Warp Divergence:** å¦‚æœ Warp å†…çº¿ç¨‹è¿›å…¥ä¸åŒçš„åˆ†æ”¯ï¼Œç¡¬ä»¶ä¼šä¸²è¡Œæ‰§è¡Œä¸åŒè·¯å¾„ã€‚è™½ç„¶ç‹¬ç«‹è°ƒåº¦é¿å…äº†æ­»é”é£é™©ï¼Œä½†**æ€§èƒ½æƒ©ç½šä¾ç„¶å­˜åœ¨**ï¼Œå› ä¸ºç¡¬ä»¶å•å…ƒåœ¨æŸä¸€æ—¶åˆ»åªèƒ½æœåŠ¡äºä¸€ç§æŒ‡ä»¤è·¯å¾„ã€‚

- **æé€Ÿè°ƒåº¦ (Dispatch Speed):**

  - H100 çš„æ¯ä¸ª SM åˆ’åˆ†ä¸º 4 ä¸ª SMSPï¼Œæ¯ä¸ª SMSP éƒ½æœ‰ç‹¬ç«‹çš„ Warp Schedulerã€‚
  - è¿™æ„å‘³ç€ä¸€ä¸ª SM åœ¨æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸå¯ä»¥åŒæ—¶ä» 4 ä¸ªä¸åŒçš„ Warp ä¸­å‘å°„æŒ‡ä»¤ï¼Œæå¤§åœ°æé«˜äº†æŒ‡ä»¤ååé‡ã€‚

- **å»¶è¿Ÿéšè— (Latency Hiding) 2.0:**

  - ä¼ ç»Ÿçš„å»¶è¿Ÿéšè—ä¾èµ–äºâ€œä¸Šä¸‹æ–‡åˆ‡æ¢â€ï¼šå½“ Warp A ç­‰å¾…å†…å­˜æ—¶ï¼Œåˆ‡æ¢åˆ° Warp Bã€‚
  - **H100 çš„è¿›åŒ–:** ç»“åˆ **TMA** å’Œ **å¼‚æ­¥æ‹·è´ (`cp.async`)**ï¼ŒWarp ä¸å†éœ€è¦ä¸ºäº†æ¬è¿æ•°æ®è€Œâ€œåœé¡¿â€åœ¨ Load æŒ‡ä»¤ä¸Šã€‚Warp å¯ä»¥å‘å‡ºå¼‚æ­¥æ‹·è´å‘½ä»¤ï¼Œç„¶åç»§ç»­æ‰§è¡Œå…¶ä»–è®¡ç®—ä»»åŠ¡ï¼ˆå¦‚ GEMMï¼‰ï¼Œç›´åˆ°çœŸæ­£éœ€è¦æ•°æ®æ—¶æ‰åœ¨ `mbarrier` ä¸Šç­‰å¾…ã€‚è¿™ä½¿å¾—è®¡ç®—å’Œè®¿å­˜çš„é‡å  (Overlap) è¾¾åˆ°äº†æ–°çš„é«˜åº¦ã€‚

- **å¯„å­˜å™¨å‹åŠ›ä¸ Occupancy:**

  - H100 æ¯ä¸ª SM æœ‰ 64K (32-bit) å¯„å­˜å™¨ã€‚
  - ä¸ºäº†ç»´æŒè¶³å¤Ÿçš„ Active Warps æ¥éšè—å»¶è¿Ÿï¼Œå¿…é¡»ä¸¥æ ¼æ§åˆ¶æ¯ä¸ª Kernel ä½¿ç”¨çš„å¯„å­˜å™¨æ•°é‡ã€‚å¦‚æœæ¯ä¸ªçº¿ç¨‹ä½¿ç”¨è¿‡å¤šå¯„å­˜å™¨ï¼ŒSM ä¸Šèƒ½åŒæ—¶é©»ç•™çš„ Warp å°±ä¼šå‡å°‘ï¼Œå¯¼è‡´è°ƒåº¦å™¨â€œæ—  Warp å¯åˆ‡â€ï¼Œæµæ°´çº¿å‡ºç°æ°”æ³¡ã€‚

## CUDA è®¡ç®—æ¨¡å‹[^cuda2]

CUDA å°†è®¡ç®—ä»»åŠ¡ç»„ç»‡æˆä¸€ä¸ªä¸‰çº§å±‚æ¬¡ç»“æ„ ã€‚è¿™æ˜¯ä¸€ä¸ªç”±ç¨‹åºå‘˜åˆ›å»ºçš„ã€ç”¨äºç»„ç»‡é—®é¢˜çš„**é€»è¾‘å±‚æ¬¡**ï¼Œè€Œéç¡¬ä»¶çš„ç›´æ¥ä½“ç° ã€‚

- çº¿ç¨‹ (Thread)ï¼šæœ€åŸºæœ¬çš„æ‰§è¡Œå•ä½ã€‚å•ä¸ªçº¿ç¨‹æ‰§è¡Œä¸€ä¸ª Kernel å‡½æ•°çš„å®ä¾‹ ã€‚æ¯ä¸ªçº¿ç¨‹åœ¨å…¶æ‰€å±çš„çº¿ç¨‹å—å†…æ‹¥æœ‰ä¸€ä¸ªå”¯ä¸€çš„ ID(threadIdx)
- çº¿ç¨‹å—(Thread Block)ï¼šä¸€ç»„å¯ä»¥ç›¸äº’åä½œçš„çº¿ç¨‹ï¼ˆåœ¨ç°ä»£æ¶æ„ä¸Šæœ€å¤š 1024 ä¸ªï¼‰ã€‚ä¸€ä¸ªå—å†…çš„æ‰€æœ‰çº¿ç¨‹å¯ä»¥é€šè¿‡é«˜é€Ÿçš„ç‰‡ä¸Š Â  å…±äº«å†…å­˜å…±äº«æ•°æ®ï¼Œå¹¶èƒ½é€šè¿‡`__syncthreads()`æ¥åè°ƒå®ƒä»¬çš„æ‰§è¡Œ ã€‚æ¯ä¸ªçº¿ç¨‹å—åœ¨å…¶æ‰€å±çš„ Grid å†…ä¹Ÿæ‹¥æœ‰ä¸€ä¸ªå”¯ä¸€çš„ ID(blockIdx)
- ç½‘æ ¼ (Grid)ï¼šä¸ºæ‰§è¡Œå•ä¸ª Kernel è€Œå¯åŠ¨çš„æ‰€æœ‰çº¿ç¨‹å—çš„é›†åˆ ã€‚ä¸€ä¸ª Grid å†…çš„æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥è®¿é—®åŒä¸€ä¸ªå…¨å±€å†…å­˜ç©ºé—´ã€‚Grid å†…çš„çº¿ç¨‹å—è¢«å‡å®šä¸ºç‹¬ç«‹æ‰§è¡Œï¼Œä¸”æ‰§è¡Œé¡ºåºä¸ç¡®å®šï¼›å®ƒä»¬ä¹‹é—´æ²¡æœ‰ç›´æ¥çš„åŒæ­¥æœºåˆ¶ã€‚

çº¿ç¨‹å—å’Œç½‘æ ¼å¯ä»¥è¢«ç»„ç»‡æˆä¸€ç»´ã€äºŒç»´æˆ–ä¸‰ç»´çš„ç»“æ„ï¼Œè¿™ä¸ºå°†è®¡ç®—ä»»åŠ¡æ˜ å°„åˆ°å‘é‡ã€çŸ©é˜µã€ä½“æ•°æ®ç­‰æ•°æ®ç»“æ„ä¸Šæä¾›äº†æå¤§çš„ä¾¿åˆ© ã€‚
![](img/grid.jpg)

### ä¸ç¡¬ä»¶çš„å¯¹åº”

#### Thread

- åœ¨æŸä¸€æ—¶åˆ»ï¼Œä¸€ä¸ª CUDA core æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹çš„æŒ‡ä»¤ã€‚

- :warning: å®é™…ä¸ŠåŒä¸€ä¸ª CUDA core ä¼šåœ¨å¤šä¸ª thread ä¹‹é—´åˆ†æ—¶å¤ç”¨ï¼Œå› ä¸ºçº¿ç¨‹æ•°è¿œå¤šäºæ ¸å¿ƒæ•°

#### Thread Block

- ä¸€ä¸ª block è¢«åˆ†é…åˆ°ä¸€ä¸ª SM (Streaming Multiprocessor) ä¸Šæ‰§è¡Œã€‚
- ä¸€ä¸ª block ä¸­çš„çº¿ç¨‹ä¼šè¢«åˆ†æˆè‹¥å¹² warpã€‚SM å†…æœ‰å¤šä¸ªèµ„æºï¼ˆå¯„å­˜å™¨æ–‡ä»¶ã€å…±äº«å†…å­˜ï¼‰ï¼Œè¿™äº›èµ„æºåœ¨ä¸åŒ block ä¹‹é—´åˆ†é…ã€‚

#### Grid

- ä¸€ä¸ª grid å¯¹åº”ä¸€æ¬¡ kernel å¯åŠ¨ï¼ˆLaunchï¼‰ã€‚
- Grid ä¸­çš„ block ä¼šè¢«åˆ†é…åˆ°ä¸åŒçš„ SM æ‰§è¡Œã€‚
- GPU ä¸Šçš„ GPCï¼ˆGraphics Processing Clusterï¼‰ æˆ– TPCï¼ˆTexture Processing Clusterï¼‰ å±‚çº§ä¼šå‚ä¸å…¨å±€è°ƒåº¦ã€‚

### Kernel æ‰§è¡Œæµç¨‹

- å½“ä¸€ä¸ª Kernel è¢«å¯åŠ¨æ—¶ï¼Œç”±ç¡¬ä»¶è°ƒåº¦å™¨å°† Grid ä¸­çš„æ‰€æœ‰çº¿ç¨‹å—åˆ†é…åˆ° GPU ä¸Šå¯ç”¨çš„ SM ä¸­
- ä¸€ä¸ª Thread Block ä¼šè¢«å®Œæ•´åœ°åˆ†é…ç»™ä¸€ä¸ª SMï¼Œå¹¶åœ¨å…¶ä¸Šå®Œæ•´åœ°æ‰§è¡Œã€‚åœ¨å…¶ç”Ÿå‘½å‘¨æœŸå†…ï¼Œå®ƒä¸ä¼šè¢«è¿ç§»åˆ°å…¶ä»– SM
- ä¸€ä¸ª SM å¯ä»¥å¹¶å‘åœ°æ‰§è¡Œå¤šä¸ª Thread Blockï¼Œå‰ææ˜¯å®ƒæ‹¥æœ‰è¶³å¤Ÿçš„èµ„æºï¼ˆå¦‚å¯„å­˜å™¨ã€å…±äº«å†…å­˜ï¼‰æ¥å®¹çº³è¿™äº›çº¿ç¨‹å—
- åœ¨ SM å†…éƒ¨ï¼Œä¸€ä¸ª Thread Block çš„æ‰€æœ‰çº¿ç¨‹è¢«åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ª Warpï¼ˆæ¯ç»„ 32 ä¸ªçº¿ç¨‹ï¼‰ã€‚è¿™äº› Warp æ‰æ˜¯è¢« SM çš„ Warp è°ƒåº¦å™¨å®é™…è°ƒåº¦æ‰§è¡Œçš„å•å…ƒ ã€‚

è¿™ä¸ªæ˜ å°„å…³ç³»æ˜¯å±‚çº§åŒ–çš„ï¼š`Grid -> GPU`ï¼Œ`Block -> SM`ï¼Œ`Thread -> Warp -> CUDAæ ¸å¿ƒ`

## Roofline æ¨¡å‹

Roofline æ¨¡å‹ï¼ˆå±‹é¡¶çº¿æ¨¡å‹ï¼‰æ˜¯ä¸€ç§ç”¨æ¥**åˆ†æç¨‹åºæ€§èƒ½ç“¶é¢ˆ**ï¼ˆè®¡ç®—å—é™è¿˜æ˜¯å¸¦å®½å—é™ï¼‰çš„æ–¹æ³•ã€‚\
å®ƒæŠŠ**è®¡ç®—æ€§èƒ½**ï¼ˆFLOPs/sï¼‰å’Œ**è®¿å­˜æ€§èƒ½**ï¼ˆBytes/sï¼‰è”ç³»åœ¨ä¸€èµ·ï¼Œã€‚ä»¥å¯è§†åŒ–çš„æ–¹å¼å±•ç¤ºæ€§èƒ½ä¸Šé™

$$
AchievableÂ \\space FLOPs=min(AIÃ—Memory\\spaceÂ BW,PeakÂ FLOPs)
$$

![](img/roofline.png)

## CUDA Kernel æ€§èƒ½è°ƒä¼˜

æ ¹æ® Roofline æ¨¡å‹ï¼Œç®—å­åˆ†ä¸º Compute-Bound å’Œ Memory-Bound å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åˆ†å¼€è®¨è®ºï¼Œä½†å®é™…ä¸Šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸¤ç§ç“¶é¢ˆä¼šäº¤æ›¿å‡ºç°ğŸ˜­

### Memory-Bound

#### 1. æŒ‡ä»¤å±‚çº§ï¼šæŒ‡ä»¤å‘å°„æ•ˆç‡ (Instruction Issue Efficiency)

è¿™æ˜¯ `float4` ä¼˜åŒ–çš„ç›´æ¥ä½œç”¨åŸŸã€‚

- **LSU (Load Store Unit) å‹åŠ›ï¼š**

  - **Scalar (`float`):** æ¬è¿åŒæ ·å¤šçš„æ•°æ®ï¼Œéœ€è¦å‘å°„ **4 å€** çš„æŒ‡ä»¤æ•°ã€‚è¿™ä¼šå¤§é‡å ç”¨å‰ç«¯ï¼ˆFetch/Decodeï¼‰å¸¦å®½ï¼Œå¹¶å¢åŠ  LSU ç»´æŠ¤ In-flight çŠ¶æ€çš„å¼€é”€ã€‚

  - **Vectorized (`float4`):** **å•æŒ‡ä»¤é«˜åå**ã€‚ä¸€æ¡æŒ‡ä»¤å³å¯æ¬è¿ 128-bit æ•°æ®ã€‚LSU é˜Ÿåˆ—å ç”¨å°‘ï¼Œæ›´å®¹æ˜“ç»´æŒæµæ°´çº¿é¥±å’Œã€‚

  - _ä¿®æ­£ç‚¹ï¼š_ å³ä½¿æ˜¯æ ‡é‡è®¿é—®ï¼Œå¦‚æœæ˜¯ Coalesced çš„ï¼ŒWarp ä¹Ÿåªä¼šç”Ÿæˆ 1 ä¸ª Transactionï¼Œä½†éœ€è¦ **4 æ¡æŒ‡ä»¤** æ‰èƒ½å®Œæˆ 4 ä¸ªå…ƒç´ çš„åŠ è½½ã€‚

- **æŒ‡ä»¤å‘å°„å»¶è¿Ÿæ©ç›– (Issue Latency Hiding)ï¼š**

  - **æ°”æ³¡é—®é¢˜ï¼š** æŒ‡ä»¤å‘å°„æœ‰å›ºæœ‰å»¶è¿Ÿã€‚å¦‚æœæ¯æ¡æŒ‡ä»¤æ¬è¿çš„æ•°æ®é‡å¤ªå°ï¼ˆå¦‚ 4 Bytesï¼‰ï¼ŒæŒ‡ä»¤å‘å°„çš„é€Ÿåº¦å¯èƒ½è·Ÿä¸ä¸Šå†…å­˜æ€»çº¿çš„æ¶ˆè€—é€Ÿåº¦ï¼Œå¯¼è‡´æ€»çº¿å‡ºç°â€œç©ºé—²æ°”æ³¡â€ã€‚

  - **ä¼˜åŠ¿ï¼š** `float4` (16 Bytes/thread) è®©æ¯æ¬¡å‘å°„çš„â€œå«é‡‘é‡â€æ›´é«˜ï¼Œæ›´å®¹æ˜“å¡«æ»¡å†…å­˜ç®¡é“ã€‚

#### 2. æ•°æ®å±‚çº§ï¼šå†…å­˜çº§å¹¶è¡Œåº¦ (MLP, Memory Level Parallelism)

è¿™æ˜¯å†³å®šå¸¦å®½ä¸Šé™çš„å…³é”®è½¯ä»¶ç­–ç•¥ã€‚

- **åŸç†ï¼š** HBM å»¶è¿Ÿæé«˜ (~600 cycles)ã€‚ä¸ºäº†æ©ç›–å»¶è¿Ÿï¼Œå¿…é¡»è®©æ€»çº¿ä¸ŠåŒæ—¶é£ç€è¶³å¤Ÿå¤šçš„è¯·æ±‚ (In-flight Requests)ã€‚

- **ä¼˜åŒ–æ‰‹æ®µï¼š** **å¾ªç¯å±•å¼€ (Loop Unrolling)**ã€‚

  - _Bad:_ `Load -> Use -> Load -> Use` (ä¸²è¡Œä¾èµ–ï¼Œå»¶è¿Ÿæ— æ³•æ©ç›–)ã€‚

  - _Good:_ `Load1 -> Load2 -> Load3 -> Load4 ... -> Use1` (å¹¶è¡Œå‘å°„ï¼Œä¸€æ¬¡ç­‰å¾…ï¼Œå…¨éƒ¨è¿”å›)ã€‚

#### 3. ç¡¬ä»¶å±‚çº§ï¼šä¼ è¾“ç²’åº¦ä¸åˆ©ç”¨ç‡ (Transaction & Utilization)

å³ä½¿è½¯ä»¶å†™å¾—å¥½ï¼Œç¡¬ä»¶æœºåˆ¶ä¹Ÿå¯èƒ½å¯¼è‡´æµªè´¹ã€‚

- **æ‰‡åŒºåˆ©ç”¨ç‡ (Sector Utilization):**

  - **æœºåˆ¶ï¼š** DRAM åˆ° L2 çš„æœ€å°ä¼ è¾“ç²’åº¦æ˜¯ **32 Bytes (Sector)**ã€‚

  - **æµªè´¹ï¼š** å¦‚æœä½ åªè¯» 1 ä¸ª Byte (`char`) ä¸”æœªæ‰“åŒ…ï¼Œç¡¬ä»¶ä¹Ÿè¢«è¿«æ¬è¿ 32 Bytesã€‚**æœ‰æ•ˆå¸¦å®½ (Effective Bandwidth)** åªæœ‰ 1/32ã€‚

  - **å¯¹ç­–ï¼š** å¯¹äºå°æ•°æ®ç±»å‹ï¼ˆINT8/FP16ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ‰“åŒ…å¯¹é½ï¼ˆPack Alignmentï¼‰è®¿é—®ã€‚

- **åœ°å€å¯¹é½ (Address Alignment):**

  - **æœºåˆ¶ï¼š** ç¡¬ä»¶è¦æ±‚è®¿é—®åœ°å€æŒ‰ 32B æˆ– 128B å¯¹é½ã€‚

  - **åæœï¼š** å¦‚æœæŒ‡é’ˆåœ°å€åç§»ï¼ˆMisalignedï¼‰ï¼Œä¸€æ¬¡ 128 Bytes çš„è¯»å–å¯èƒ½ä¼šè·¨è¶Šä¸¤ä¸ª 128B å—ï¼Œå¯¼è‡´ç¡¬ä»¶å¿…é¡»å‘èµ· **2 ä¸ª Transactions**ã€‚è¿™ä¼šç›´æ¥å¯¼è‡´å¸¦å®½æ€§èƒ½å‡åŠã€‚

#### 4. æ¶æ„å±‚çº§ï¼šç‰©ç†å†²çª (Physical Conflict)

é€šå¸¸ç”±ç¡¬ä»¶è§£å†³ï¼Œä½†åœ¨æç«¯ä¼˜åŒ–æ—¶éœ€æ³¨æ„ã€‚

- **åˆ†åŒºå†²çª (Partition Camping / Channel Conflict):**

  - **åŸç†ï¼š** æ˜¾å­˜è¢«åˆ’åˆ†ä¸ºå¤šä¸ªç‰©ç†åˆ†åŒºï¼ˆMemory Controllersï¼‰ã€‚

  - **ç°è±¡ï¼š** ç‰¹å®šçš„è®¿é—®æ­¥é•¿ï¼ˆStrideï¼Œé€šå¸¸æ˜¯ 2 çš„å¹‚æ¬¡ï¼‰å¯èƒ½å¯¼è‡´æ‰€æœ‰è¯·æ±‚é›†ä¸­æ‰“å‘åŒä¸€ä¸ª Controllerï¼Œé€ æˆå±€éƒ¨æ‹¥å µï¼ˆSerializationï¼‰ï¼Œè€Œå…¶ä»– Controller ç©ºé—²ã€‚

  - **ç°çŠ¶ï¼š** ç°ä»£ GPU (Pascal+) å·²é€šè¿‡ç‰©ç†åœ°å€å“ˆå¸Œï¼ˆAddress Swizzlingï¼‰æå¤§ç¼“è§£äº†æ­¤é—®é¢˜ï¼Œä½†åœ¨å†™æé™ Kernel æ—¶ä»éœ€é¿å…å®Œç¾çš„ 2 çš„å¹‚æ¬¡è·¨åº¦ã€‚

#### 5. Asynchronous Copy (cp.async)

åœ¨ Ampere (A100) å’Œ Hopper (H100) æ¶æ„ä¸Šï¼Œä¸ºäº†æè‡´çš„ Double/Multi-stage Bufferingï¼Œæˆ‘ä»¬ä½¿ç”¨å¼‚æ­¥æ‹·è´æŒ‡ä»¤ã€‚

- **æœºåˆ¶ï¼š** `cp.async` (PTX: `cp.async.ca.shared.global`) å…è®¸æ•°æ®ç›´æ¥ä» Global Memory ä¼ è¾“åˆ° Shared Memoryã€‚
- **ä¼˜åŠ¿ï¼š**
  - **Bypass Register File:** æ•°æ®ä¸ç»è¿‡å¯„å­˜å™¨ï¼Œç›´æ¥è¿›å…¥ Shared Memoryï¼Œæå¤§é™ä½å¯„å­˜å™¨å‹åŠ› (Register Pressure)ã€‚
  - **Non-blocking:** å‘å‡ºæŒ‡ä»¤åï¼Œçº¿ç¨‹å¯ä»¥ç»§ç»­æ‰§è¡Œå…¶ä»–è®¡ç®—ï¼ˆå¦‚è®¡ç®—ä¸Šä¸€å—æ•°æ®çš„ GEMMï¼‰ï¼Œå®ç°è®¡ç®—ä¸è®¿å­˜çš„å®Œç¾æµæ°´çº¿æ©ç›–ã€‚
- **æµæ°´çº¿ (Pipeline):**
  - `cp.async.commit_group()`: æäº¤ä¸€ç»„æ‹·è´ä»»åŠ¡ã€‚
  - `cp.async.wait_group<N>()`: ç­‰å¾…ç›´åˆ°åªå‰©ä¸‹ N ç»„æœªå®Œæˆã€‚

### Compute-Bound

#### 1. ä½¿ç”¨ Tensor Cores (WMMA / MMA)

è¿™æ˜¯ç°ä»£ LLM æ¨ç†åŠ é€Ÿçš„**ç»å¯¹æ ¸å¿ƒ**ã€‚åœ¨ LLM æ¨ç†ä¸­ï¼Œ99% çš„ GEMM éƒ½æ˜¯ FP16/INT8/BF16ï¼Œå¿…é¡»è¿è¡Œåœ¨ Tensor Cores ä¸Šæ‰èƒ½è·å¾— 8-16 å€çš„åŠ é€Ÿã€‚

- **åŸç†ï¼š**

  - **CUDA Core (FP32):** SIMT æ¨¡å‹ï¼Œæ¯ä¸ª Thread å¤„ç†ä¸€ä¸ªæ ‡é‡è®¡ç®—ã€‚æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸæ‰§è¡Œ 1 æ¬¡ `FMA (a*b+c)`ã€‚
  - **Tensor Core (FP16/INT8):** Warp-level æ¨¡å‹ï¼Œä¸€ä¸ª Warp (32 threads) ååŒå¤„ç†ä¸€ä¸ªçŸ©é˜µå—ï¼ˆå¦‚ 16x16x16ï¼‰ã€‚æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸæ‰§è¡Œ 1 æ¬¡å®Œæ•´çš„çŸ©é˜µä¹˜åŠ ã€‚
  - **å·®è·ï¼š** A100 ä¸Šï¼ŒTensor Core çš„ååé‡æ˜¯ CUDA Core çš„ **16 å€**ä»¥ä¸Šã€‚

- **å…³é”®æ¦‚å¿µï¼šFragment**

  - æ•°æ®ä¸èƒ½éšæ„æ”¾åœ¨å¯„å­˜å™¨é‡Œï¼Œå¿…é¡»æŒ‰ç…§ Tensor Core è¦æ±‚çš„å¸ƒå±€ï¼ˆLayoutï¼‰å­˜æ”¾ã€‚
  - `nvcuda::wmma::fragment`: è¿™æ˜¯ä¸€ä¸ªæ¨¡æ¿ç±»ï¼Œä»£è¡¨çŸ©é˜µçš„ä¸€ä¸ªå—ï¼ˆTileï¼‰ã€‚

- **API å±‚æ¬¡ï¼š**

  1. **WMMA API (`nvcuda::wmma`)**: C++ æ¨¡æ¿å°è£…ï¼Œæ˜“ç”¨æ€§å¥½ã€‚
     ```cpp
     using namespace nvcuda;
     wmma::fragment<wmma::matrix_a, 16, 16, 16, half, wmma::row_major> a_frag;
     wmma::load_matrix_sync(a_frag, a_ptr, lda);
     wmma::mma_sync(c_frag, a_frag, b_frag, c_frag);
     wmma::store_matrix_sync(c_ptr, c_frag, ldc, wmma::mem_row_major);
     ```
  1. **PTX MMA (`mma.sync`)**: æ›´åº•å±‚ï¼Œæ§åˆ¶åŠ›æ›´å¼ºï¼ŒCutlass å¸¸ç”¨ã€‚

- **ä¼˜åŒ–æ‰‹æ®µï¼š**

  - æŠ›å¼ƒ `c = a * b + c` çš„æ ‡é‡å†™æ³•ã€‚
  - ä½¿ç”¨ `nvcuda::wmma` æˆ– Cutlass åº“ã€‚
  - **æ•°æ®å¸ƒå±€ (Layout):** å¿…é¡»å¤„ç†å¥½ Shared Memory åˆ° Register çš„åŠ è½½ï¼Œé¿å… Bank Conflictã€‚

#### 2. é™ä½ç²¾åº¦ (Precision Reduction)

è®¡ç®—ååé‡ä¸æ•°æ®ç±»å‹çš„ä½å®½æˆåæ¯”

- **åŸç†ï¼š** ç¡¬ä»¶å¤„ç†ä½æ•°è¶Šå°‘ï¼Œå¹¶è¡Œåº¦è¶Šé«˜ã€‚
- **å±‚çº§ï¼š**
  - **FP32:** åŸºå‡† (1x)
  - **TF32 (Ampere+):** Tensor Core ä¸“ç”¨ï¼Œ19 bitsï¼Œååé‡é€šå¸¸æ˜¯ FP32 çš„ high-speed æ¨¡å¼ã€‚
  - **FP16 / BF16:** ååé‡æ˜¯ FP32 çš„ **2x** (CUDA Core) æˆ–æ›´é«˜ (Tensor Core)ã€‚
  - **INT8:** ååé‡æ˜¯ FP16 çš„ **2x**ã€‚
- **ä¼˜åŒ–æ‰‹æ®µï¼š** åœ¨ LLM æ¨ç†ä¸­ï¼Œæƒé‡ï¼ˆWeightï¼‰å’Œæ¿€æ´»ï¼ˆActivationï¼‰é€šå¸¸é‡åŒ–ä¸º **FP16** æˆ– **INT8/W8A16**ã€‚

#### 3. æŒ‡ä»¤çº§å¹¶è¡Œ (ILP, Instruction Level Parallelism)

å³ä½¿æ²¡æœ‰ Tensor Coreï¼ŒSM å†…éƒ¨ä¹Ÿæœ‰æµæ°´çº¿ä¼˜åŒ–ç©ºé—´ã€‚

- **åŸç†ï¼š** ç®—æœ¯æŒ‡ä»¤ï¼ˆå¦‚ FMAï¼‰æœ‰å»¶è¿Ÿï¼ˆLatencyï¼Œæ¯”å¦‚ 4-6 ä¸ªå‘¨æœŸï¼‰
- **ä¼˜åŒ–æ‰‹æ®µï¼š** **å¾ªç¯å±•å¼€ (Loop Unrolling)**ã€‚è¿™èƒ½è®©æ¯ä¸ªçº¿ç¨‹åŒæ—¶ç»´æŒå¤šä¸ªâ€œIn-flightâ€çš„è®¡ç®—æŒ‡ä»¤
  ```cpp
  // å±•å¼€åï¼Œç¼–è¯‘å™¨ä¼šäº¤é”™å‘å°„æŒ‡ä»¤
  a1 = b1 * c1; // å‘å°„
  a2 = b2 * c2; // å‘å°„ï¼Œä¸éœ€è¦ç­‰ a1
  a3 = b3 * c3; // å‘å°„
  // ... æ­¤æ—¶ ALU æµæ°´çº¿è¢«å¡«æ»¡
  ```

#### 4. åŒå‘å°„ (Dual Issue) / å¹¶å‘æ‰§è¡Œ

- **åŸç†ï¼š** ç°ä»£ SMï¼ˆAmpere/Ada/Hopperï¼‰é€šå¸¸åŒ…å«ï¼š

  - FP32 è®¡ç®—å•å…ƒ
  - INT32 è®¡ç®—å•å…ƒ
  - è¿™ä¸¤è€…åœ¨æŸäº›æ¶æ„ä¸Šæ˜¯**ç‹¬ç«‹**çš„ï¼Œå¯ä»¥**åŒæ—¶æ‰§è¡Œ**ã€‚

- **åœºæ™¯ï¼š**

  - `val = val * x + y` (FP32 è®¡ç®—)
  - `ptr += stride` (INT32 æŒ‡é’ˆè®¡ç®—/å¾ªç¯è®¡æ•°)

- **ä¼˜åŒ–æ‰‹æ®µï¼š** ç¼–å†™ Kernel æ—¶ï¼Œå¦‚æœèƒ½è®©æµ®ç‚¹è®¡ç®—ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰å’Œæ•´æ•°è®¡ç®—ï¼ˆåœ°å€ç´¢å¼•ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ï¼ŒSM å¯ä»¥**åœ¨ä¸€ä¸ªå‘¨æœŸå†…åŒæ—¶å‘å°„è¿™ä¸¤æ¡æŒ‡ä»¤**ï¼Œä»è€Œæ©ç›–æ‰ç´¢å¼•è®¡ç®—çš„å¼€é”€ã€‚

#### 5. ä½¿ç”¨ FMA (Fused Multiply-Add)

- **åŸç†ï¼š** `a * b + c`ã€‚
  - å¦‚æœä¸ä¼˜åŒ–ï¼š`MUL` (ä¹˜æ³•) + `ADD` (åŠ æ³•) = 2 æ¡æŒ‡ä»¤ã€‚
  - ä¼˜åŒ–åï¼š`FFMA` (Fused FMA) = 1 æ¡æŒ‡ä»¤ã€‚
- **ä¼˜åŒ–æ‰‹æ®µï¼š** ç¼–è¯‘å™¨é€šå¸¸ä¼šè‡ªåŠ¨ä¼˜åŒ–ã€‚ä½†åœ¨æ‰‹å†™ intrinsic æ—¶ï¼Œç¡®ä¿è°ƒç”¨ `__fmaf_rn(a, b, c)` è€Œä¸æ˜¯åˆ†å¼€å†™ã€‚

### Examples

- **Memory-Bound Vector Add**ï¼š[ä¸€æ­¥æ­¥å®ç° CUDA Vector Add ä¼˜åŒ–](https://tom-jerr.github.io/blogs/posts/Vector%20Add%20Optimization%20Example/)

## CUDA å¸¸ç”¨ä¼˜åŒ–æŠ€å·§

### Maximize Compiler Computation[^gpu-miarch]

- Unroll Loops
  > å±•å¼€å¾ªç¯ï¼ˆloop unrollingï¼‰ï¼Œè®©å¾ªç¯ä½“é‡å¤å±•å¼€å¤šæ¬¡ï¼Œ**å‡å°‘å¾ªç¯æ§åˆ¶å¼€é”€**ï¼ˆæ¯”å¦‚ i++ã€i\<N çš„åˆ¤æ–­ï¼‰ï¼Œæé«˜ GPU çš„ååé‡ã€‚
- Write code using compile-time constants (not same as constant registers)
  > åœ¨ä»£ç é‡Œç”¨ ç¼–è¯‘æœŸå·²çŸ¥çš„å¸¸é‡æ¥åšç´¢å¼•ã€å¾ªç¯æ¬¡æ•°ã€æ•°ç»„å¤§å°ç­‰ï¼Œè€Œä¸æ˜¯ä¾èµ– GPU çš„å¸¸é‡å¯„å­˜å™¨

### Coalescing Memory Access

- å½“ä¸€ä¸ª Warp ä¸­çš„**æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹è®¿é—®å…¨å±€å†…å­˜ä¸­çš„è¿ç»­ä½ç½®æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å°†è¿™ 32 ä¸ªå°çš„è¯·æ±‚â€œåˆå¹¶â€æˆä¸€ä¸ªå•ä¸€ã€å¤§å‹ã€é«˜æ•ˆçš„å†…å­˜äº‹åŠ¡**

- Memory Access Patterns:

  - åˆå¹¶è®¿é—®ï¼ˆç†æƒ³ï¼‰ï¼šWarp ä¸­çš„çº¿ç¨‹ i è®¿é—®å†…å­˜åœ°å€ base + iã€‚è¿™åœ¨å¤„ç†æŒ‰è¡Œä¸»åºå­˜å‚¨çš„çŸ©é˜µçš„è¡Œæ—¶éå¸¸å¸¸è§

    ![](img/coalesce-smem.png)

  - è·¨æ­¥è®¿é—®ï¼ˆé—®é¢˜ï¼‰ï¼šçº¿ç¨‹ i è®¿é—® base + i * strideã€‚å¦‚æœæ­¥é•¿ï¼ˆstrideï¼‰å¾ˆå¤§ï¼Œè¿™å°†å¯¼è‡´è®¸å¤šç‹¬ç«‹çš„ã€ä½æ•ˆçš„å†…å­˜äº‹åŠ¡ã€‚è¿™åœ¨è®¿é—®æŒ‰è¡Œä¸»åºå­˜å‚¨çš„çŸ©é˜µçš„åˆ—æ—¶å¾ˆå¸¸è§

    ![](img/uncoalesce-smem.png)

  - éå¯¹é½è®¿é—®ï¼šWarp è®¿é—®çš„èµ·å§‹åœ°å€æœªä¸å†…å­˜äº‹åŠ¡çš„å¤§å°å¯¹é½

### Avoid Bank Conflicts in Shared Memory

:warning: Shared memory is organized into **32 banks**. Each bank is a slice of SRAM that can load or store **4 bytes of data every cycle**.

![](img/smem.jpg)

- å½“ä¸€ä¸ª Warp ä¸­çš„æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹è®¿é—®å…¨å±€å†…å­˜ä¸­çš„è¿ç»­ä½ç½®æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å°†è¿™ 32 ä¸ªå°çš„è¯·æ±‚ **â€œåˆå¹¶â€æˆä¸€ä¸ªå•ä¸€ã€å¤§å‹ã€é«˜æ•ˆçš„å†…å­˜äº‹åŠ¡**

  ![](img/conflict-free.png)

- å½“åŒä¸€ä¸ª Warp ä¸­çš„ä¸¤ä¸ªæˆ–æ›´å¤šçº¿ç¨‹è¯•å›¾è®¿é—®ä½äºåŒä¸€ä¸ªå†…å­˜é“¶è¡Œä¸­çš„ä¸åŒåœ°å€æ—¶ï¼Œå°±ä¼šå‘ç”Ÿé“¶è¡Œå†²çª ã€‚æ­¤æ—¶ï¼Œè¿™äº›è®¿é—®ä¼šè¢«ä¸²è¡ŒåŒ–å¤„ç†ï¼Œä»è€Œé™ä½äº†å…±äº«å†…å­˜çš„æœ‰æ•ˆå¸¦å®½

  ![](img/bank-conflict.png)

#### Solutions[^bankconflict]

- Padding: åœ¨æ•°æ®ç»“æ„ä¸­æ’å…¥å¡«å……å…ƒç´ ï¼Œä»¥æ”¹å˜æ•°æ®åœ¨å†…å­˜ä¸­çš„å¸ƒå±€ï¼Œé¿å…å¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€é“¶è¡Œ
  ![](img/padding.jpg)
  - å¯èƒ½é™ä½ SM çš„ occupancy
  - **å¯èƒ½åœ°å€è®¿é—®ä¸å¯¹é½ï¼Œæ— æ³•ä½¿ç”¨å‘é‡åŒ–è®¿é—®**
- **Swizzling (XOR Swizzling):** é‡æ–°ç»„ç»‡æ•°æ®çš„å­˜å‚¨æ–¹å¼ï¼Œä½¿å¾—å¹¶è¡Œè®¿é—®æ—¶æ›´å°‘å†²çª(æ›´å¸¸ç”¨):rocket:
  - **åŸç†ï¼š** é€šè¿‡å¯¹åœ°å€ä½è¿›è¡Œå¼‚æˆ–æ“ä½œï¼Œæ‰“ä¹± Bank çš„æ˜ å°„å…³ç³»ã€‚

  - **å…¬å¼ç¤ºä¾‹ï¼š** `bank_id = ((addr / 4) / 8) ^ ((addr / 4) % 32)` (ç®€åŒ–ç‰ˆ)ã€‚

  - **LDGSTS (Load Global Store Shared):** ç°ä»£æ¶æ„ (Ampere+) æä¾›äº† `cp.async` ç­‰æŒ‡ä»¤ï¼Œç¡¬ä»¶è‡ªåŠ¨å¤„ç† Swizzled å­˜å‚¨ï¼Œä¸éœ€è¦ç¨‹åºå‘˜æ‰‹åŠ¨è®¡ç®—å¤æ‚çš„åœ°å€æ˜ å°„ã€‚

  - æŸäº› swizzling æ–¹æ³•åœ¨ä» shared memory è¯»æ•°æ®åˆ° register æ—¶ä¸èƒ½è¿›è¡Œ float4 çš„åˆå¹¶è¯»å–

    ![](img/swizzling.jpg)

  - é€»è¾‘ä½ç½®è¡¨ç¤ºå…ƒç´ åœ¨çŸ©é˜µä¸­çš„é€»è¾‘åæ ‡ã€‚

  - ç‰©ç†ä½ç½®è¡¨ç¤ºå…¶å¯¹åº”å…ƒç´ åœ¨å®é™…å­˜å‚¨æ•°æ®çš„ shared memory ä¸­çš„ä½ç½®åæ ‡ã€‚

    > å½“æˆ‘ä»¬è¯´è¯»å–çŸ©é˜µçš„ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ ï¼Œ(2,3)å°±è¡¨ç¤ºé€»è¾‘ä½ç½®ï¼Œè€ŒçœŸæ­£è¯»å–æ•°æ®çš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦ä»å®é™…å­˜å‚¨æ•°(2,1)çš„ shared memory ä¸­å¯¹åº”çš„ä½ç½®

    ![](img/smem2.jpg)

:warning: å¹¿æ’­ (Broadcast): å¦‚æœä¸€ä¸ª Warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bank ä¸­çš„å®Œå…¨ç›¸åŒçš„åœ°å€ï¼Œè¿™æ˜¯ä¸€ç§å¹¿æ’­æ“ä½œï¼Œä¸ä¼šäº§ç”Ÿå†²çª

#### Matmul Bank Conflict Avoidance Example[^bankconflict2]

```cpp
const int warp_id = tid / 32;
const int lane_id = tid % 32;
const int a_tile_index =  warp_id / 2 * 16 + lane_id / 8 * 4
const int b_tile_index =  warp_id % 2 * 32 + lane_id % 8 * 4;
```

ä½¿ç”¨çš„æ˜¯ 4Ã—2 çš„ warp å¸ƒå±€, warp ä¸­çš„æ¯ä¸ªçº¿ç¨‹æŒ‰ç…§ 4Ã—8 è¿›è¡Œæ’å¸ƒ, æ¯ä¸ª warp å¯¹åº” 16Ã—32 çš„æ•°æ®

- æ¯ä¸ª wrap 32 ä¸ªçº¿ç¨‹åªè·å– As çš„ 4\*4=16 ä¸ªæ•°æ®
- æ¯ä¸ª wrap 32 ä¸ªçº¿ç¨‹åªè·å– Bs çš„ 8\*4=32 ä¸ªæ•°æ®
- **shared memory ä¸­æœ‰ 32 ä¸ª bankï¼Œæ‰€ä»¥ä¸ä¼šäº§ç”Ÿ bank conflict**
  ![](img/avoid-bank-conflict.png)

### Double Buffering[^bankconflict2]

åœ¨å…±äº«å†…å­˜ä¸­åˆ†é…ä¸¤ä¸ªç¼“å†²åŒº: å½“ SM æ­£åœ¨å¯¹ç¼“å†²åŒº 1 ä¸­çš„æ•°æ®è¿›è¡Œè®¡ç®—æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å¼‚æ­¥åœ°å°†ä¸‹ä¸€å—æ•°æ®ä»å…¨å±€å†…å­˜é¢„å–åˆ°ç¼“å†²åŒº 2 ä¸­ã€‚ä¸€æ—¦è®¡ç®—å®Œæˆï¼Œä¸¤ä¸ªç¼“å†²åŒºçš„è§’è‰²äº’æ¢ã€‚è¿™ç§æ–¹å¼æœ‰æ•ˆåœ°å°†å…¨å±€å†…å­˜çš„è®¿é—®å»¶è¿Ÿéšè—åœ¨äº†è®¡ç®—çš„èƒŒå
![](img/double_buffer.png)

```cpp
if (doubleBufferIdx == 0) {
  // load first (B0)
  db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
      N, K, A, B, As, Bs, innerRowA, innerColA, innerRowB, innerColB);
}
__syncthreads();

// outer-most loop over block tiles
for (uint bkIdx = 0; bkIdx < K; bkIdx += 2 * BK) {
  if (doubleBufferIdx == 0) {
    // process current (B0)
    db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN, TM,
                        TN>(regM, regN, threadResults, As, Bs, warpRow,
                            warpCol, threadRowInWarp, threadColInWarp);
    __syncthreads();

    // process current+1 (B1)
    if (bkIdx + BK < K) {
      db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN,
                          TM, TN>(regM, regN, threadResults, As + (BM * BK),
                                  Bs + (BK * BN), warpRow, warpCol,
                                  threadRowInWarp, threadColInWarp);
    }
    __syncthreads();

    // load current + 2 (B0)
    if (bkIdx + 2 * BK < K) {
      db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
          N, K, A + 2 * BK, B + 2 * BK * N, As, Bs, innerRowA, innerColA,
          innerRowB, innerColB);
    }
  } else {
    // load current + 1 (B1)
    if (bkIdx + BK < K) {
      db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
          N, K, A + BK, B + BK * N, As + (BM * BK), Bs + (BK * BN), innerRowA,
          innerColA, innerRowB, innerColB);
    }
    __syncthreads();

    // process current (B0)
    db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN, TM,
                        TN>(regM, regN, threadResults, As, Bs, warpRow,
                            warpCol, threadRowInWarp, threadColInWarp);
    __syncthreads();

    // process current+1 (B1)
    if (bkIdx + BK < K) {
      db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN,
                          TM, TN>(regM, regN, threadResults, As + (BM * BK),
                                  Bs + (BK * BN), warpRow, warpCol,
                                  threadRowInWarp, threadColInWarp);
    }
  }

  A += 2 * BK;     // move BK columns to right
  B += 2 * BK * N; // move BK rows down
  __syncthreads();
}
```

### Warp Shuffle & Reduction

åœ¨ Softmax å’Œ LayerNorm ç­‰ Memory-bound ç®—å­ä¸­ï¼Œç“¶é¢ˆå¾€å¾€åœ¨äºè§„çº¦ (Reduction) æ“ä½œï¼ˆå¦‚æ±‚ Max æˆ– Sumï¼‰ã€‚

- **ä¼ ç»Ÿæ–¹æ³•ï¼š** ä½¿ç”¨ Shared Memory è¿›è¡Œæ ‘çŠ¶è§„çº¦ã€‚ç¼ºç‚¹æ˜¯éœ€è¦é¢‘ç¹è¯»å†™ Shared Memoryï¼Œä¸”éœ€è¦ `__syncthreads()` åŒæ­¥ã€‚
- **ä¼˜åŒ–æ–¹æ³•ï¼š** ä½¿ç”¨ Warp Shuffle API åœ¨å¯„å­˜å™¨ä¹‹é—´ç›´æ¥é€šä¿¡ã€‚
  - `__shfl_down_sync(mask, val, delta)`: å°†æ•°æ®ä»å½“å‰çº¿ç¨‹å‘ä¸‹ä¼ é€’ç»™ ID ä¸º `tid + delta` çš„çº¿ç¨‹ã€‚
  - `__shfl_xor_sync(mask, val, laneMask)`: è´è¶äº¤æ¢ (Butterfly Exchange)ï¼Œç”¨äºå…¨è§„çº¦ã€‚
- **Block Reduce:** å½“ä¸€è¡Œæ•°æ®è¶…è¿‡ 32 ä¸ªï¼ˆæ¯”å¦‚ hidden_size=4096ï¼‰æ—¶ï¼Œå¦‚ä½•è·¨ Warp è¿›è¡Œè§„çº¦ï¼Ÿ
  - ç­–ç•¥ï¼šWarp Reduce -> å­˜å…¥ Shared Memory -> æœ€åä¸€ä¸ª Warp å†è¯»å‡ºæ¥ Reduceã€‚

### Tile[^matmul]

- å°†åŸæœ¬ä¸€è¡Œ Ã— ä¸€åˆ—çš„è®¡ç®—è¿›è¡Œåˆ†å—ï¼Œæ¯æ¬¡åªè®¡ç®—ä¸€å—

- ä¸€æ¬¡æ€§ä»å…¨å±€å†…å­˜ä¸­åŠ è½½ä¸€å°å— A (BM x BK) å’Œä¸€å°å— B (BK x BN) åˆ°å…±äº«å†…å­˜ä¸­

- ä¸€ä¸ªçº¿ç¨‹å—å†…çš„æ‰€æœ‰çº¿ç¨‹å°±å¯ä»¥åœ¨å…±äº«å†…å­˜ä¸Šå¿«é€Ÿåœ°è¿›è¡Œå¤§é‡çš„è®¡ç®—ï¼Œä»¥å®Œæˆå¯¹åº”çš„ä¸€å°å— C (BM x BN) çš„è®¡ç®—

- æ¯ä¸ªçº¿ç¨‹ä¸å†æ˜¯åªè®¡ç®— C å—ä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œè€Œæ˜¯è´Ÿ**è´£è®¡ç®—ä¸€ä¸ªæ›´å°çš„ç»“æœç½‘æ ¼**ï¼ˆå›¾ä¸­æ˜¯ 2x2ï¼‰ã€‚è¿™æ ·åšå¯ä»¥è¿›ä¸€æ­¥æå‡æ•°æ®å¤ç”¨ç‡å’Œè®¡ç®—æ•ˆç‡

  ![](img/tile2.png)

### Wrap Tile[^matmul]

- **çº¿ç¨‹å—åˆ†ç‰‡(blocktiling)**: ä¸åŒçš„çº¿ç¨‹å—å¯ä»¥åœ¨ä¸åŒçš„ SM ä¸Šå¹¶è¡Œæ‰§è¡Œ.
- **warp åˆ†ç‰‡(warptiling)**: ä¸åŒçš„ warp å¯ä»¥åœ¨ä¸åŒçš„ warp è°ƒåº¦å™¨ä¸Šå¹¶è¡Œæ‰§è¡Œ, ä¹Ÿå¯ä»¥åœ¨åŒä¸€ä¸ª warp è°ƒåº¦å™¨ä¸Šå¹¶å‘æ‰§è¡Œ.
- **çº¿ç¨‹åˆ†ç‰‡(threadtiling)**: (æ•°é‡éå¸¸æœ‰é™çš„)æŒ‡ä»¤å¯ä»¥åœ¨ç›¸åŒçš„ CUDA å†…æ ¸ä¸Šå¹¶è¡Œæ‰§è¡Œ(=æŒ‡ä»¤çº§å¹¶è¡Œ, instruction-level parallelism, å³ ILP).

![](img/warp_tile.png)

```cpp
for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {
  // load from gmem
  for (uint offset = 0; offset + rowStrideA <= BM; offset += rowStrideA) {
    const float4 tmp = reinterpret_cast<const float4 *>(
        &A[(innerRowA + offset) * K + innerColA * 4])[0];
    // float4 tmp;
    // asm("ld.global.nc.v4.f32 {%0, %1, %2, %3}, [%4];"
    //     : "=f"(tmp.x), "=f"(tmp.y), "=f"(tmp.z), "=f"(tmp.w)
    //     : "l"(&A[(innerRowA + offset) * K + innerColA * 4]));
    As[(innerColA * 4 + 0) * BM + innerRowA + offset] = tmp.x;
    As[(innerColA * 4 + 1) * BM + innerRowA + offset] = tmp.y;
    As[(innerColA * 4 + 2) * BM + innerRowA + offset] = tmp.z;
    As[(innerColA * 4 + 3) * BM + innerRowA + offset] = tmp.w;
  }

  for (uint offset = 0; offset + rowStrideB <= BK; offset += rowStrideB) {
    reinterpret_cast<float4 *>(
        &Bs[(innerRowB + offset) * BN + innerColB * 4])[0] =
        reinterpret_cast<const float4 *>(
            &B[(innerRowB + offset) * N + innerColB * 4])[0];
    // asm("ld.global.v4.f32 {%0, %1, %2, %3}, [%4];"
    //     : "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 0]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 1]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 2]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 3])
    //     : "l"(&B[(innerRowB + offset) * N + innerColB * 4]));
  }
  __syncthreads();
  // dotIdx loops over contents of SMEM
  for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
    // å®é™…ä¸Šæ˜¯ä¿®æ”¹ warp çš„å¸ƒå±€ï¼Œå˜æˆ 2x2 çš„å¸ƒå±€
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
      for (uint i = 0; i < TM; ++i) {
        regM[wSubRowIdx * TM + i] =
            As[(dotIdx * BM) + warpRow * WM + wSubRowIdx * WSUBM +
              threadRowInWarp * TM + i];
      }
    }
    for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
      for (uint i = 0; i < TN; ++i) {
        regN[wSubColIdx * TN + i] =
            Bs[(dotIdx * BN) + warpCol * WN + wSubColIdx * WSUBN +
              threadColInWarp * TN + i];
      }
    }

    // æ¯æ¬¡æ¯ä¸ª warp åŠ è½½ 32 * 16 ä¸ªæ•°æ®ï¼Œä¸ä¼šäº§ç”Ÿ bank conflictï¼Œéœ€è¦è¿­ä»£ 4 æ¬¡
    // æ¯ä¸ª warp å¤šæ¬¡è¿­ä»£çš„æ•°æ®æ˜¯è¿ç»­çš„ä¸€å—, ä¸åŒ warp åŒä¸€æ¬¡è¿­ä»£çš„æ•°æ®åˆ™æ˜¯åˆ†æ•£çš„å¤šå—
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
      for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
        // calculate per-thread results with register-cache locality
        for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
          for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
            threadResults[(wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                          (wSubColIdx * TN) + resIdxN] +=
                regM[wSubRowIdx * TM + resIdxM] *
                regN[wSubColIdx * TN + resIdxN];
          }
        }
      }
    }
  }
  A += BK;     // move BK columns to right
  B += BK * N; // move BK rows down
  __syncthreads();
}
```

## Transformer å†…éƒ¨ç®—å­ä¼˜åŒ–

### Softmax & LayerNorm ä¼˜åŒ–

è¿™ä¸¤ä¸ªç®—å­æ˜¯å…¸å‹çš„ Memory-bound ç®—å­ã€‚

- **Softmax:** $Softmax(x_i) = \\frac{e^{x_i - x\_{max}}}{\\sum e^{x_j - x\_{max}}}$
  - éœ€è¦ä¸¤æ¬¡è§„çº¦ï¼šä¸€æ¬¡æ±‚ Maxï¼Œä¸€æ¬¡æ±‚ Sumã€‚
  - ä¼˜åŒ–æ ¸å¿ƒï¼šä½¿ç”¨ Warp Shuffle è¿›è¡Œå¯„å­˜å™¨çº§è§„çº¦ï¼Œå‡å°‘ Shared Memory è®¿é—®ã€‚
- **LayerNorm:** éœ€è¦æ±‚ Mean å’Œ Varianceã€‚
  - åŒæ ·ä¾èµ–é«˜æ•ˆçš„ Block Reduceã€‚

### Flash Attention ä¼˜åŒ–

Standard Attention çš„ç—›ç‚¹åœ¨äº $N \\times N$ çš„ Attention Matrix å¤ªå¤§ï¼Œå†™å› HBM å†è¯»å›æ¥åš Softmaxï¼Œå¸¦å®½æ’‘ä¸ä½ã€‚

- **æ ¸å¿ƒæ€æƒ³ï¼š**
  1. **Tiling (åˆ†å—):** å°† Q, K, V åˆ‡åˆ†æˆå°å—ï¼ŒåŠ è½½åˆ° SRAM (Shared Memory) ä¸­è®¡ç®—ã€‚
  1. **Recomputation (é‡è®¡ç®—):** ä¸ºäº†ä¸å­˜å‚¨ $N \\times N$ çš„ä¸­é—´çŸ©é˜µï¼Œåå‘ä¼ æ’­æ—¶å®æ„¿é‡æ–°è®¡ç®—ä¸€éå‰å‘è¿‡ç¨‹ã€‚
  1. **Online Softmax:** è¿™æ˜¯ä¸€ä¸ªæ•°å­¦æŠ€å·§ï¼Œå…è®¸ Softmax åˆ†å—è®¡ç®—ï¼Œä¸éœ€è¦ç­‰çœ‹åˆ°å…¨å±€æœ€å¤§å€¼ã€‚
     - ç»´æŠ¤å±€éƒ¨çš„ max å’Œ sumï¼Œå½“å¤„ç†æ–°çš„å—æ—¶ï¼Œæ›´æ–°å…¨å±€çš„ max å’Œ sumã€‚

## å‚è€ƒèµ„æ–™

\[^cuda1\]: [CUDAï¼ˆä¸€ï¼‰ï¼šCUDA ç¼–ç¨‹åŸºç¡€](https://zhuanlan.zhihu.com/p/645330027)
\[^cuda2\]: [CUDAï¼ˆäºŒï¼‰ï¼šGPU çš„å†…å­˜ä½“ç³»åŠå…¶ä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/654027980)
\[^matmul\]: [[CUDA å­¦ä¹ ç¬”è®°] å¦‚ä½•ä¼˜åŒ– CUDA çŸ©é˜µä¹˜å†…æ ¸ä»¥è·å¾—ç±»ä¼¼ cuBLAS çš„æ€§èƒ½: å·¥ä½œæ—¥å¿—](https://blog.csdn.net/LostUnravel/article/details/138034380#t12)
\[^gpu-miarch\]: [EE 7722, Lecture Slides: NVIDIA GPU Microarchitecture](https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf)
\[^bankconflict\]: [CUDA shared memory é¿å… bank conflict çš„ swizzling æœºåˆ¶è§£æ](https://zhuanlan.zhihu.com/p/4746910252)
\[^bankconflict2\]: [GEMM ä¼˜åŒ–: åŒç¼“å†² (Prefetch) å’Œ Bank Conflict è§£å†³](https://zhuanlan.zhihu.com/p/696844342)
