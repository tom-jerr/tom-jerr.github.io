---
title: CUDA Optimization for LLM Inference
tags:
  - CUDA
  - LLMInference
date: 2025/11/3
---

# CUDA Optimization for LLM Inference

## Overview

åœ¨å¤§è§„æ¨¡è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰æ¨ç†ä¸­ï¼Œä¼˜åŒ– CUDA ä»£ç å¯¹äºæå‡æ€§èƒ½å’Œæ•ˆç‡è‡³å…³é‡è¦ã€‚æœ¬æ–‡æ¡£ä»‹ç»äº†ä¸€äº›å…³é”®çš„ CUDA ä¼˜åŒ–æŠ€æœ¯ï¼Œå¸®åŠ©å¼€å‘è€…æ›´å¥½åœ°åˆ©ç”¨ GPU èµ„æºè¿›è¡Œ LLM æ¨ç†ã€‚è¿™é‡Œæˆ‘ä»¬å°†ä»‹ç» Transformer-based è‡ªå›å½’é¢„è®­ç»ƒæ¨¡å‹æ¨ç†ç”¨åˆ°çš„ç®—å­çš„ä¼˜åŒ–æ–¹æ³•ã€‚è¿™äº›ä¼˜åŒ–çš„æ–¹æ³•ä¹Ÿé€‚ç”¨äºå…¶ä»–ç®—å­ã€‚

æˆ‘ä»¬ä¸»è¦å…³æ³¨ä»¥ä¸‹å‡ ä¸ªæ–¹é¢ï¼š

1. GPU ç¡¬ä»¶æ¶æ„(Hirerarchy Memory, SM, Warp ç­‰)
2. CUDA è®¡ç®—æ¨¡å‹(Thread, Thread Block, Grid ç­‰)
3. CUDA Kernel æ€§èƒ½è°ƒä¼˜(Nsight Compute, Occupancy)
4. CUDA å¸¸ç”¨ä¼˜åŒ–æŠ€å·§(Double buffering, Memory Coalescing, Overcoming Bank Conflict ç­‰)
5. Transformer å†…éƒ¨ç®—å­ä¼˜åŒ–
   - Matrix Multiplication ä¼˜åŒ–
   - Softmax ä¼˜åŒ–
   - LayerNorm ä¼˜åŒ–
   - Self-Attention ä¼˜åŒ–
   - Flash Attention ä¼˜åŒ–

## GPU ç¡¬ä»¶æ¶æ„

GPU ä»¥ Throughput ä¸ºè®¾è®¡ç›®æ ‡ï¼Œå’Œ CPU æœ‰å¾ˆå¤§çš„ä¸åŒã€‚

- GPU ä¸­è™½æœ‰**ç¼“å­˜ç»“æ„ä½†æ˜¯æ•°é‡**å°‘ã€‚ å› ä¸ºè¦å‡å°‘æŒ‡ä»¤è®¿é—®ç¼“å­˜çš„æ¬¡æ•°ã€‚
- GPU ä¸­**æ§åˆ¶å•å…ƒéå¸¸ç®€å•**ã€‚ æ§åˆ¶å•å…ƒä¸­æ²¡æœ‰åˆ†æ”¯é¢„æµ‹æœºåˆ¶å’Œæ•°æ®è½¬å‘æœºåˆ¶ï¼Œå¯¹äºå¤æ‚çš„æŒ‡ä»¤è¿ç®—å°±ä¼šæ¯”è¾ƒæ…¢ã€‚
- GPU çš„**è¿ç®—å•å…ƒ (Core) éå¸¸å¤š**ï¼Œé‡‡ç”¨é•¿å»¶æ—¶æµæ°´çº¿ä»¥å®ç°é«˜ååé‡ã€‚ æ¯ä¸€è¡Œçš„è¿ç®—å•å…ƒçš„æ§åˆ¶å™¨åªæœ‰ä¸€ä¸ªï¼Œæ„å‘³ç€æ¯ä¸€è¡Œçš„è¿ç®—å•å…ƒä½¿ç”¨çš„æŒ‡ä»¤æ˜¯ç›¸åŒçš„ï¼Œä¸åŒçš„æ˜¯å®ƒä»¬çš„æ•°æ®å†…å®¹ã€‚é‚£ä¹ˆè¿™ç§æ•´é½åˆ’ä¸€çš„è¿ç®—æ–¹å¼ä½¿å¾— GPU å¯¹äºé‚£äº›æ§åˆ¶ç®€å•ä½†è¿ç®—é«˜æ•ˆçš„æŒ‡ä»¤çš„æ•ˆç‡æ˜¾è‘—å¢åŠ ã€‚[^cuda1]

![](img/gpu_hardware.jpg)

### Streaming Multiprocessor (SM)[^gpu-miarch]

- Functional Units (FP32, INT32, SF, LS)
- Data Storage
  - Registers
  - Constant Cache (Const.)
  - Shared Memory
- Warp Contexts (Warp Ctxs)
  - One for each warp assigned to SM.
  - Holds PC (addr of next insn to execute)
  - Execution state
- Warp Scheduler
  - round-robin policy

![](img/gpu_sm.png)

#### Functional Units[^gpu-miarch]

- CPU å¯ä»¥é€šè¿‡é‡æ–°è°ƒåº¦æŒ‡ä»¤æ¥é¿å… stalls
  - æŒ‡ä»¤é‡æ’åº: å°†æ— å…³æŒ‡ä»¤æ’å…¥åˆ°æœ‰ä¾èµ–å…³ç³»çš„æŒ‡ä»¤ä¹‹é—´
  - æ•°æ®å‰é€’: ç¡¬ä»¶å±‚é¢çš„ä¼˜åŒ–ï¼Œå…è®¸æ•°æ®åœ¨æµæ°´çº¿ä¸­æå‰ä¼ é€’
    ![](img/fu_latency.png)
- GPU å¯ä»¥é€šè¿‡ çº¿ç¨‹åˆ‡æ¢ï¼ˆthread switching / warp schedulingï¼‰ æ¥è§„é¿å»¶è¿Ÿå¸¦æ¥çš„åœé¡¿ã€‚
  > GPU æœ‰æˆç™¾ä¸Šåƒä¸ªçº¿ç¨‹ï¼ˆæˆ– warpï¼‰ï¼Œå½“æŸä¸ªçº¿ç¨‹åœ¨ç­‰å¾…åŠŸèƒ½å•å…ƒç»“æœæ—¶ï¼ŒGPU å¯ä»¥ç›´æ¥è°ƒåº¦å¦ä¸€ä¸ªçº¿ç¨‹æ‰§è¡Œã€‚è¿™æ ·ï¼Œå»¶è¿Ÿä¸ä¼šç›´æ¥å½±å“æ•´ä½“ååé‡

![](img/gpu_latency.png)

#### Warp Contexts[^gpu-miarch]

warp contexts çš„æ•°é‡å†³å®šäº† SM ä¸Šèƒ½åŒæ—¶å¹¶å‘çš„ block æ•°é‡
![](img/wrap_contexts.png)

#### Warp Scheduler[^gpu-miarch]

- ç¡¬ä»¶æ¥å†³å®šå“ªä¸ª wrap å°†ä¸‹ä¸€ä¸ªæ‰§è¡Œ
- **Branches and Warp Divergence**
  1. NVIDIA 7.0 ä¹‹å‰ï¼Œå½“ warp é‡åˆ°åˆ†æ”¯æ—¶ï¼Œå¿…é¡»ç­‰åˆ°æœ€å¤–å±‚çš„é‡æ–°æ”¶æ•›ç‚¹æ‰èƒ½ç»§ç»­æ‰§è¡Œ
  ```cpp
  if (condition) {
  // Branch A: ä¸€äº›çº¿ç¨‹æ‰§è¡Œè¿™é‡Œ
  instruction_a1();
  instruction_a2();
  } else {
  // Branch B: å¦ä¸€äº›çº¿ç¨‹æ‰§è¡Œè¿™é‡Œ
  instruction_b1();
  instruction_b2();
  instruction_b3(); // æ›´å¤šæŒ‡ä»¤
  }
  // é‡æ–°æ”¶æ•›ç‚¹: æ‰€æœ‰çº¿ç¨‹åœ¨è¿™é‡Œæ±‡åˆ
  instruction_after_branch();
  ```
  2.  æ›¿ä»£æ–¹æ¡ˆï¼šè°ƒåº¦å™¨å¯ä»¥é€‰æ‹©ä»»æ„å…·æœ‰ç›¸åŒ PC å€¼çš„çº¿ç¨‹å­é›†æ¥æ‰§è¡Œï¼Œä¸éœ€è¦ç­‰å¾…æœ€å¤–å±‚çš„æ”¶æ•›ç‚¹
  ```shell
  æ—¶åˆ»1: è°ƒåº¦å™¨é€‰æ‹©8ä¸ªåœ¨Branch Açš„çº¿ç¨‹æ‰§è¡Œ
  æ—¶åˆ»2: è°ƒåº¦å™¨é€‰æ‹©16ä¸ªåœ¨Branch Bçš„çº¿ç¨‹æ‰§è¡Œ
  æ—¶åˆ»3: è°ƒåº¦å™¨é€‰æ‹©å‰©ä½™çš„çº¿ç¨‹ç»§ç»­æ‰§è¡Œ
  ...
  ```
  3. é—®é¢˜
  - ä»éœ€è¦ä¸ºæ‰€æœ‰æœª masked çš„çº¿ç¨‹è§£ç ç›¸åŒæŒ‡ä»¤
    è§£ç å¤æ‚åº¦æ²¡æœ‰é™ä½
  - éœ€è¦æ›´å¤æ‚çš„è°ƒåº¦é€»è¾‘ï¼Œè¦è¿½è¸ªæ¯ä¸ªçº¿ç¨‹çš„çŠ¶æ€ï¼Œè¦å†³å®šæœ€ä½³çš„çº¿ç¨‹å­é›†ç»„åˆ
  - æ¯æ¬¡éƒ½è¦è®¡ç®—æœ€ä¼˜çš„çº¿ç¨‹è°ƒåº¦ç»„åˆï¼Œå¢åŠ äº†è°ƒåº¦å¼€é”€

### GPU Memory Hierarchy[^cuda2]

- Global Memory: å…¨å±€å†…å­˜çš„ä¸»è¦è§’è‰²æ˜¯ä¸ºæ ¸å‡½æ•°æä¾›æ•°æ®ï¼Œå¹¶åœ¨ä¸»æœºä¸è®¾å¤‡åŠè®¾å¤‡ä¸è®¾å¤‡ä¹‹é—´ä¼ é€’æ•°æ®
- Constant Memory: ç‰¹æ®Šçš„å¸¸é‡å†…å­˜ç¼“å­˜ï¼ˆconstant cacheï¼‰è¿›è¡Œç¼“å­˜è¯»å–ï¼Œå¸¸é‡å†…å­˜ä¸ºåªè¯»å†…å­˜
- Texture Memory & Surface Memory: çº¹ç†å†…å­˜å’Œè¡¨é¢å†…å­˜ç±»ä¼¼äºå¸¸é‡å†…å­˜ï¼Œä¹Ÿæ˜¯ä¸€ç§å…·æœ‰ç¼“å­˜çš„å…¨å±€å†…å­˜ï¼Œæœ‰ç›¸åŒçš„å¯è§èŒƒå›´å’Œç”Ÿå‘½å‘¨æœŸï¼Œè€Œä¸”ä¸€èˆ¬ä»…å¯è¯»(è¡¨é¢å†…å­˜ä¹Ÿå¯å†™)
- Register: å¯„å­˜å™¨çš„é€Ÿåº¦æ˜¯è®¿é—®ä¸­æœ€å¿«çš„ï¼Œä½†æ˜¯å®ƒçš„å®¹é‡è¾ƒå°ã€‚
- Local Memory: å±€éƒ¨å†…å­˜æ˜¯æ¯ä¸ªçº¿ç¨‹ç§æœ‰çš„å†…å­˜ç©ºé—´ï¼Œä½†ä»ç¡¬ä»¶æ¥çœ‹ï¼Œ**å±€éƒ¨å†…å­˜åªæ˜¯å…¨å±€å†…å­˜çš„ä¸€éƒ¨åˆ†**ã€‚æ‰€ä»¥ï¼Œå±€éƒ¨å†…å­˜çš„å»¶è¿Ÿä¹Ÿå¾ˆé«˜
- Shared Memory: å…±äº«å†…å­˜æ˜¯æ¯ä¸ªçº¿ç¨‹å—ï¼ˆblockï¼‰å†…æ‰€æœ‰çº¿ç¨‹å…±äº«çš„å†…å­˜ç©ºé—´ã€‚å…±äº«å†…å­˜çš„è®¿é—®å»¶è¿Ÿè¿œä½äºå…¨å±€å†…å­˜
- L1 / L2 Cache: L1 ç¼“å­˜æ˜¯æ¯ä¸ª SM ç‹¬æœ‰çš„ï¼Œ**è€Œ L2 ç¼“å­˜æ˜¯æ‰€æœ‰ SM å…±äº«çš„**

![](img/gpu_hardware.jpg)

:laughing:**Summary**

- æ¯ä¸ª thread éƒ½æœ‰è‡ªå·±çš„ä¸€ä»½ register å’Œ local memory çš„ç©ºé—´
- åŒä¸€ä¸ª block ä¸­çš„æ¯ä¸ª thread åˆ™æœ‰å…±äº«çš„ä¸€ä»½ share memory
- æ‰€æœ‰çš„ thread (åŒ…æ‹¬ä¸åŒ block çš„ thread) éƒ½å…±äº«ä¸€ä»½ global memory
- ä¸åŒçš„ grid åˆ™æœ‰å„è‡ªçš„ global memoryã€‚

  | å±‚çº§                      | ç±»å‹     | ç‰¹æ€§                                   |
  | ------------------------- | -------- | -------------------------------------- |
  | Registers                 | å¯„å­˜å™¨   | æ¯çº¿ç¨‹ç§æœ‰ï¼Œå»¶è¿Ÿæœ€ä½                   |
  | Shared Memory             | å…±äº«å†…å­˜ | æ¯ block å…±äº«ï¼Œä½å»¶è¿Ÿ (~100x DRAM å¿«)  |
  | L1 / L2 Cache             | Cache    | SM å±€éƒ¨ / GPU å…¨å±€ç¼“å­˜                 |
  | Global Memory             | å…¨å±€å†…å­˜ | æ‰€æœ‰ SM å…±äº«ï¼Œå»¶è¿Ÿé«˜ (~400-600 cycles) |
  | Constant / Texture Memory | åªè¯»ç¼“å­˜ | é€‚åˆå¹¿æ’­æ•°æ®ï¼Œç¼“å­˜ä¼˜åŒ–è®¿å­˜             |

### Warp

ç¨‹åºå‘˜ä¸ºå•ä¸ªçº¿ç¨‹ç¼–å†™ä»£ç ï¼Œä½†ç¡¬ä»¶å±‚é¢ä¼šå°†çº¿ç¨‹ç»„ç»‡æˆå›ºå®šå¤§å°ï¼ˆ32 ä¸ªï¼‰çš„æŸï¼Œç§°ä¸º Warp ã€‚Warp æ˜¯ SM ä¸Šè°ƒåº¦å’Œæ‰§è¡Œçš„çœŸæ­£åŸºæœ¬å•ä½

- SIMT æ‰§è¡Œï¼šä¸€ä¸ª Warp ä¸­çš„æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹åœ¨åŒä¸€æ—¶åˆ»æ‰§è¡Œç›¸åŒçš„æŒ‡ä»¤ï¼Œä½†å¤„ç†ä¸åŒçš„æ•°æ®ã€‚
- Warp åˆ†åŒ– (Warp Divergence)ï¼šå¦‚æœä¸€ä¸ª Warp å†…çš„çº¿ç¨‹å› æ¡ä»¶åˆ¤æ–­è€Œèµ°å‘äº†ä¸åŒçš„ä»£ç è·¯å¾„ï¼Œç¡¬ä»¶å¿…é¡»ä¸²è¡ŒåŒ–æ‰§è¡Œè¿™äº›è·¯å¾„
  > å®ƒä¼šé¦–å…ˆä¸ºèµ°å‘ if åˆ†æ”¯çš„çº¿ç¨‹æ‰§è¡Œä»£ç ï¼Œå±è”½æ‰å…¶ä»–çº¿ç¨‹ï¼›ç„¶åå†ä¸ºèµ°å‘ else åˆ†æ”¯çš„çº¿ç¨‹æ‰§è¡Œä»£ç ã€‚è¿™ç§åˆ†åŒ–ä¼šä¸¥é‡é™ä½æ€§èƒ½ï¼Œ**å› ä¸ºåœ¨æ¯ä¸ªåˆ†æ”¯çš„æ‰§è¡Œè¿‡ç¨‹ä¸­ï¼Œæ€»æœ‰ä¸€éƒ¨åˆ†çº¿ç¨‹å¤„äºç©ºé—²çŠ¶æ€** ã€‚
- å»¶è¿Ÿéšè— (Latency Hiding)ï¼šå½“ä¸€ä¸ª Warp åœé¡¿ï¼ˆä¾‹å¦‚ï¼Œç­‰å¾…ä»å…¨å±€å†…å­˜è¯»å–æ•°æ®ï¼‰æ—¶ï¼ŒSM ä¸Šçš„ Warp è°ƒåº¦å™¨ä¼šç«‹å³åˆ‡æ¢åˆ°å¦ä¸€ä¸ªâ€œå‡†å¤‡å°±ç»ªâ€çš„ Warp æ¥æ‰§è¡Œã€‚

## CUDA è®¡ç®—æ¨¡å‹[^cuda2]

CUDA å°†è®¡ç®—ä»»åŠ¡ç»„ç»‡æˆä¸€ä¸ªä¸‰çº§å±‚æ¬¡ç»“æ„ ã€‚è¿™æ˜¯ä¸€ä¸ªç”±ç¨‹åºå‘˜åˆ›å»ºçš„ã€ç”¨äºç»„ç»‡é—®é¢˜çš„**é€»è¾‘å±‚æ¬¡**ï¼Œè€Œéç¡¬ä»¶çš„ç›´æ¥ä½“ç° ã€‚

- çº¿ç¨‹ (Thread)ï¼šæœ€åŸºæœ¬çš„æ‰§è¡Œå•ä½ã€‚å•ä¸ªçº¿ç¨‹æ‰§è¡Œä¸€ä¸ª Kernel å‡½æ•°çš„å®ä¾‹ ã€‚æ¯ä¸ªçº¿ç¨‹åœ¨å…¶æ‰€å±çš„çº¿ç¨‹å—å†…æ‹¥æœ‰ä¸€ä¸ªå”¯ä¸€çš„ ID(threadIdx)
- çº¿ç¨‹å—(Thread Block)ï¼šä¸€ç»„å¯ä»¥ç›¸äº’åä½œçš„çº¿ç¨‹ï¼ˆåœ¨ç°ä»£æ¶æ„ä¸Šæœ€å¤š 1024 ä¸ªï¼‰ã€‚ä¸€ä¸ªå—å†…çš„æ‰€æœ‰çº¿ç¨‹å¯ä»¥é€šè¿‡é«˜é€Ÿçš„ç‰‡ä¸Š Â  å…±äº«å†…å­˜å…±äº«æ•°æ®ï¼Œå¹¶èƒ½é€šè¿‡`__syncthreads()`æ¥åè°ƒå®ƒä»¬çš„æ‰§è¡Œ ã€‚æ¯ä¸ªçº¿ç¨‹å—åœ¨å…¶æ‰€å±çš„ Grid å†…ä¹Ÿæ‹¥æœ‰ä¸€ä¸ªå”¯ä¸€çš„ ID(blockIdx)
- ç½‘æ ¼ (Grid)ï¼šä¸ºæ‰§è¡Œå•ä¸ª Kernel è€Œå¯åŠ¨çš„æ‰€æœ‰çº¿ç¨‹å—çš„é›†åˆ ã€‚ä¸€ä¸ª Grid å†…çš„æ‰€æœ‰çº¿ç¨‹éƒ½å¯ä»¥è®¿é—®åŒä¸€ä¸ªå…¨å±€å†…å­˜ç©ºé—´ã€‚Grid å†…çš„çº¿ç¨‹å—è¢«å‡å®šä¸ºç‹¬ç«‹æ‰§è¡Œï¼Œä¸”æ‰§è¡Œé¡ºåºä¸ç¡®å®šï¼›å®ƒä»¬ä¹‹é—´æ²¡æœ‰ç›´æ¥çš„åŒæ­¥æœºåˆ¶ã€‚

çº¿ç¨‹å—å’Œç½‘æ ¼å¯ä»¥è¢«ç»„ç»‡æˆä¸€ç»´ã€äºŒç»´æˆ–ä¸‰ç»´çš„ç»“æ„ï¼Œè¿™ä¸ºå°†è®¡ç®—ä»»åŠ¡æ˜ å°„åˆ°å‘é‡ã€çŸ©é˜µã€ä½“æ•°æ®ç­‰æ•°æ®ç»“æ„ä¸Šæä¾›äº†æå¤§çš„ä¾¿åˆ© ã€‚ Â 
![](img/grid.jpg)

### ä¸ç¡¬ä»¶çš„å¯¹åº”

#### Thread

- åœ¨æŸä¸€æ—¶åˆ»ï¼Œä¸€ä¸ª CUDA core æ‰§è¡Œä¸€ä¸ªçº¿ç¨‹çš„æŒ‡ä»¤ã€‚

- :warning: å®é™…ä¸ŠåŒä¸€ä¸ª CUDA core ä¼šåœ¨å¤šä¸ª thread ä¹‹é—´åˆ†æ—¶å¤ç”¨ï¼Œå› ä¸ºçº¿ç¨‹æ•°è¿œå¤šäºæ ¸å¿ƒæ•°

#### Thread Block

- ä¸€ä¸ª block è¢«åˆ†é…åˆ°ä¸€ä¸ª SM (Streaming Multiprocessor) ä¸Šæ‰§è¡Œã€‚
- ä¸€ä¸ª block ä¸­çš„çº¿ç¨‹ä¼šè¢«åˆ†æˆè‹¥å¹² warpã€‚SM å†…æœ‰å¤šä¸ªèµ„æºï¼ˆå¯„å­˜å™¨æ–‡ä»¶ã€å…±äº«å†…å­˜ï¼‰ï¼Œè¿™äº›èµ„æºåœ¨ä¸åŒ block ä¹‹é—´åˆ†é…ã€‚

#### Grid

- ä¸€ä¸ª grid å¯¹åº”ä¸€æ¬¡ kernel å¯åŠ¨ï¼ˆLaunchï¼‰ã€‚
- Grid ä¸­çš„ block ä¼šè¢«åˆ†é…åˆ°ä¸åŒçš„ SM æ‰§è¡Œã€‚
- GPU ä¸Šçš„ GPCï¼ˆGraphics Processing Clusterï¼‰ æˆ– TPCï¼ˆTexture Processing Clusterï¼‰ å±‚çº§ä¼šå‚ä¸å…¨å±€è°ƒåº¦ã€‚

### Kernel æ‰§è¡Œæµç¨‹

- å½“ä¸€ä¸ª Kernel è¢«å¯åŠ¨æ—¶ï¼Œç”±ç¡¬ä»¶è°ƒåº¦å™¨å°† Grid ä¸­çš„æ‰€æœ‰çº¿ç¨‹å—åˆ†é…åˆ° GPU ä¸Šå¯ç”¨çš„ SM ä¸­
- ä¸€ä¸ª Thread Block ä¼šè¢«å®Œæ•´åœ°åˆ†é…ç»™ä¸€ä¸ª SMï¼Œå¹¶åœ¨å…¶ä¸Šå®Œæ•´åœ°æ‰§è¡Œã€‚åœ¨å…¶ç”Ÿå‘½å‘¨æœŸå†…ï¼Œå®ƒä¸ä¼šè¢«è¿ç§»åˆ°å…¶ä»– SM
- ä¸€ä¸ª SM å¯ä»¥å¹¶å‘åœ°æ‰§è¡Œå¤šä¸ª Thread Blockï¼Œå‰ææ˜¯å®ƒæ‹¥æœ‰è¶³å¤Ÿçš„èµ„æºï¼ˆå¦‚å¯„å­˜å™¨ã€å…±äº«å†…å­˜ï¼‰æ¥å®¹çº³è¿™äº›çº¿ç¨‹å—
- åœ¨ SM å†…éƒ¨ï¼Œä¸€ä¸ª Thread Block çš„æ‰€æœ‰çº¿ç¨‹è¢«åˆ’åˆ†ä¸ºè‹¥å¹²ä¸ª Warpï¼ˆæ¯ç»„ 32 ä¸ªçº¿ç¨‹ï¼‰ã€‚è¿™äº› Warp æ‰æ˜¯è¢« SM çš„ Warp è°ƒåº¦å™¨å®é™…è°ƒåº¦æ‰§è¡Œçš„å•å…ƒ ã€‚

è¿™ä¸ªæ˜ å°„å…³ç³»æ˜¯å±‚çº§åŒ–çš„ï¼š`Grid -> GPU`ï¼Œ`Block -> SM`ï¼Œ`Thread -> Warp -> CUDAæ ¸å¿ƒ`

## Roofline æ¨¡å‹

Roofline æ¨¡å‹ï¼ˆå±‹é¡¶çº¿æ¨¡å‹ï¼‰æ˜¯ä¸€ç§ç”¨æ¥**åˆ†æç¨‹åºæ€§èƒ½ç“¶é¢ˆ**ï¼ˆè®¡ç®—å—é™è¿˜æ˜¯å¸¦å®½å—é™ï¼‰çš„æ–¹æ³•ã€‚  
å®ƒæŠŠ**è®¡ç®—æ€§èƒ½**ï¼ˆFLOPs/sï¼‰å’Œ**è®¿å­˜æ€§èƒ½**ï¼ˆBytes/sï¼‰è”ç³»åœ¨ä¸€èµ·ï¼Œã€‚ä»¥å¯è§†åŒ–çš„æ–¹å¼å±•ç¤ºæ€§èƒ½ä¸Šé™

$$
AchievableÂ \space FLOPs=min(AIÃ—Memory\spaceÂ BW,PeakÂ FLOPs)
$$

![](img/roofline.png)

## CUDA Kernel æ€§èƒ½è°ƒä¼˜


æ ¹æ® Roofline æ¨¡å‹ï¼Œç®—å­åˆ†ä¸º Compute-Bound å’Œ Memory-Bound å‹ï¼Œæ‰€ä»¥æˆ‘ä»¬éœ€è¦åˆ†å¼€è®¨è®ºï¼Œä½†å®é™…ä¸Šåœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œè¿™ä¸¤ç§ç“¶é¢ˆä¼šäº¤æ›¿å‡ºç°ğŸ˜­

### Memory-Bound

#### 1. æŒ‡ä»¤å±‚çº§ï¼šæŒ‡ä»¤å‘å°„æ•ˆç‡ (Instruction Issue Efficiency)

è¿™æ˜¯ `float4` ä¼˜åŒ–çš„ç›´æ¥ä½œç”¨åŸŸã€‚

- **LSU (Load Store Unit) å‹åŠ›ï¼š**
    
    - **Scalar (`float`):** æ¬è¿åŒæ ·å¤šçš„æ•°æ®ï¼Œéœ€è¦å‘å°„ **4 å€** çš„æŒ‡ä»¤æ•°ã€‚è¿™ä¼šå¤§é‡å ç”¨å‰ç«¯ï¼ˆFetch/Decodeï¼‰å¸¦å®½ï¼Œå¹¶å¢åŠ  LSU ç»´æŠ¤ In-flight çŠ¶æ€çš„å¼€é”€ã€‚
        
    - **Vectorized (`float4`):** **å•æŒ‡ä»¤é«˜åå**ã€‚ä¸€æ¡æŒ‡ä»¤å³å¯æ¬è¿ 128-bit æ•°æ®ã€‚LSU é˜Ÿåˆ—å ç”¨å°‘ï¼Œæ›´å®¹æ˜“ç»´æŒæµæ°´çº¿é¥±å’Œã€‚
        
    - _ä¿®æ­£ç‚¹ï¼š_ å³ä½¿æ˜¯æ ‡é‡è®¿é—®ï¼Œå¦‚æœæ˜¯ Coalesced çš„ï¼ŒWarp ä¹Ÿåªä¼šç”Ÿæˆ 1 ä¸ª Transactionï¼Œä½†éœ€è¦ **4 æ¡æŒ‡ä»¤** æ‰èƒ½å®Œæˆ 4 ä¸ªå…ƒç´ çš„åŠ è½½ã€‚
        
- **æŒ‡ä»¤å‘å°„å»¶è¿Ÿæ©ç›– (Issue Latency Hiding)ï¼š**
    
    - **æ°”æ³¡é—®é¢˜ï¼š** æŒ‡ä»¤å‘å°„æœ‰å›ºæœ‰å»¶è¿Ÿã€‚å¦‚æœæ¯æ¡æŒ‡ä»¤æ¬è¿çš„æ•°æ®é‡å¤ªå°ï¼ˆå¦‚ 4 Bytesï¼‰ï¼ŒæŒ‡ä»¤å‘å°„çš„é€Ÿåº¦å¯èƒ½è·Ÿä¸ä¸Šå†…å­˜æ€»çº¿çš„æ¶ˆè€—é€Ÿåº¦ï¼Œå¯¼è‡´æ€»çº¿å‡ºç°â€œç©ºé—²æ°”æ³¡â€ã€‚
        
    - **ä¼˜åŠ¿ï¼š** `float4` (16 Bytes/thread) è®©æ¯æ¬¡å‘å°„çš„â€œå«é‡‘é‡â€æ›´é«˜ï¼Œæ›´å®¹æ˜“å¡«æ»¡å†…å­˜ç®¡é“ã€‚
        

#### 2. æ•°æ®å±‚çº§ï¼šå†…å­˜çº§å¹¶è¡Œåº¦ (MLP, Memory Level Parallelism)

è¿™æ˜¯å†³å®šå¸¦å®½ä¸Šé™çš„å…³é”®è½¯ä»¶ç­–ç•¥ã€‚

- **åŸç†ï¼š** HBM å»¶è¿Ÿæé«˜ (~600 cycles)ã€‚ä¸ºäº†æ©ç›–å»¶è¿Ÿï¼Œå¿…é¡»è®©æ€»çº¿ä¸ŠåŒæ—¶é£ç€è¶³å¤Ÿå¤šçš„è¯·æ±‚ (In-flight Requests)ã€‚
    
- **ä¼˜åŒ–æ‰‹æ®µï¼š** **å¾ªç¯å±•å¼€ (Loop Unrolling)**ã€‚
    
    - _Bad:_ `Load -> Use -> Load -> Use` (ä¸²è¡Œä¾èµ–ï¼Œå»¶è¿Ÿæ— æ³•æ©ç›–)ã€‚
        
    - _Good:_ `Load1 -> Load2 -> Load3 -> Load4 ... -> Use1` (å¹¶è¡Œå‘å°„ï¼Œä¸€æ¬¡ç­‰å¾…ï¼Œå…¨éƒ¨è¿”å›)ã€‚
        

#### 3. ç¡¬ä»¶å±‚çº§ï¼šä¼ è¾“ç²’åº¦ä¸åˆ©ç”¨ç‡ (Transaction & Utilization)

å³ä½¿è½¯ä»¶å†™å¾—å¥½ï¼Œç¡¬ä»¶æœºåˆ¶ä¹Ÿå¯èƒ½å¯¼è‡´æµªè´¹ã€‚

- **æ‰‡åŒºåˆ©ç”¨ç‡ (Sector Utilization):**
    
    - **æœºåˆ¶ï¼š** DRAM åˆ° L2 çš„æœ€å°ä¼ è¾“ç²’åº¦æ˜¯ **32 Bytes (Sector)**ã€‚
        
    - **æµªè´¹ï¼š** å¦‚æœä½ åªè¯» 1 ä¸ª Byte (`char`) ä¸”æœªæ‰“åŒ…ï¼Œç¡¬ä»¶ä¹Ÿè¢«è¿«æ¬è¿ 32 Bytesã€‚**æœ‰æ•ˆå¸¦å®½ (Effective Bandwidth)** åªæœ‰ 1/32ã€‚
        
    - **å¯¹ç­–ï¼š** å¯¹äºå°æ•°æ®ç±»å‹ï¼ˆINT8/FP16ï¼‰ï¼Œå¿…é¡»ä½¿ç”¨æ‰“åŒ…å¯¹é½ï¼ˆPack Alignmentï¼‰è®¿é—®ã€‚
        
- **åœ°å€å¯¹é½ (Address Alignment):**
    
    - **æœºåˆ¶ï¼š** ç¡¬ä»¶è¦æ±‚è®¿é—®åœ°å€æŒ‰ 32B æˆ– 128B å¯¹é½ã€‚
        
    - **åæœï¼š** å¦‚æœæŒ‡é’ˆåœ°å€åç§»ï¼ˆMisalignedï¼‰ï¼Œä¸€æ¬¡ 128 Bytes çš„è¯»å–å¯èƒ½ä¼šè·¨è¶Šä¸¤ä¸ª 128B å—ï¼Œå¯¼è‡´ç¡¬ä»¶å¿…é¡»å‘èµ· **2 ä¸ª Transactions**ã€‚è¿™ä¼šç›´æ¥å¯¼è‡´å¸¦å®½æ€§èƒ½å‡åŠã€‚
        

#### 4. æ¶æ„å±‚çº§ï¼šç‰©ç†å†²çª (Physical Conflict)

é€šå¸¸ç”±ç¡¬ä»¶è§£å†³ï¼Œä½†åœ¨æç«¯ä¼˜åŒ–æ—¶éœ€æ³¨æ„ã€‚

- **åˆ†åŒºå†²çª (Partition Camping / Channel Conflict):**
    
    - **åŸç†ï¼š** æ˜¾å­˜è¢«åˆ’åˆ†ä¸ºå¤šä¸ªç‰©ç†åˆ†åŒºï¼ˆMemory Controllersï¼‰ã€‚
        
    - **ç°è±¡ï¼š** ç‰¹å®šçš„è®¿é—®æ­¥é•¿ï¼ˆStrideï¼Œé€šå¸¸æ˜¯ 2 çš„å¹‚æ¬¡ï¼‰å¯èƒ½å¯¼è‡´æ‰€æœ‰è¯·æ±‚é›†ä¸­æ‰“å‘åŒä¸€ä¸ª Controllerï¼Œé€ æˆå±€éƒ¨æ‹¥å µï¼ˆSerializationï¼‰ï¼Œè€Œå…¶ä»– Controller ç©ºé—²ã€‚
        
    - **ç°çŠ¶ï¼š** ç°ä»£ GPU (Pascal+) å·²é€šè¿‡ç‰©ç†åœ°å€å“ˆå¸Œï¼ˆAddress Swizzlingï¼‰æå¤§ç¼“è§£äº†æ­¤é—®é¢˜ï¼Œä½†åœ¨å†™æé™ Kernel æ—¶ä»éœ€é¿å…å®Œç¾çš„ 2 çš„å¹‚æ¬¡è·¨åº¦ã€‚
### Compute-Bound

#### 1. ä½¿ç”¨ Tensor Cores 
è¿™æ˜¯ç°ä»£ LLM æ¨ç†åŠ é€Ÿçš„**ç»å¯¹æ ¸å¿ƒ**ã€‚
- **åŸç†ï¼š**
    - **CUDA Core (FP32):** æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸæ‰§è¡Œ 1 æ¬¡ `FMA (a*b+c)`ã€‚
    - **Tensor Core (FP16):** æ¯ä¸ªæ—¶é’Ÿå‘¨æœŸæ‰§è¡Œ 1 æ¬¡å®Œæ•´çš„ `4x4` æˆ– `16x8` çŸ©é˜µä¹˜åŠ ã€‚
    - **å·®è·ï¼š** A100 ä¸Šï¼ŒTensor Core çš„ååé‡æ˜¯ CUDA Core çš„ **16 å€**ä»¥ä¸Šã€‚
- **ä¼˜åŒ–æ‰‹æ®µï¼š**
    - æŠ›å¼ƒ `c = a * b + c` çš„æ ‡é‡å†™æ³•ã€‚
    - ä½¿ç”¨ `nvcuda::wmma` (Warp-level Matrix Multiply Accumulate) æˆ– PTX `mma` æŒ‡ä»¤ã€‚
    - **æ•°æ®å¸ƒå±€ (Layout):** Tensor Core è¦æ±‚æ•°æ®åœ¨ Shared Memory æˆ–å¯„å­˜å™¨ä¸­æ»¡è¶³ç‰¹å®šçš„å¸ƒå±€ï¼ˆå¦‚ fragmentï¼‰ï¼Œå¦åˆ™ä¸ºäº†å¯¹é½æ•°æ®èŠ±è´¹çš„æŒ‡ä»¤ä¼šæŠµæ¶ˆåŠ é€Ÿæ•ˆæœã€‚
#### 2. é™ä½ç²¾åº¦ (Precision Reduction)
è®¡ç®—ååé‡ä¸æ•°æ®ç±»å‹çš„ä½å®½æˆåæ¯”
- **åŸç†ï¼š** ç¡¬ä»¶å¤„ç†ä½æ•°è¶Šå°‘ï¼Œå¹¶è¡Œåº¦è¶Šé«˜ã€‚
- **å±‚çº§ï¼š**
    - **FP32:** åŸºå‡† (1x)
    - **TF32 (Ampere+):** Tensor Core ä¸“ç”¨ï¼Œ19 bitsï¼Œååé‡é€šå¸¸æ˜¯ FP32 çš„ high-speed æ¨¡å¼ã€‚
    - **FP16 / BF16:** ååé‡æ˜¯ FP32 çš„ **2x** (CUDA Core) æˆ–æ›´é«˜ (Tensor Core)ã€‚
    - **INT8:** ååé‡æ˜¯ FP16 çš„ **2x**ã€‚
- **ä¼˜åŒ–æ‰‹æ®µï¼š** åœ¨ LLM æ¨ç†ä¸­ï¼Œæƒé‡ï¼ˆWeightï¼‰å’Œæ¿€æ´»ï¼ˆActivationï¼‰é€šå¸¸é‡åŒ–ä¸º **FP16** æˆ– **INT8/W8A16**ã€‚
#### 3. æŒ‡ä»¤çº§å¹¶è¡Œ (ILP, Instruction Level Parallelism)

å³ä½¿æ²¡æœ‰ Tensor Coreï¼ŒSM å†…éƒ¨ä¹Ÿæœ‰æµæ°´çº¿ä¼˜åŒ–ç©ºé—´ã€‚
- **åŸç†ï¼š** ç®—æœ¯æŒ‡ä»¤ï¼ˆå¦‚ FMAï¼‰æœ‰å»¶è¿Ÿï¼ˆLatencyï¼Œæ¯”å¦‚ 4-6 ä¸ªå‘¨æœŸï¼‰
- **ä¼˜åŒ–æ‰‹æ®µï¼š** **å¾ªç¯å±•å¼€ (Loop Unrolling)**ã€‚è¿™èƒ½è®©æ¯ä¸ªçº¿ç¨‹åŒæ—¶ç»´æŒå¤šä¸ªâ€œIn-flightâ€çš„è®¡ç®—æŒ‡ä»¤
    ```cpp
    // å±•å¼€åï¼Œç¼–è¯‘å™¨ä¼šäº¤é”™å‘å°„æŒ‡ä»¤
    a1 = b1 * c1; // å‘å°„
    a2 = b2 * c2; // å‘å°„ï¼Œä¸éœ€è¦ç­‰ a1
    a3 = b3 * c3; // å‘å°„
    // ... æ­¤æ—¶ ALU æµæ°´çº¿è¢«å¡«æ»¡
    ```
    
#### 4. åŒå‘å°„ (Dual Issue) / å¹¶å‘æ‰§è¡Œ


- **åŸç†ï¼š** ç°ä»£ SMï¼ˆAmpere/Ada/Hopperï¼‰é€šå¸¸åŒ…å«ï¼š
    - FP32 è®¡ç®—å•å…ƒ
    - INT32 è®¡ç®—å•å…ƒ
    - è¿™ä¸¤è€…åœ¨æŸäº›æ¶æ„ä¸Šæ˜¯**ç‹¬ç«‹**çš„ï¼Œå¯ä»¥**åŒæ—¶æ‰§è¡Œ**ã€‚
- **åœºæ™¯ï¼š**
    - `val = val * x + y` (FP32 è®¡ç®—)
    - `ptr += stride` (INT32 æŒ‡é’ˆè®¡ç®—/å¾ªç¯è®¡æ•°)
        
- **ä¼˜åŒ–æ‰‹æ®µï¼š** ç¼–å†™ Kernel æ—¶ï¼Œå¦‚æœèƒ½è®©æµ®ç‚¹è®¡ç®—ï¼ˆä¸šåŠ¡é€»è¾‘ï¼‰å’Œæ•´æ•°è®¡ç®—ï¼ˆåœ°å€ç´¢å¼•ï¼‰äº¤ç»‡åœ¨ä¸€èµ·ï¼ŒSM å¯ä»¥**åœ¨ä¸€ä¸ªå‘¨æœŸå†…åŒæ—¶å‘å°„è¿™ä¸¤æ¡æŒ‡ä»¤**ï¼Œä»è€Œæ©ç›–æ‰ç´¢å¼•è®¡ç®—çš„å¼€é”€ã€‚
#### 5. ä½¿ç”¨ FMA (Fused Multiply-Add)

- **åŸç†ï¼š** `a * b + c`ã€‚
    - å¦‚æœä¸ä¼˜åŒ–ï¼š`MUL` (ä¹˜æ³•) + `ADD` (åŠ æ³•) = 2 æ¡æŒ‡ä»¤ã€‚
    - ä¼˜åŒ–åï¼š`FFMA` (Fused FMA) = 1 æ¡æŒ‡ä»¤ã€‚
- **ä¼˜åŒ–æ‰‹æ®µï¼š** ç¼–è¯‘å™¨é€šå¸¸ä¼šè‡ªåŠ¨ä¼˜åŒ–ã€‚ä½†åœ¨æ‰‹å†™ intrinsic æ—¶ï¼Œç¡®ä¿è°ƒç”¨ `__fmaf_rn(a, b, c)` è€Œä¸æ˜¯åˆ†å¼€å†™ã€‚

### Examples

- **Memory-Bound Vector Add**ï¼š[ä¸€æ­¥æ­¥å®ç° CUDA Vector Add ä¼˜åŒ–](https://tom-jerr.github.io/blogs/posts/Vector%20Add%20Optimization%20Example/)

## CUDA å¸¸ç”¨ä¼˜åŒ–æŠ€å·§

### Maximize Compiler Computation[^gpu-miarch]

- Unroll Loops
  > å±•å¼€å¾ªç¯ï¼ˆloop unrollingï¼‰ï¼Œè®©å¾ªç¯ä½“é‡å¤å±•å¼€å¤šæ¬¡ï¼Œ**å‡å°‘å¾ªç¯æ§åˆ¶å¼€é”€**ï¼ˆæ¯”å¦‚ i++ã€i<N çš„åˆ¤æ–­ï¼‰ï¼Œæé«˜ GPU çš„ååé‡ã€‚
- Write code using compile-time constants (not same as constant registers)
  > åœ¨ä»£ç é‡Œç”¨ ç¼–è¯‘æœŸå·²çŸ¥çš„å¸¸é‡æ¥åšç´¢å¼•ã€å¾ªç¯æ¬¡æ•°ã€æ•°ç»„å¤§å°ç­‰ï¼Œè€Œä¸æ˜¯ä¾èµ– GPU çš„å¸¸é‡å¯„å­˜å™¨

### Coalescing Memory Access

- å½“ä¸€ä¸ª Warp ä¸­çš„**æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹è®¿é—®å…¨å±€å†…å­˜ä¸­çš„è¿ç»­ä½ç½®æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å°†è¿™ 32 ä¸ªå°çš„è¯·æ±‚â€œåˆå¹¶â€æˆä¸€ä¸ªå•ä¸€ã€å¤§å‹ã€é«˜æ•ˆçš„å†…å­˜äº‹åŠ¡**
- Memory Access Patterns:

  - åˆå¹¶è®¿é—®ï¼ˆç†æƒ³ï¼‰ï¼šWarp ä¸­çš„çº¿ç¨‹ i è®¿é—®å†…å­˜åœ°å€ base + iã€‚è¿™åœ¨å¤„ç†æŒ‰è¡Œä¸»åºå­˜å‚¨çš„çŸ©é˜µçš„è¡Œæ—¶éå¸¸å¸¸è§

    ![](img/coalesce-smem.png)

  - è·¨æ­¥è®¿é—®ï¼ˆé—®é¢˜ï¼‰ï¼šçº¿ç¨‹ i è®¿é—® base + i \* strideã€‚å¦‚æœæ­¥é•¿ï¼ˆstrideï¼‰å¾ˆå¤§ï¼Œè¿™å°†å¯¼è‡´è®¸å¤šç‹¬ç«‹çš„ã€ä½æ•ˆçš„å†…å­˜äº‹åŠ¡ã€‚è¿™åœ¨è®¿é—®æŒ‰è¡Œä¸»åºå­˜å‚¨çš„çŸ©é˜µçš„åˆ—æ—¶å¾ˆå¸¸è§

    ![](img/uncoalesce-smem.png)

  - éå¯¹é½è®¿é—®ï¼šWarp è®¿é—®çš„èµ·å§‹åœ°å€æœªä¸å†…å­˜äº‹åŠ¡çš„å¤§å°å¯¹é½

### Avoid Bank Conflicts in Shared Memory

:warning: Shared memory is organized into **32 banks**. Each bank is a slice of SRAM that can load or store **4 bytes of data every cycle**.

![](img/smem.jpg)

- å½“ä¸€ä¸ª Warp ä¸­çš„æ‰€æœ‰ 32 ä¸ªçº¿ç¨‹è®¿é—®å…¨å±€å†…å­˜ä¸­çš„è¿ç»­ä½ç½®æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å°†è¿™ 32 ä¸ªå°çš„è¯·æ±‚ **â€œåˆå¹¶â€æˆä¸€ä¸ªå•ä¸€ã€å¤§å‹ã€é«˜æ•ˆçš„å†…å­˜äº‹åŠ¡**

  ![](img/conflict-free.png)

- å½“åŒä¸€ä¸ª Warp ä¸­çš„ä¸¤ä¸ªæˆ–æ›´å¤šçº¿ç¨‹è¯•å›¾è®¿é—®ä½äºåŒä¸€ä¸ªå†…å­˜é“¶è¡Œä¸­çš„ä¸åŒåœ°å€æ—¶ï¼Œå°±ä¼šå‘ç”Ÿé“¶è¡Œå†²çª ã€‚æ­¤æ—¶ï¼Œè¿™äº›è®¿é—®ä¼šè¢«ä¸²è¡ŒåŒ–å¤„ç†ï¼Œä»è€Œé™ä½äº†å…±äº«å†…å­˜çš„æœ‰æ•ˆå¸¦å®½

  ![](img/bank-conflict.png)

#### Solutions[^bankconflict]

- Padding: åœ¨æ•°æ®ç»“æ„ä¸­æ’å…¥å¡«å……å…ƒç´ ï¼Œä»¥æ”¹å˜æ•°æ®åœ¨å†…å­˜ä¸­çš„å¸ƒå±€ï¼Œé¿å…å¤šä¸ªçº¿ç¨‹è®¿é—®åŒä¸€é“¶è¡Œ
  ![](img/padding.jpg)
  - å¯èƒ½é™ä½ SM çš„ occupancy
  - **å¯èƒ½åœ°å€è®¿é—®ä¸å¯¹é½ï¼Œæ— æ³•ä½¿ç”¨å‘é‡åŒ–è®¿é—®**
- **swizzling:** é‡æ–°ç»„ç»‡æ•°æ®çš„å­˜å‚¨æ–¹å¼ï¼Œä½¿å¾—å¹¶è¡Œè®¿é—®æ—¶æ›´å°‘å†²çª(æ›´å¸¸ç”¨):rocket:

  - æŸäº› swizzling æ–¹æ³•åœ¨ä» shared memory è¯»æ•°æ®åˆ° register æ—¶ä¸èƒ½è¿›è¡Œ float4 çš„åˆå¹¶è¯»å–

    ![](img/swizzling.jpg)

  - é€»è¾‘ä½ç½®è¡¨ç¤ºå…ƒç´ åœ¨çŸ©é˜µä¸­çš„é€»è¾‘åæ ‡ã€‚
  - ç‰©ç†ä½ç½®è¡¨ç¤ºå…¶å¯¹åº”å…ƒç´ åœ¨å®é™…å­˜å‚¨æ•°æ®çš„ shared memory ä¸­çš„ä½ç½®åæ ‡ã€‚

    > å½“æˆ‘ä»¬è¯´è¯»å–çŸ©é˜µçš„ç¬¬ 2 è¡Œç¬¬ 3 åˆ—çš„å…ƒç´ ï¼Œ(2,3)å°±è¡¨ç¤ºé€»è¾‘ä½ç½®ï¼Œè€ŒçœŸæ­£è¯»å–æ•°æ®çš„æ—¶å€™ï¼Œæˆ‘ä»¬éœ€è¦ä»å®é™…å­˜å‚¨æ•°(2,1)çš„ shared memory ä¸­å¯¹åº”çš„ä½ç½®

    ![](img/smem2.jpg)

:warning: å¹¿æ’­ (Broadcast): å¦‚æœä¸€ä¸ª Warp ä¸­çš„æ‰€æœ‰çº¿ç¨‹è®¿é—®åŒä¸€ä¸ª bank ä¸­çš„å®Œå…¨ç›¸åŒçš„åœ°å€ï¼Œè¿™æ˜¯ä¸€ç§å¹¿æ’­æ“ä½œï¼Œä¸ä¼šäº§ç”Ÿå†²çª

#### Matmul Bank Conflict Avoidance Example[^bankconflict2]

```cpp
const int warp_id = tid / 32;
const int lane_id = tid % 32;
const int a_tile_index =  warp_id / 2 * 16 + lane_id / 8 * 4
const int b_tile_index =  warp_id % 2 * 32 + lane_id % 8 * 4;
```

ä½¿ç”¨çš„æ˜¯ 4Ã—2 çš„ warp å¸ƒå±€, warp ä¸­çš„æ¯ä¸ªçº¿ç¨‹æŒ‰ç…§ 4Ã—8 è¿›è¡Œæ’å¸ƒ, æ¯ä¸ª warp å¯¹åº” 16Ã—32 çš„æ•°æ®

- æ¯ä¸ª wrap 32 ä¸ªçº¿ç¨‹åªè·å– As çš„ 4\*4=16 ä¸ªæ•°æ®
- æ¯ä¸ª wrap 32 ä¸ªçº¿ç¨‹åªè·å– Bs çš„ 8\*4=32 ä¸ªæ•°æ®
- **shared memory ä¸­æœ‰ 32 ä¸ª bankï¼Œæ‰€ä»¥ä¸ä¼šäº§ç”Ÿ bank conflict**
  ![](img/avoid-bank-conflict.png)

### Double Buffering[^bankconflict2]

åœ¨å…±äº«å†…å­˜ä¸­åˆ†é…ä¸¤ä¸ªç¼“å†²åŒº: å½“ SM æ­£åœ¨å¯¹ç¼“å†²åŒº 1 ä¸­çš„æ•°æ®è¿›è¡Œè®¡ç®—æ—¶ï¼Œç¡¬ä»¶å¯ä»¥å¼‚æ­¥åœ°å°†ä¸‹ä¸€å—æ•°æ®ä»å…¨å±€å†…å­˜é¢„å–åˆ°ç¼“å†²åŒº 2 ä¸­ã€‚ä¸€æ—¦è®¡ç®—å®Œæˆï¼Œä¸¤ä¸ªç¼“å†²åŒºçš„è§’è‰²äº’æ¢ã€‚è¿™ç§æ–¹å¼æœ‰æ•ˆåœ°å°†å…¨å±€å†…å­˜çš„è®¿é—®å»¶è¿Ÿéšè—åœ¨äº†è®¡ç®—çš„èƒŒå
![](img/double_buffer.png)

```cpp
if (doubleBufferIdx == 0) {
  // load first (B0)
  db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
      N, K, A, B, As, Bs, innerRowA, innerColA, innerRowB, innerColB);
}
__syncthreads();

// outer-most loop over block tiles
for (uint bkIdx = 0; bkIdx < K; bkIdx += 2 * BK) {
  if (doubleBufferIdx == 0) {
    // process current (B0)
    db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN, TM,
                        TN>(regM, regN, threadResults, As, Bs, warpRow,
                            warpCol, threadRowInWarp, threadColInWarp);
    __syncthreads();

    // process current+1 (B1)
    if (bkIdx + BK < K) {
      db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN,
                          TM, TN>(regM, regN, threadResults, As + (BM * BK),
                                  Bs + (BK * BN), warpRow, warpCol,
                                  threadRowInWarp, threadColInWarp);
    }
    __syncthreads();

    // load current + 2 (B0)
    if (bkIdx + 2 * BK < K) {
      db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
          N, K, A + 2 * BK, B + 2 * BK * N, As, Bs, innerRowA, innerColA,
          innerRowB, innerColB);
    }
  } else {
    // load current + 1 (B1)
    if (bkIdx + BK < K) {
      db::loadFromGmem<BM, BN, BK, rowStrideA, rowStrideB>(
          N, K, A + BK, B + BK * N, As + (BM * BK), Bs + (BK * BN), innerRowA,
          innerColA, innerRowB, innerColB);
    }
    __syncthreads();

    // process current (B0)
    db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN, TM,
                        TN>(regM, regN, threadResults, As, Bs, warpRow,
                            warpCol, threadRowInWarp, threadColInWarp);
    __syncthreads();

    // process current+1 (B1)
    if (bkIdx + BK < K) {
      db::processFromSmem<BM, BN, BK, WM, WN, WMITER, WNITER, WSUBM, WSUBN,
                          TM, TN>(regM, regN, threadResults, As + (BM * BK),
                                  Bs + (BK * BN), warpRow, warpCol,
                                  threadRowInWarp, threadColInWarp);
    }
  }

  A += 2 * BK;     // move BK columns to right
  B += 2 * BK * N; // move BK rows down
  __syncthreads();
}
```

### Tile[^matmul]

- å°†åŸæœ¬ä¸€è¡Œ Ã— ä¸€åˆ—çš„è®¡ç®—è¿›è¡Œåˆ†å—ï¼Œæ¯æ¬¡åªè®¡ç®—ä¸€å—
- ä¸€æ¬¡æ€§ä»å…¨å±€å†…å­˜ä¸­åŠ è½½ä¸€å°å— A (BM x BK) å’Œä¸€å°å— B (BK x BN) åˆ°å…±äº«å†…å­˜ä¸­
- ä¸€ä¸ªçº¿ç¨‹å—å†…çš„æ‰€æœ‰çº¿ç¨‹å°±å¯ä»¥åœ¨å…±äº«å†…å­˜ä¸Šå¿«é€Ÿåœ°è¿›è¡Œå¤§é‡çš„è®¡ç®—ï¼Œä»¥å®Œæˆå¯¹åº”çš„ä¸€å°å— C (BM x BN) çš„è®¡ç®—
- æ¯ä¸ªçº¿ç¨‹ä¸å†æ˜¯åªè®¡ç®— C å—ä¸­çš„ä¸€ä¸ªå…ƒç´ ï¼Œè€Œæ˜¯è´Ÿ**è´£è®¡ç®—ä¸€ä¸ªæ›´å°çš„ç»“æœç½‘æ ¼**ï¼ˆå›¾ä¸­æ˜¯ 2x2ï¼‰ã€‚è¿™æ ·åšå¯ä»¥è¿›ä¸€æ­¥æå‡æ•°æ®å¤ç”¨ç‡å’Œè®¡ç®—æ•ˆç‡

  ![](img/tile2.png)

### Wrap Tile[^matmul]

- **çº¿ç¨‹å—åˆ†ç‰‡(blocktiling)**: ä¸åŒçš„çº¿ç¨‹å—å¯ä»¥åœ¨ä¸åŒçš„ SM ä¸Šå¹¶è¡Œæ‰§è¡Œ.
- **warp åˆ†ç‰‡(warptiling)**: ä¸åŒçš„ warp å¯ä»¥åœ¨ä¸åŒçš„ warp è°ƒåº¦å™¨ä¸Šå¹¶è¡Œæ‰§è¡Œ, ä¹Ÿå¯ä»¥åœ¨åŒä¸€ä¸ª warp è°ƒåº¦å™¨ä¸Šå¹¶å‘æ‰§è¡Œ.
- **çº¿ç¨‹åˆ†ç‰‡(threadtiling)**: (æ•°é‡éå¸¸æœ‰é™çš„)æŒ‡ä»¤å¯ä»¥åœ¨ç›¸åŒçš„ CUDA å†…æ ¸ä¸Šå¹¶è¡Œæ‰§è¡Œ(=æŒ‡ä»¤çº§å¹¶è¡Œ, instruction-level parallelism, å³ ILP).

![](img/warp_tile.png)

```cpp
for (uint bkIdx = 0; bkIdx < K; bkIdx += BK) {
  // load from gmem
  for (uint offset = 0; offset + rowStrideA <= BM; offset += rowStrideA) {
    const float4 tmp = reinterpret_cast<const float4 *>(
        &A[(innerRowA + offset) * K + innerColA * 4])[0];
    // float4 tmp;
    // asm("ld.global.nc.v4.f32 {%0, %1, %2, %3}, [%4];"
    //     : "=f"(tmp.x), "=f"(tmp.y), "=f"(tmp.z), "=f"(tmp.w)
    //     : "l"(&A[(innerRowA + offset) * K + innerColA * 4]));
    As[(innerColA * 4 + 0) * BM + innerRowA + offset] = tmp.x;
    As[(innerColA * 4 + 1) * BM + innerRowA + offset] = tmp.y;
    As[(innerColA * 4 + 2) * BM + innerRowA + offset] = tmp.z;
    As[(innerColA * 4 + 3) * BM + innerRowA + offset] = tmp.w;
  }

  for (uint offset = 0; offset + rowStrideB <= BK; offset += rowStrideB) {
    reinterpret_cast<float4 *>(
        &Bs[(innerRowB + offset) * BN + innerColB * 4])[0] =
        reinterpret_cast<const float4 *>(
            &B[(innerRowB + offset) * N + innerColB * 4])[0];
    // asm("ld.global.v4.f32 {%0, %1, %2, %3}, [%4];"
    //     : "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 0]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 1]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 2]),
    //       "=f"(Bs[(innerRowB + offset) * BN + innerColB * 4 + 3])
    //     : "l"(&B[(innerRowB + offset) * N + innerColB * 4]));
  }
  __syncthreads();
  // dotIdx loops over contents of SMEM
  for (uint dotIdx = 0; dotIdx < BK; ++dotIdx) {
    // å®é™…ä¸Šæ˜¯ä¿®æ”¹ warp çš„å¸ƒå±€ï¼Œå˜æˆ 2x2 çš„å¸ƒå±€
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
      for (uint i = 0; i < TM; ++i) {
        regM[wSubRowIdx * TM + i] =
            As[(dotIdx * BM) + warpRow * WM + wSubRowIdx * WSUBM +
              threadRowInWarp * TM + i];
      }
    }
    for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
      for (uint i = 0; i < TN; ++i) {
        regN[wSubColIdx * TN + i] =
            Bs[(dotIdx * BN) + warpCol * WN + wSubColIdx * WSUBN +
              threadColInWarp * TN + i];
      }
    }

    // æ¯æ¬¡æ¯ä¸ª warp åŠ è½½ 32 * 16 ä¸ªæ•°æ®ï¼Œä¸ä¼šäº§ç”Ÿ bank conflictï¼Œéœ€è¦è¿­ä»£ 4 æ¬¡
    // æ¯ä¸ª warp å¤šæ¬¡è¿­ä»£çš„æ•°æ®æ˜¯è¿ç»­çš„ä¸€å—, ä¸åŒ warp åŒä¸€æ¬¡è¿­ä»£çš„æ•°æ®åˆ™æ˜¯åˆ†æ•£çš„å¤šå—
    for (uint wSubRowIdx = 0; wSubRowIdx < WMITER; ++wSubRowIdx) {
      for (uint wSubColIdx = 0; wSubColIdx < WNITER; ++wSubColIdx) {
        // calculate per-thread results with register-cache locality
        for (uint resIdxM = 0; resIdxM < TM; ++resIdxM) {
          for (uint resIdxN = 0; resIdxN < TN; ++resIdxN) {
            threadResults[(wSubRowIdx * TM + resIdxM) * (WNITER * TN) +
                          (wSubColIdx * TN) + resIdxN] +=
                regM[wSubRowIdx * TM + resIdxM] *
                regN[wSubColIdx * TN + resIdxN];
          }
        }
      }
    }
  }
  A += BK;     // move BK columns to right
  B += BK * N; // move BK rows down
  __syncthreads();
}
```

## å‚è€ƒèµ„æ–™

[^cuda1]: [CUDAï¼ˆä¸€ï¼‰ï¼šCUDA ç¼–ç¨‹åŸºç¡€](https://zhuanlan.zhihu.com/p/645330027)
[^cuda2]: [CUDAï¼ˆäºŒï¼‰ï¼šGPU çš„å†…å­˜ä½“ç³»åŠå…¶ä¼˜åŒ–æŒ‡å—](https://zhuanlan.zhihu.com/p/654027980)
[^matmul]: [[CUDA å­¦ä¹ ç¬”è®°] å¦‚ä½•ä¼˜åŒ– CUDA çŸ©é˜µä¹˜å†…æ ¸ä»¥è·å¾—ç±»ä¼¼ cuBLAS çš„æ€§èƒ½: å·¥ä½œæ—¥å¿—](https://blog.csdn.net/LostUnravel/article/details/138034380#t12)
[^gpu-miarch]: [EE 7722, Lecture Slides: NVIDIA GPU Microarchitecture](https://www.ece.lsu.edu/koppel/gp/notes/set-nv-org.pdf)
[^bankconflict]: [CUDA shared memory é¿å… bank conflict çš„ swizzling æœºåˆ¶è§£æ](https://zhuanlan.zhihu.com/p/4746910252)
[^bankconflict2]: [GEMM ä¼˜åŒ–: åŒç¼“å†² (Prefetch) å’Œ Bank Conflict è§£å†³](https://zhuanlan.zhihu.com/p/696844342)
