---

title: Speculative Decoding in SGLang(EAGLE2)
created: 2026-01-12
tags:
- LLMInference
description: æœ¬æ–‡å°†ä»ä¸ºä»€ä¹ˆéœ€è¦ speculative decoding å¼€å§‹è®²èµ·ï¼Œé€šè¿‡å‡ ç¯‡è®ºæ–‡æ¥è®²è¿°ç°åœ¨æŠ•æœºé‡‡æ ·æ¼”è¿›çš„è·¯çº¿ï¼Œä»¥ SGLang ä¸­ Eagle2 çš„å®ç°ä½œä¸º example è¿›è¡Œè§£æï¼Œå¹¶ç»“åˆçº¦æŸè§£ç è¿›è¡Œå¼‚æ­¥ä¼˜åŒ–è¿›è¡Œéƒ¨åˆ†ç»“æœå¯¹æ¯”ã€‚
cover: /img/eagle2.png
---
# Speculative Decoding in SGLang(EAGLE2)

åœ¨æ­£å¼è¿›å…¥ SGLang æºç å‰ï¼Œæˆ‘ä»¬éœ€è¦çŸ¥é“ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ speculative decoding ä»¥åŠä»€ä¹ˆæ˜¯ speculative decodingã€‚æœ‰äº†è¿™äº›è®¤è¯†åï¼Œæˆ‘ä»¬å¯ä»¥æ·±å…¥æºç æ¥äº†è§£ SGLang æ˜¯å¦‚ä½•å°† speculative decoding ä¸ scheduler é›†æˆèµ·æ¥çš„

è¿™é‡Œå…ˆå¤§æ¦‚ç»™ä¸€ä¸‹ speculative decoding çš„ç¤ºæ„å›¾

![](static/XtNobrb2EoAGDMxMx1LcRq5VnfT.png)

## **Speculative Decoding Motivation**

åŸºäºä»¥ä¸‹è§‚å¯Ÿï¼Œæå‡ºäº† speculative decodingï¼Œåˆ©ç”¨ç©ºé—²è®¡ç®—èµ„æºå¢åŠ å¹¶è¡Œæ€§

- Many easy tokens can be predicted with less computational overhead(using a smaller model)
- LLM æ¨ç†æ˜¯é«˜åº¦ memory-bound çš„ï¼Œå»¶è¿Ÿä¸»è¦åœ¨è¯»å–/å†™å…¥æ¨¡å‹å‚æ•°è€Œä¸æ˜¯è®¡ç®—

> æ¯æ¬¡ç”Ÿæˆä¸€ä¸ª tokenï¼Œéƒ½éœ€è¦æ¬è¿æ‰€æœ‰æ¨¡å‹æƒé‡ä» HBM åˆ° cache ä¸­

æˆ‘ä»¬éœ€è¦ä¿è¯ speculative decoding ç”Ÿæˆå¤šä¸ª token çš„æˆæœ¬è¦æ¯” auto-regressive ç”Ÿæˆ 1 ä¸ª token æˆæœ¬å·®ä¸å¤šï¼Œæ‰€ä»¥æŠ•æœºé‡‡æ ·è¦æƒ³è·å¾—æ€§èƒ½æ”¶ç›Šï¼Œæ ¸å¿ƒè¦è§£å†³ä»¥ä¸‹ä¸¤ä¸ªé—®é¢˜ï¼š

> åœ¨æ•´ä¸ªæŠ•æœºé‡‡æ ·çš„æµç¨‹ä¸­ï¼Œå‡è®¾è½»é‡ LLM ç”Ÿæˆ Draft Tokens çš„å¼€é”€ä¸º  $p$ ï¼ŒåŸå§‹ LLM éªŒè¯ & Next Token ç”Ÿæˆçš„å¼€é”€è¿‘ä¼¼ä¸º 1 ï¼Œé‚£ä¹ˆ**æŠ•æœºé‡‡æ ·åœ¨æ¥å— Tokens æ•°å¤§äº  **$1 + p$**  çš„æƒ…å†µä¸‹æ‰æœ‰æ€§èƒ½æ”¶ç›Š**ï¼Œå¹¶ä¸”éšç€æ¥å—çš„ Tokens æ•°å¢åŠ è€Œæ€§èƒ½æ”¶ç›Šè¶Šå¤§ã€‚

1. **å¦‚ä½•é™ä½æŠ•æœºé‡‡æ ·çš„ overheadï¼Ÿ**
2. **å¦‚ä½•æå‡ Verify é˜¶æ®µçš„æ¥å—ç‡ï¼Ÿ**

## **What is Speculative Decoding(Original)**

ä¸€èˆ¬éµå¾ªä»¥ä¸‹æµç¨‹ï¼š

- Draft: ç”Ÿæˆ k ä¸ª candidate tokens
- Verify:
  - ç”¨ target model å¹¶è¡Œ verify all candidate tokens
  - å¯¹æ¯ä¸ªä½ç½®çš„åˆ†å¸ƒè®¡ç®—æ¦‚ç‡
- Accept / Reject:
  - æ¥å—æ‰€æœ‰ correct token
  - åœ¨ç¬¬ä¸€ä¸ªè¢«æ‹’ç»é‡‡æ ·çš„ä½ç½®ï¼Œåœ¨**è°ƒæ•´åçš„åˆ†å¸ƒ**ä¸­é‡æ–°é‡‡æ ·

> æ³¨æ„ï¼šæˆ‘ä»¬å¯¹ draft åˆ†å¸ƒéœ€è¦ä¸ target çš„åˆ†å¸ƒç›¸åŒ

![](static/JeLcbcEP5oxItmx1rKacnhGGnne.png)

---

## Some Limitations

æˆ‘ä»¬éœ€è¦è®© Draft Model çš„ç”Ÿæˆçš„ç›®æ ‡åˆ†å¸ƒä¸ Target Model ç›¸åŒï¼Œè¿™é‡Œæœ‰å¤šç§é€‰æ‹©æ–¹å¼

- é€‰å–ä¸ target model åŒä¸€ family çš„è¾ƒå°çš„æ¨¡å‹
- Distill ä¸€ä¸ªè½»é‡æ¨¡å‹ï¼Œteacher é€‰å– target modelï¼Œè¿™ä¼šå¼•å…¥ training cost
- é€‰å– ngram æ¨¡å‹ï¼Œä½†æ˜¯è¿™ä¸ªçš„æ¥å—ç‡å¾ˆä½

Draft model å’Œ target model éœ€è¦å…±äº« vocabulary æˆ–è€…ä½¿ç”¨ç›¸åŒçš„ tokenizer

- $p(x)$ å’Œ $q(x)$ å¿…é¡»å®šä¹‰åœ¨åŒä¸€ä¸ªæ ·æœ¬ç©ºé—´ä¸Š

---

## **å‘å±•å†ç¨‹**

### **Speculative Decoding (Leviathan et al., 2023, Google)**

ğŸ“„ _Fast Inference from Transformers via Speculative Decoding_

- é¦–æ¬¡æå‡ºè¯¥æ–¹æ³•ï¼Œæ€è·¯ï¼š

  - ç”¨ä¸€ä¸ªå°æ¨¡å‹ï¼ˆdraft modelï¼‰ç”Ÿæˆå¤šä¸ªå€™é€‰ tokenï¼›
  - ç”¨å¤§æ¨¡å‹ï¼ˆtarget modelï¼‰éªŒè¯å…¶ä¸­çš„ä¸€éƒ¨åˆ†ï¼›
  - è‹¥éªŒè¯é€šè¿‡ï¼Œåˆ™ä¸€æ¬¡æäº¤å¤šä¸ª tokenï¼Œå‡å°‘å¤§æ¨¡å‹è°ƒç”¨æ¬¡æ•°ã€‚
- ç”¨æ›´é«˜æ•ˆçš„æ¨¡å‹ $M_q$ ç”Ÿæˆ **Î³ **ä¸ªå€™é€‰ tokenï¼›
- ç„¶åä½¿ç”¨ç›®æ ‡æ¨¡å‹ $M_p$ **å¹¶è¡Œ Verify** è¿™äº›å€™é€‰æ ·æœ¬åŠå…¶åœ¨ $M_q$ ä¸­çš„æ¦‚ç‡ï¼Œ å¹¶**æ¥å—**æ‰€æœ‰èƒ½ä½¿åˆ†å¸ƒä¸ $M_p$ ä¸€è‡´çš„å€™é€‰ï¼›

> _å†³å®šè¢«æ¥å—çš„æ•°é‡ nï¼ˆç”¨éšæœºæ•°è¿›è¡Œæ‹’ç»é‡‡æ ·ï¼‰_

- æ¥å—æ‰€æœ‰æ­£ç¡®çš„é‡‡æ ·
  - å¯¹ç¬¬ä¸€ä¸ªè¢«æ‹’ç»çš„å€™é€‰ï¼Œä»ä¸€ä¸ª**è°ƒæ•´åçš„åˆ†å¸ƒ**ä¸­é‡æ–°é‡‡æ ·

  > å¦‚æœè‰ç¨¿æ¨¡å‹ $M_q$ åœ¨ç¬¬ n+1 ä¸ª token è¢«æ‹’ç»ï¼Œé‚£ä¹ˆç›®æ ‡æ¨¡å‹ $M_p$ ä¸èƒ½ç›´æ¥ç”¨è‡ªå·±çš„åˆ†å¸ƒé‡‡æ ·ï¼Œè€Œå¿…é¡»æŠŠå·²ç»è¢« $M_q$ å°è¯•ä½†æ‹’ç»æ‰çš„æ¦‚ç‡è´¨é‡ $M_p$ ä¸­å‡æ‰ï¼Œå†é‡æ–°å½’ä¸€åŒ–
  >

  - å¦‚æœæ‰€æœ‰å€™é€‰éƒ½è¢«æ¥å—ï¼Œåªæ¥å— **Î³-1** ä¸ªï¼Œç„¶åå†é‡‡æ ·ä¸€ä¸ªé¢å¤–çš„ token

> ä¿®æ­£çš„æ‹’ç»é‡‡æ ·æ–¹æ¡ˆå¯ä»¥ä¿æŒä¸ target é‡‡æ ·ç›¸åŒçš„ç›®æ ‡åˆ†å¸ƒ
> ![](static/TtMJbQcBQoZLxAxbcoQcdKJUnBe.png)

- Limitations
  - ä½¿ç”¨ separate çš„ draft model
    - Draft model è®­ç»ƒåæ•ˆæœæ¯”è¾ƒå¥½ï¼Œå¼•å…¥äº†è®­ç»ƒæˆæœ¬
    - Draft model å’Œ target model ä¹‹é—´çš„ distribution shift
  - Memory overhead of second model
  - æ¯æ¬¡è¿­ä»£åªæœ‰ä¸€ä¸ª draft sequence
  - $\gamma$æ˜¯å›ºå®šçš„ï¼Œç®€å•çš„ token ä¼šæµªè´¹èµ„æºï¼Œå¤æ‚çš„ token éœ€è¦æ›´å¤šçš„æ¨æµ‹

---

### **Medusa(No need for a separate draft model)**

ğŸ“„ MEDUSA: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads

- ä¸æ˜¯å•ç‹¬çš„å°æ¨¡å‹ï¼Œè€Œæ˜¯ **åœ¨å¤§æ¨¡å‹ decoder çš„ last hidden states ä¸Šç›´æ¥åŠ å¤šä¸ªé¢„æµ‹å¤´(1-2 layer mlp)**ï¼›
- æ¯ä¸ª head ä¼šä¸ºå®ƒè´Ÿè´£çš„ä½ç½®ç”Ÿæˆå¤šä¸ªæœ€å¯èƒ½çš„é¢„æµ‹

  - ç¬¬ t ä¸ª position ä½ç½®å¤„ï¼Œç¬¬ k ä¸ª head é¢„æµ‹ t+k+1 ä¸ª token
  - è¿™ä¼šå¢åŠ ä¸€æ¬¡ decoding step çš„æ¥å—é•¿åº¦
  - Verification can be computationally intensive for a large number of completions

  > We need trade-off
  >
- ä¸ºäº†**åŠ é€Ÿ**ä¸**è®¡ç®—ä»£ä»·**ä¹‹é—´å–å¾—å¹³è¡¡ï¼Œå¼•å…¥äº†ä¸€ç§ **æ ‘çŠ¶ç»“æ„çš„æ³¨æ„åŠ›æœºåˆ¶ï¼ˆtree-structured attentionï¼‰**ï¼Œå¯ä»¥**å¹¶è¡Œå¤„ç†å¤šä¸ªå€™é€‰åºåˆ—ï¼Œä¸€æ¬¡éªŒè¯ä¸€æ•´æ£µæ ‘**

  - ä¾æ®é¢„æµ‹çš„ probs æ„é€ äº†ä¸€ä¸ªç¨€ç–çš„ candidate tokens æ ‘
  - **Tree Mask**ï¼š**æ¯ä¸ª token åªèƒ½çœ‹åˆ°æ¥è‡ªåŒä¸€æ¡å€™é€‰åºåˆ—ï¼ˆcontinuationï¼‰çš„å†å² token**ï¼Œä¸èƒ½è®¿é—®å…¶ä»–å€™é€‰çš„ tokenã€‚
  - æŒ‰ç…§æ ‘ç»“æ„ï¼Œæ­£ç¡®åˆ†é… positional encoding

  > åŒä¸€æ·±åº¦çš„ token ç”¨åŒä¸€ä¸ª position id
  >
- **ä¼šé€‰æ‹©é•¿åº¦æœ€é•¿çš„å‰ç¼€ candidate tokens**

![](static/SBtkb0kqConi4NxoTkHcNGs3nbb.png)
![](static/IacXbP7UpoGGfoxTT0Mcw1qBnyh.png)

- Limitations:
  - Position-Independent Prediction / Limited Information
    - æ¯ä¸ª Medusa head é¢„æµ‹ä½ç½® i+kï¼Œä½†å®ƒä¸çŸ¥é“ä½ç½®$i+1\dots i+k-1$ å®é™…é¢„æµ‹äº†ä»€ä¹ˆ

    > ä¸çœŸå®çš„è‡ªå›å½’ä¸åŒ
    >

    - Medusa head åªèƒ½çœ‹åˆ°æœ€åä¸€å±‚çš„è¡¨ç¤º
    - æœ€åä¸€å±‚æ˜¯ä¸ºé¢„æµ‹ä¸‹ä¸€ä¸ª token è®­ç»ƒçš„ï¼Œè€Œä¸æ˜¯ç¬¬ 2ã€3 ä¸ªä¹‹åçš„ token
    - æ— æ³•åˆ©ç”¨ä¸­é—´å±‚æ›´ä¸°å¯Œçš„è¡¨ç¤º
  - Lower acceptance rates for later positions in the draft
  - Speedup plateaus at ~3Ã— even with more heads

---

### **Lookahead Decoding**

ğŸ“„ Lookahead: An Inference Acceleration Framework for Large Language Model with Lossles

![](static/UF1Bb6GNQoa8VsxFiKucHlNan4e.png)

- å±‚æ¬¡åŒ–å¤šåˆ†æ”¯è‰ç¨¿ç­–ç•¥ (Hierarchical Multi-Branch Draft Strategy)ï¼š_åˆ©ç”¨å…±åŒçš„å‰ç¼€æ ‡è®°å°†å¤šä¸ªé¢„æµ‹çš„è‰ç¨¿åºåˆ—ï¼ˆåˆ†æ”¯ï¼‰è¿›è¡Œåˆå¹¶å’Œå‹ç¼©_
- åŸºäº Trie æ ‘çš„è‰ç¨¿æ£€ç´¢å’Œç®¡ç† (Trie-tree-based Retrieval and Management)

  - Trie æ ‘å­˜å‚¨äº†**è¾“å…¥æç¤º (Prompt)** å’Œ**å·²ç”Ÿæˆå“åº” (Generated response)** ä¸­å‡ºç°çš„ n-gram æ ‡è®°åºåˆ—ï¼ˆå³åˆ†æ”¯ï¼‰ã€‚
  - å¼•å…¥äº† **ç”Ÿæˆåˆ†æ”¯æ’å…¥ (Generated branch Inserting)** æœºåˆ¶ï¼Œèƒ½å¤ŸåŠ¨æ€åœ° (on-the-fly) å°†ç”Ÿæˆçš„é‡å¤åºåˆ—æ”¾å…¥ Trie æ ‘ä¸­ï¼Œä»è€Œåˆ©ç”¨è¾“å‡ºä¸­çš„é‡å¤æ¨¡å¼è¿›è¡ŒåŠ é€Ÿã€‚
  - é€šè¿‡**åˆ†æ”¯æ¶ˆé™¤**å’Œ**èŠ‚ç‚¹ä¿®å‰ª**ç­–ç•¥æ¥ä¿æŒ Trie æ ‘çš„é«˜æ•ˆæ€§ï¼Œæ§åˆ¶å†…å­˜æ¶ˆè€—ã€‚

---

### **EAGLE-1ï¼š Extrapolation Algorithm for Greater Language-model Efficiency**

ğŸ“„ EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty

- Train a lightweight plugin, called AutoRegression Head, in conjunction with the Original LLM's frozen embedding layer, to predict the next feature based on the current feature sequence form the second-top-layer of the Original model
- Decode using the frozen classification head of the Original LLM, which maps features to tokens
- Feed in tokens from one time step ahead
  ![](static/SGvZbgg5Ho8of8xOc3zcUEFlnOe.png)

#### EAGLE-1 Drafting

EAGLE-1 è§£å†³äº† Medusa çš„ä¸ç¡®å®šæ€§ï¼Œä½¿ç”¨äº† feature-level è¿›è¡Œé¢„æµ‹

- Feature prediction with known context is much more accurate
- EAGLE integrates embeddings and generates the next feature
- When predicting $f_j$, we already know $f_{j-1}$

$$
\begin{aligned}
(f_n, t_n)&\rightarrow f_{n+1} \\
f_{n+1} &\rightarrow LM\_Head \rightarrow t_{n+1}\\
(f_{n+1}, t_{n+1}) &\rightarrow f_{n+2}
\end{aligned}
$$

#### EAGLE-1 Verification

- å¯¹æ¯ä¸ª draft token position ç”Ÿæˆ target probs p
- **Tree Attention**ï¼šç›´æ¥ verify æ•´ä¸ª tree

  - EAGLE ç”Ÿæˆçš„æ ‘æ›´ç¨€ç–ã€ä¸Šä¸‹æ–‡æ›´å……åˆ†

  > æ¯ä¸€æ­¥é¢„æµ‹éƒ½çŸ¥é“å‰é¢ token æ˜¯ä»€ä¹ˆï¼Œé€šè¿‡ feature å¯ä»¥å¾—åˆ°æ›´å¤šçš„ä¿¡æ¯
  >

  - Medusa Head 2 predicting t+2 doesnâ€™t know what t+1 will be

![](static/ESsIb5cwLo30xcx1fQScTQsknLd.png)
![](static/SxXrbrkuuoaXzjxMMvzcEB9Vnxf.png)

#### Multi-round speculative sampling

æŠŠåŒä¸€ä½ç½®çš„å¤šä¸ªå€™é€‰ token æŒ‰é¡ºåºåšå¤šè½® accept / rejectï¼›åªæœ‰å½“æ‰€æœ‰å€™é€‰éƒ½æ‹’ç»äº†ï¼Œæ‰ä»è°ƒæ•´åçš„åˆ†å¸ƒé‡ŒçœŸæ­£é‡‡æ ·ä¸€ä¸ª tokenã€‚è¿™æ ·èƒ½ æå‡æ¥å—ç‡ / å‡å°‘ fallbackï¼ŒåŒæ—¶ä»ç„¶ä¿æŒä¸¥æ ¼æ— åï¼ˆæœ€ç»ˆ token ä»æœä»ç›®æ ‡åˆ†å¸ƒ pï¼‰

- **ä¾æ¬¡å°è¯•** $t_1,t_2,t_3\dots ,t_k$
- æ¯æ¬¡å°è¯•å¤±è´¥ï¼Œä¸æ˜¯ç«‹åˆ»ä» adjusted distribution é‡‡æ ·ï¼Œè€Œæ˜¯ï¼š

  - **æ›´æ–°ç›®æ ‡åˆ†å¸ƒ p**ï¼ˆæ‰£æ‰æœ¬è½® proposal é‚£éƒ¨åˆ†æ¦‚ç‡é‡æ–°è¿›è¡Œåˆ†å¸ƒï¼‰
  - ç„¶åç»§ç»­å°è¯•ä¸‹ä¸€ä¸ªå€™é€‰
- å¦‚æœ k ä¸ªéƒ½å¤±è´¥äº†ï¼Œæ‰ä»æœ€ç»ˆå‰©ä¸‹çš„ adjusted åˆ†å¸ƒé‡‡æ ·ä¸€æ¬¡

![](static/CPIYbPOrEofK1RxCLjAc1vRCnTd.png)

---

### **EAGLE-2: Context-Aware speculation**

ğŸ“„ EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees

- High confidence â†’ high acceptance probability â†’ expand the tree
- Low confidence â†’ low acceptance probability â†’ prune the tree

Context-aware speculation:

- Easy context (code boilerplate): Deeper trees
- Hard context (creative writing): Shallower, wider trees

#### Motivation

- æ¥å—ç‡é™¤äº†ä¸ **Token æ‰€åœ¨ä½ç½®ç›¸å…³ä»¥å¤–ï¼ˆåœ¨æ ‘ä¸­æ‰€å¤„çš„ä½ç½®ï¼‰ï¼Œè¿˜å’Œä¸Šæ–‡ç›¸å…³ï¼ˆæ ‘ä¸­çš„ç¥–å®—èŠ‚ç‚¹ï¼‰**ã€‚

> ![](static/XjY6bHv81omhS1xPgw3cFAxznGc.png)
> _P3ã€P4 å’Œ P5ã€P6 è™½ç„¶éƒ½æ˜¯åŒä¸€å±‚çš„èŠ‚ç‚¹ï¼ˆå³åŒä¸€ä¸ª Step çš„ Draft Tokensï¼‰ï¼Œä½†æ¥æ”¶ç‡ä¸Š P3ã€P4 æ™®éé«˜äº P5ã€P6 èŠ‚ç‚¹ï¼Œä¸€ä¸ªé‡è¦çš„åŸå› æ˜¯ P3ã€P4 çš„çˆ¶èŠ‚ç‚¹ä¸º P1ï¼Œå…¶æ¦‚ç‡é«˜äº P5ã€P6 èŠ‚ç‚¹çš„çˆ¶èŠ‚ç‚¹ P2ã€‚P3ã€P4 çš„æ¦‚ç‡ç”šè‡³æ™®éé«˜äº P2 è¿™æ›´åŠ è¯´æ˜åœ¨ç”Ÿæˆ Draft æ ‘çš„æ—¶å€™ï¼Œé‡‡ç”¨é™æ€ Draft æ ‘å¹¶ä¸æ˜¯ä¸€ä¸ªæœ€ä¼˜é€‰æ‹©ï¼Œæ›´åº”è¯¥é€‰æ‹©åŠ¨æ€ Draft æ ‘ã€‚_

- åŸå§‹ LLM è‡ªå›å½’ç”Ÿæˆçš„ Token æ¦‚ç‡åˆ†å¸ƒè¡¨ç¤º Token æ¥æ”¶æ¦‚ç‡ã€‚Eagle çš„ Draft æ¨¡å‹ç”Ÿæˆçš„ Draft Tokens æ¦‚ç‡åˆ†å¸ƒä¸ Token æ¥æ”¶ç‡åˆ†å¸ƒæ¥è¿‘ã€‚ä¸‹å›¾å±•ç¤ºäº† Draft Tokens ç”Ÿæˆæ¦‚ç‡å’Œ Token æ¥æ”¶ç‡çš„åˆ†å¸ƒå›¾ï¼Œå¯ä»¥çœ‹å‡ºåˆ†å¸ƒå¾ˆæ¥è¿‘ï¼Œ**å¯ä»¥é€šè¿‡ Draft Tokens ç”Ÿæˆæ¦‚ç‡é¢„ä¼° Token çš„æ¥æ”¶ç‡ã€‚**
  ![](static/Q5SYb75o0oQ01pxLzJ3ck8Zbnjd.png)

#### EAGLE-2 Dynamic Tree Construction

EAGLE-2 ä¸ä¿®æ”¹ draft æ¨¡å‹çš„è®­ç»ƒä¸æ¨ç†æ–¹å¼ï¼Œä¹Ÿä¸æ”¹å˜éªŒè¯é˜¶æ®µã€‚å®ƒçš„æ”¹è¿›é›†ä¸­åœ¨ä¸¤ä¸ªæ–¹é¢ï¼š

1. å¦‚ä½•æ‰©å±•è‰ç¨¿æ ‘ï¼›
2. å¦‚ä½•å¯¹è‰ç¨¿ token è¿›è¡Œé‡æ–°æ’åº

- **Expand é˜¶æ®µ**ï¼šä»å½“å‰å±‚é€‰æ‹© **å…¨å±€æ¥å—æ¦‚ç‡ï¼ˆglobal acceptance probabilityï¼‰æœ€é«˜çš„å‰ k ä¸ª token** è¿›è¡Œæ‰©å±•ã€‚
  - ä¸€ä¸ª token çš„**å…¨å±€æ¥å—æ¦‚ç‡**æ˜¯å®ƒä»æ ¹èŠ‚ç‚¹åˆ°è¯¥èŠ‚ç‚¹è·¯å¾„ä¸Šæ‰€æœ‰ token çš„æ¥å—æ¦‚ç‡çš„ä¹˜ç§¯ï¼š
    $V_i = \prod_{t_j \in \text{Path(root, }t_i\text{)}} p_j \approx \prod c_jï¼Œ$ å…¶ä¸­$c_j$æ˜¯ draft æ¨¡å‹çš„ç½®ä¿¡åº¦
- **ReRank é˜¶æ®µ**ï¼š**ä¸ä¼šç›´æ¥ä½¿ç”¨æ‰©å±•é˜¶æ®µçš„ç»“æœï¼Œè€Œæ˜¯å¯¹æ‰€æœ‰è‰ç¨¿ token é‡æ–°æ’åº**ï¼Œé€‰å‡ºå…¨å±€ top-m çš„èŠ‚ç‚¹ã€‚

> _Expand çš„ç›®æ ‡æ˜¯åŠ æ·±è‰ç¨¿æ ‘ã€‚ç„¶è€Œï¼Œç”±äºæ¥å—ç‡åœ¨ 0â€“1 ä¹‹é—´ï¼Œè¶Šæ·±çš„ token å…¶å€¼è¶Šå°ã€‚_**ä¸€äº›æµ…å±‚æœªæ‰©å±•èŠ‚ç‚¹å¯èƒ½æ¯”æ·±å±‚èŠ‚ç‚¹æ›´æœ‰ä»·å€¼**_ã€‚_
> **å¯¹äºå€¼ç›¸åŒçš„èŠ‚ç‚¹ï¼Œä¼˜å…ˆé€‰æ‹©æµ…å±‚èŠ‚ç‚¹ï¼Œå§‹ç»ˆä¿æŒçˆ¶èŠ‚ç‚¹åœ¨å­èŠ‚ç‚¹ä¹‹å‰è¢«é€‰ä¸­ã€‚** è¿™æ ·å¯ä»¥ä¿è¯ top-m èŠ‚ç‚¹ä»æ„æˆä¸€æ£µè¿é€šæ ‘ã€‚
> ![](static/MY8GbmDMloCvYHxBR8ackqBEnMg.png)

- **Tree Mask**ï¼šæŠŠè¿™äº›é€‰ä¸­çš„ token **æ‹‰å¹³æˆä¸€ç»´åºåˆ—**ï¼Œé€å…¥ä¸»æ¨¡å‹è¿›è¡ŒéªŒè¯é˜¶æ®µã€‚ä¸ºäº†ä¸æ ‡å‡†è‡ªå›å½’è§£ç ä¿æŒä¸€è‡´ï¼Œæˆ‘ä»¬éœ€è¦è°ƒæ•´æ³¨æ„åŠ›æ©ç ã€‚ä¸åŒåˆ†æ”¯çš„ token ä¸åº”äº’ç›¸å¯è§ï¼Œå› æ­¤æ³¨æ„åŠ›æ©ç éœ€æ ¹æ®æ ‘ç»“æ„ä¿®æ”¹ï¼Œä½¿æ¯ä¸ª token ä»…èƒ½çœ‹åˆ°å®ƒçš„ç¥–å…ˆèŠ‚ç‚¹ã€‚

![](static/YvJ0bRiPko1c35xB773cDJEjnuc.png)

---

### EAGLE-3: Training-time test + Direct Token Prediction + Multi-level fusion

#### Motivation

- Increasing training data for EAGLE-1/2 provides **limited** improvement
- **EAGLE Training: Loss = L_fea + L_token**

  - Feature prediction loss constrains the model
  - Model must output something close to target features
  - This limits expressiveness!
- **The Feature Prediction Constraint:**

  - Forces draft model output to approximate target features
  - Limits the model's representational capacity
  - Token prediction is the goal; feature prediction is an auxiliary objective
  - If those outputs arenâ€™t â€œfeature-like,â€ the next step becomes out-of-distribution

#### EAGLE-3: Training-time test

**With L_fea (EAGLE-1/2)**

- Training step 2 still uses **ground-truth features** as context
- But at test time, step 2 uses **predicted** features
- L_fea partially masks this because predicted â‰ˆ ground-truth
  **Without L_fea (failure mode)**
- Training never sees the modelâ€™s unconstrained predictions as inputs
- Test time does â†’ out-of-distribution â†’ compounding error
  Core rule: **if the model will consume its own predictions at test time, it should practice that during training**
- Train by simulating multiple draft steps **using predicted states**, not ground truth
- Important implementation detail: **stop-gradient** through the simulated rollout so training remains stable

![](static/CHSkbIARBo6KKRxvu2Kcf36jn0g.png)

- Removes the need for a strict â€œfeature-likenessâ€ constraint
- Model learns to be robust to its own imperfect intermediate states
- Capacity is spent on â€œwhat helps token accuracy under rollout,â€ not â€œmatch a specific feature targetâ€

#### EAGLE-3: Tree Attention Mask during training of the draft model

- è™½ç„¶è‡ªå›å½’æ¨ç†åœ¨æ—¶é—´ä¸Šæ˜¯é¡ºåºçš„ï¼Œä½†åœ¨è®­ç»ƒæ—¶å¯ä»¥æŠŠå¤šæ­¥ rollout é‡æ„æˆä¸€æ£µæ ‘ï¼Œå¹¶ç”¨æ ‘çŠ¶ attention mask ä¸€æ¬¡æ€§å¹¶è¡Œè®¡ç®—ï¼Œä»è€Œè®©æ¨¡å‹åœ¨è®­ç»ƒé˜¶æ®µå°±ç»å†â€œä½¿ç”¨è‡ªèº«é¢„æµ‹ä½œä¸ºä¸Šä¸‹æ–‡â€çš„çœŸå®æµ‹è¯•æ¡ä»¶ã€‚
  ![](static/PHTwbskerob0S2xP7SFcSZDHngb.png)

#### EAGLE-3: Multi-layer feature fusion

ä¸€æ—¦ä¸å†å¼ºè¿«é¢„æµ‹ feature å»åƒæŸä¸€å±‚çš„ ground-truth featureâ€ï¼ˆç§»é™¤ $L_{\text{fea}}$ï¼‰ï¼Œæ¨¡å‹å°±å¯ä»¥è‡ªç”±åœ°åˆ©ç”¨ä¸åŒå±‚çš„è¡¨ç¤ºï¼š

- Top-layer features are "committed" to the immediate next token
- Lower layers contain richer, more general semantic information
- Predicting t+2 benefits from information not yet "collapsed" into next-token prediction
- Fusion captures multiple abstraction levels

![](static/SstFbSngvoTJwqxqwBOcGDp6n3b.png)

---

## **Eagle2 in SGLang**

### Overview

- Draft çš„çŠ¶æ€é€šè¿‡ batch.spec_info æ¥è¿›è¡Œç®¡ç†ï¼›ä¸åœåœ°åœ¨ Scheduler å’Œ EAGLEWorkerV2 ä¸­æµè½¬
- å°† draft å’Œ verify æ•°æ®å‡†å¤‡å·¥ä½œå•ç‹¬å¼€è¾Ÿä¸€ä¸ª GPU plan stream è¿›è¡Œï¼Œä¸ GPU compute stream å¹¶è¡Œ
- å› ä¸ºæ•°æ®å‡†å¤‡å·¥ä½œ(**KV Cache ç­‰**)ä¼šå½±å“åˆ°åç»­è¿™äº›è¿‡ç¨‹è®¡ç®—çš„æ­£ç¡®æ€§ï¼Œæ‰€ä»¥åœ¨å¯åŠ¨ GPU compute kernel å‰éœ€è¦ç­‰å¾… plan kernel æ‰§è¡Œå®Œæˆ

**åŒæ­¥ç‚¹**[è§ä¸‹æ–‡](#åŒæ­¥ç‚¹)**ï¼š**

1. Verify ä¸ Draft Extend ä¹‹é—´çš„åŒæ­¥
2. Draft ä¸ Verify ä¹‹é—´çš„åŒæ­¥
3. Verify ä¸ Sampling ä¹‹é—´çš„åŒæ­¥

![](static/WiM4bkw4Ko06PLxMGJgcyYAhnKb.png)

### æ•´ä½“æµç¨‹

Prefill é˜¶æ®µç”Ÿæˆ batch ä¸ä¸å¼€å¯ spec å¹¶æ— ä¸åŒï¼›é€šè¿‡ self.future_map.resolve_future() å’Œ self.future_map.store_to_map() å®ç°å¼‚æ­¥çš„ç»“æœå­˜å–

`run_batch()` ä¼šå…ˆè°ƒç”¨ EAGLEWorkerV2::forward_batch_generation()

- target_worker(TpWorker) æ‰§è¡Œ `forward_batch_generation`ï¼Œé‡‡æ ·å‡ºç¬¬ä¸€ä¸ª token

> Eagle éœ€è¦è¿™ä¸ª token å¯¹åº”çš„ hidden states è¿›è¡Œ draft çš„æ¨ç†

- draft_worker(EAGLEWorkerV2) æ‰§è¡Œ `_draft_extend_for_prefill`
  - å°† target model ç”Ÿæˆçš„ token ä¹ŸåŠ å…¥åˆ° batch.input_ids é‡Œé¢
  - æ„é€  EagleDraftInputï¼ŒEAGLE v2 çš„ decode é˜¶æ®µä¼šåŸºäº `verified_id` ä½œä¸ºæ ‘çš„æ ¹èŠ‚ç‚¹ï¼Œå†ç”Ÿæˆ topk åˆ†æ”¯å¹¶æ„å»º tree maskã€positions ç­‰
    ```shell
      EagleDraftInput(            _  _
      _hidden_states_  =_  target_hidden_states_  ,
      _  verified_id_  =_  next_token_ids_  ,
      _  new_seq_lens_  =_  batch_  .seq_lens,
      # draft mode is same with decode mode, only 1 num token per batch
      _  num_tokens_per_batch_  =1,
      _  num_tokens_for_logprob_per_batch_  =1,
      )
    ```
	- è°ƒç”¨ draft model çš„ forward_extend() å¹¶è¿›è¡Œé‡‡æ ·ï¼Œå¾—åˆ° topk çš„ token index & prob ä»¥åŠ hidden states

- åœ¨ decode é˜¶æ®µï¼Œdraft worker å…ˆè°ƒç”¨ draft()ï¼Œå†è°ƒç”¨ verify()ï¼Œæœ€å _draft_extend_for_decode()
	- draft(): æŒ‰ `speculative_num_steps` å¤šæ­¥æ‰©å±•ä¸€æ£µ top-k æ ‘ï¼Œè®°å½•æ¯ä¸ªèŠ‚ç‚¹ token/score/parentï¼Œæœ€åä»æ•´æ£µæ ‘é‡ŒæŒ‘ä¸€æ‰¹æœ€é«˜åˆ†èŠ‚ç‚¹ä½œä¸º `draft_tokens`ï¼Œå¹¶è¿”å›æ ‘ç»“æ„ä¿¡æ¯ç»™ `build_tree_kernel_efficient`ï¼Œæ„é€ å‡º EageleVerifyInput
      ```python
      EagleVerifyInput(
          _draft_token_=draft_tokens, # draft é˜¶æ®µæŒ‘é€‰å‡ºæ¥ã€å‡†å¤‡è®© target ä¸€æ¬¡éªŒè¯çš„ **å€™é€‰ token é›†åˆ**
          _custom_mask_=tree_mask, # æ¯ä¸ªå€™é€‰èŠ‚ç‚¹èƒ½çœ‹åˆ°å“ªäº› token
          _positions_=position, # æ¯ä¸ªå€™é€‰èŠ‚ç‚¹å¯¹åº”çš„ **position id**
          _retrive_index_=retrive_index, # verify batch ä¸­ç¬¬ i è¡Œï¼Œå¯¹åº” draft æ ‘é‡Œçš„å“ªä¸ªèŠ‚ç‚¹
          _retrive_next_token_=retrive_next_token, # å¦‚æœæŸä¸ªèŠ‚ç‚¹è¢«æ¥å—ï¼Œä¸‹ä¸€ä¸ªåº”è¯¥è·³åˆ°å“ªä¸ªèŠ‚ç‚¹
          _retrive_next_sibling_=retrive_next_sibling, # å¦‚æœå½“å‰èŠ‚ç‚¹æ²¡è¢«æ¥å—ï¼Œä¸‹ä¸€ä¸ªå¤‡é€‰å…„å¼ŸèŠ‚ç‚¹æ˜¯è°
          _retrive_cum_len_=None,
          _spec_steps_=_self_.speculative_num_steps, # draft æ‰©å±•çš„æœ€å¤§æ·±åº¦ï¼ˆæ ‘çš„é«˜åº¦ï¼‰
          _topk_=_self_.topk, # draft é˜¶æ®µæ¯ä¸ªèŠ‚ç‚¹çš„åˆ†æ”¯æ•°
          _draft_token_num_=_self_.speculative_num_draft_tokens,
          _capture_hidden_mode_=None,
          _seq_lens_sum_=None,
          _seq_lens_cpu_=None,
      )
      ```
- verify(): verify ä¸€æ¬¡ forwardï¼Œ**æœ€å¤šéªŒè¯ spec_steps ä¸ªæœªæ¥ token + å½“å‰ token**
	- æŠŠ verify æ‰€éœ€çš„ **tree ç´¢å¼•å’Œ token(verify_input)** ä» GPU æ‹·åˆ° CPUï¼Œå‡†å¤‡æ›´æ–° vocab mask
	- Target model æ‰§è¡Œ `forward_batch_generation()`ï¼Œå®é™…ä¸Šç›´æ¥è·³è¿‡äº† sample
	- å¯¹ draft æ ‘åš DFS æ„é€  vocab maskï¼Œå¹¶æŠŠ batch.sampling_info.vocab_mask ç½®ç©ºï¼ˆ`None`ï¼‰
	- å¯¹ draft input åš sampleï¼ŒåŒæ—¶è€ƒè™‘ target çš„æ¦‚ç‡åˆ†å¸ƒå’Œ draft çš„ token
		- å¦‚æœ draft token è¢«æ‹’ç»ï¼Œä¼šæ ¹æ® target çš„æ¦‚ç‡åˆ†å¸ƒé‡æ–°é‡‡æ ·ä¸€ä¸ªæ–°çš„ token ä½œä¸ºä¿®æ­£
- _draft_extend_for_decode():
	- è®¡ç®—çš„ `select_index`ï¼Œåªä¿ç•™æœ€åä¸€ä¸ª Token çš„é¢„æµ‹ç»“æœã€‚
	- è®¡ç®—å‡ºçš„ `topk_p`ï¼ˆæ¦‚ç‡ï¼‰ã€`topk_index`ï¼ˆToken IDï¼‰å’Œ hidden_states è¢«å¡«å…¥`next_draft_input`ã€‚è¿™ä¸ªå¯¹è±¡ä¼šè¢«ä¼ é€’ç»™ä¸‹ä¸€ä¸ªå¾ªç¯çš„ `draft()` å‡½æ•°ï¼Œä½œä¸ºç”Ÿæˆæ–°çš„ Token æ ‘çš„**root**


### KV Cache ç®¡ç†

Draft Model å’Œ Target Model éƒ½æœ‰è‡ªå·±ç‹¬ç«‹çš„ KV Cache

- **Draft Cache**: åœ¨ prepare_for_v2_draft ä¸­è®¡ç®—å†™å…¥çš„ KV Cache ä½ç½®ã€‚Draft æ¨¡å‹ç”¨å®ƒæ¥ç”Ÿæˆæ ‘
- **Target Cache**: åœ¨ prepare_for_v2_verify ä¸­è®¡ç®—å†™å…¥çš„ KV Cache ä½ç½®ã€‚Target æ¨¡å‹ç”¨å®ƒæ¥éªŒè¯æ ‘

#### åˆ†é…

åœ¨ prepare_for_decode() å¤„è¿›è¡Œ

- Target é€šè¿‡ alloc_for_decode() åˆ†é…ç‰©ç† KV Cache
- Draft ç­‰å¾…ä¸Šä¸€è½® sample ç»“æŸåï¼ŒåŒæ­¥å½“å‰ batch çš„ seq_lenï¼Œåˆ©ç”¨è¿™ä¸ª seq_len åˆ†é…ä¸‹ä¸€è½®éœ€è¦çš„ç‰©ç† KV Cache

> åˆ†é…äº† 2 * ALLOC_LENï¼Œç¡®ä¿ current step å’Œ next step æ‰€æœ‰çš„æ•°æ®éƒ½å¯ä»¥è£…ä¸‹

#### é‡Šæ”¾

> commited_len æ˜¯å®é™… KV Cache ä½¿ç”¨çš„é•¿åº¦
> allocated_len æ˜¯åˆ†é…çš„ KV Cache çš„é•¿åº¦

åœ¨ scheduler è°ƒåº¦ä¸­ï¼Œè°ƒç”¨ process_batch_result_decode è¿›è¡Œå¤„ç†ï¼Œreq ç»“æŸå

- å¤„ç†å·²æäº¤çš„ tokenï¼Œå°†"æœ‰ç”¨"çš„ token æ”¾å…¥ radix cacheï¼›å…¶ä»–ç›´æ¥é‡Šæ”¾
- Eagle v2 éœ€è¦é‡Šæ”¾è¿‡åº¦åˆ†é…çš„ kv cache

  > åœ¨ EAGLE v2 ä¸­ï¼Œ`allocated_len` é€šå¸¸æ¯” `committed_len` å¤§å¾ˆå¤šï¼ˆåŒ…å« Draft é¢„æµ‹çš„ä½†æœªè¢«æ¥å—çš„ Tokenï¼Œä»¥åŠä¸ºäº† Overlap é¢„åˆ†é…çš„ç©ºé—´ï¼‰
  >

## Rollback Lazy Processing

Rollbackï¼ˆå›é€€ï¼‰å¹¶ä¸æ˜¯çœŸçš„é‡Šæ”¾æ˜¾å­˜ï¼Œè€Œæ˜¯**ç§»åŠ¨æŒ‡é’ˆ**ã€‚

- **ç‰©ç†å±‚ï¼ˆä¸å¤„ç†ï¼‰ï¼š** å½“ Target Model éªŒè¯å‘ç° Draft é¢„æµ‹çš„ 5 ä¸ª Token åªå¯¹äº† 2 ä¸ªæ—¶ï¼Œå‰©ä¸‹çš„ 3 ä¸ªæ— æ•ˆ Token äº§ç”Ÿçš„ KV Cache ä¾ç„¶ç•™åœ¨æ˜¾å­˜çš„ç‰©ç† Block ä¸­ã€‚æˆ‘ä»¬ä¸ä¼šç«‹å³è§¦å‘ç±»ä¼¼ `free()` çš„æ“ä½œï¼Œå› ä¸ºè¿™å¤ªæ…¢äº†ã€‚
- **é€»è¾‘å±‚ï¼ˆå³æ—¶å›é€€ï¼‰ï¼š** `committed_len` ä¼šç«‹å³æ›´æ–°ä¸ºéªŒè¯é€šè¿‡çš„é•¿åº¦ã€‚ä¸‹ä¸€è½®æ¨ç†æ—¶ï¼Œæ–°çš„ KV Cache ä¼šç›´æ¥**è¦†ç›–ï¼ˆOverwriteï¼‰** æ‰é‚£äº›ä¹‹å‰é¢„æµ‹å¤±è´¥ç•™ä¸‹çš„è„æ•°æ®ã€‚
- **è¯·æ±‚ç»“æŸï¼š** åªæœ‰å½“æ•´ä¸ªè¯·æ±‚ç»“æŸï¼Œè¿›å…¥ `process_batch_result_decode` æ—¶ï¼ŒRadix Cache æ‰ä¼šæ ¹æ® `committed_len` å°†çœŸæ­£æœ‰æ•ˆçš„è·¯å¾„å­˜å…¥ç¼“å­˜æ± ï¼Œè€Œ `allocated_len` å¤šå ç”¨çš„é‚£éƒ¨åˆ†ç‰©ç† Block ä¼šè¢«ç»Ÿä¸€å›æ”¶ã€‚

### åŒæ­¥ç‚¹

äº‹å®ä¸Šé™¤äº†ä¸Šé¢æåˆ°çš„ CPU ä¸ GPU compute stream çš„åŒæ­¥ï¼Œè¿™é‡Œè¿˜æœ‰ä¸€ä¸ª GPU plan stream

#### **Plan stream å’Œ Compute stream ä¹‹é—´çš„åŒæ­¥**

- å› ä¸º plan stream ä¸­ä¸€èˆ¬éƒ½æ˜¯ä¸ºåç»­è®¡ç®—åšå‡†å¤‡çš„ï¼Œcompute stream éœ€è¦åŒæ­¥ç­‰å¾… plan stream æ“ä½œå®Œæˆ

#### CPU ä¸ Plan Stream çš„åŒæ­¥

- ç”± CPU ä¾§çš„è°ƒåº¦é€»è¾‘ä¿è¯ï¼Œè‡ªç„¶åŒæ­¥

#### CPU ä¸ Compute stream çš„åŒæ­¥

- Draft Extend N-1 ä¸ä¸‹ä¸€è½®è°ƒåº¦ç”± CPU è°ƒåº¦é€»è¾‘ä¿è¯åŒæ­¥
- Write future map å’Œ Read future map ç”± copy_to_cpu() ä¿è¯åŒæ­¥
- Verify ä¸ Draft Extend ä¹‹é—´çš„åŒæ­¥

  - Draft Extend å¡«å…… KV cache ä¾èµ–äº verify çš„ accept length å’Œ predict ç»“æœï¼Œæ‰€ä»¥éœ€è¦åŒæ­¥

  > **äº‹å®ä¸Šç”±äºæˆ‘ä»¬å‰é¢çš„ target verify ä¸ cpu çš„ launch target sampling æœ‰åŒæ­¥ç‚¹ï¼Œæ‰€ä»¥è¿™é‡Œè‡ªç„¶ä¿è¯åŒæ­¥**
  >
- Draft ä¸ Verify ä¹‹é—´çš„åŒæ­¥

  - CPU è¿›è¡Œ verify plan çš„æ—¶å€™ï¼ŒGPU çš„ draft å¯èƒ½è¿˜æ²¡è®¡ç®—å®Œæˆï¼Œæ‰€ä»¥åœ¨ launch verify kernel å‰åŒæ­¥æ›´æ–°ä¸‹ verify buffer
  - æˆ‘ä»¬ä¸ºäº†ç”Ÿæˆ vocab maskï¼Œéœ€è¦ verify inputï¼Œéœ€è¦æ‹·è´ GPU ä¸Šçš„æ•°æ®ï¼Œè¿™é‡Œç”¨çš„ .cpu() å®é™…ä¸Šæ˜¯ä¸€æ¬¡åŒæ­¥

  > **è¿™é‡Œåº”è¯¥ç”¨å¼‚æ­¥æ‹·è´**
  >
- Verify ä¸ Sampling ä¹‹é—´çš„åŒæ­¥

  - Cpu ä¸Šæ ¹æ®å¾—åˆ°çš„ verify input è¿›è¡Œ vocab mask generateï¼Œè¿™é‡Œéœ€è¦å°† cpu çš„ vocab mask æ‹·è´åˆ° GPU ä¸Š

![](static/WFRhbVkv8oZpkoxam7Nc2FFwnob.png)

### åŒæ­¥çš„ç»†èŠ‚

#### Late Binding

ä¸ºäº†è®© **Plan(prepare data for compute)** å¯ä»¥ä¸ä¾èµ– GPU compute streamï¼Œä»£ç å…è®¸ Plan é˜¶æ®µä½¿ç”¨**ä¸å®Œæ•´çš„æ•°æ®**ã€‚

- Plan é˜¶æ®µå…ˆåˆ†é…å¥½ Bufferï¼Œæˆ–è€…å¤ç”¨ CUDA Graph çš„é™æ€ Bufferã€‚
- Draft é˜¶æ®µç®—å®Œåï¼Œç›´æ¥è°ƒç”¨ update_verify_buffers_to_fill_after_draftï¼Œåˆ©ç”¨ CUDA Kernel é«˜é€ŸæŠŠ Draft ç”Ÿæˆçš„ Mask/Pos **ç›´æ¥å†™å…¥** åˆ° Plan é˜¶æ®µé¢„ç•™çš„æ˜¾å­˜åœ°å€ä¸­ã€‚

## Test SGLang Eagle + Constrained Decoding

### Test sh

```python
import requests
import time
import threading
import json

# å‚æ•°è®¾ç½®
X = 10 # æ€»è¯·æ±‚æ•°
T = 1  # æ€»æ—¶é•¿ï¼ˆç§’ï¼‰
output_file = "output.json"

url = "http://localhost:30000/v1/chat/completions"
headers = {"Content-Type": "application/json"}
payload = {
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "messages": [
        {"content": "Your task is to extract the name of the person from the following text.", "role": "system"},
        {"content": "I'm John Doe. Please write a detailed biography of John Doe, including his background, education, career, achievements, and any interesting stories or anecdotes. Make the biography as long and comprehensive as possible, and ensure the output is in valid JSON format as required by the schema. Add at least 10 different sections, and elaborate on each section with rich details and plausible events. If you need to invent information, make it realistic and consistent. This is a test. " * 20, "role": "user"}
    ],
    "response_format": {
        "json_schema": {
            "name": "extract_name",
            "schema": {
                "properties": {"name": {"type": "string"}},
                "required": ["name"],
                "title": "ExtractName",
                "type": "object"
            }
        },
        "type": "json_schema"
    },
    "max_tokens": 1000,
    "stream": False,
    "temperature": 0.0,
    "top_k": 1,
    "top_p": 1.0,
    "repetition_penalty": 1.0
}

interval = T / X
results = [None] * X

def send_request(_i_):
    try:
        response = requests.post(url, _headers_=headers, _data_=json.dumps(payload))
        results[_i_] = response.text
    except Exception as e:
        results[_i_] = str(e)

threads = []
for i in range(X):
    t = threading.Timer(interval * i, send_request, _args_=(i,))
    t.start()
    threads.append(t)

for t in threads:
    t.join()

with open(output_file, "w") as f:
    for r in results:
        f.write(r + "\n")
```

### Result

```json
[
  {
    "id": "814769f7ecfc4077a542b7a8621b4ed8",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\": \"John Doe\"\n}",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "stop",
        "matched_stop": 2
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2209,
      "completion_tokens": 13,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "bd37e0e219904597a0fb4f3c78764902",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2216,
      "completion_tokens": 20,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "c420563cc2a849a281c5ca86ffca4ec9",
    "object": "chat.completion",
    "created": 1766405504,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n...(truncated for brevity, lots of newlines)...\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "length",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 3196,
      "completion_tokens": 1000,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "e9f0bbd4575e48db8b4497a9dd1a2ec2",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\":\n\": John :\"\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2210,
      "completion_tokens": 14,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "296d8dd652474e3287efed9cb0bc9ef1",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2216,
      "completion_tokens": 20,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "efec7d1f247d4a9a9de10679e31186f0",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2218,
      "completion_tokens": 22,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "a482c6020a1941e993831b896f7f84df",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\":\n\": John :\"\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2210,
      "completion_tokens": 14,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "8f8e886e238c4c00960563d666ea9fa2",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2216,
      "completion_tokens": 20,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "41ae0850c66b4728a9550d1b0f0da787",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2221,
      "completion_tokens": 25,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  },
  {
    "id": "9c3719b8e1304d3d8ba4bc754596a7b6",
    "object": "chat.completion",
    "created": 1766405499,
    "model": "meta-llama/Llama-2-7b-chat-hf",
    "choices": [
      {
        "index": 0,
        "message": {
          "role": "assistant",
          "content": "{\n\"name\": \"John Doe\"\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
          "reasoning_content": null,
          "tool_calls": null
        },
        "logprobs": null,
        "finish_reason": "abort",
        "matched_stop": null
      }
    ],
    "usage": {
      "prompt_tokens": 2196,
      "total_tokens": 2223,
      "completion_tokens": 27,
      "prompt_tokens_details": null,
      "reasoning_tokens": 0
    },
    "metadata": {
      "weight_version": "default"
    }
  }
]
```

- å¹¶å‘é‡å˜å¤§æ—¶ï¼Œæ¯”å¦‚ 1s å†…å‘é€ 10 ä¸ªè¯·æ±‚ï¼ŒåŸºæœ¬ä¸Šå¤§é‡è¯·æ±‚ä¼šåœ¨è¿›è¡Œ grammar accept_token() æ—¶ç›´æ¥ abortï¼Œé€ æˆæœ€åçš„ç»“æœå‡ºç°é—®é¢˜
  - åœ¨å¤„ç† prefill ç”Ÿæˆçš„ç»“æœï¼Œå‡†å¤‡æ¨è¿› json grammar çš„æ—¶å€™ï¼Œaccept_token() å‘ç° next_token_id æ˜¯ INVALID çš„ï¼Œç›´æ¥å°†è¯¥è¯·æ±‚ abort

    ```python
    def process_batch_result_prefill(
    _self_: Scheduler,
    _batch_: ScheduleBatch,
    _result_: Union[GenerationBatchResult, EmbeddingBatchResult],
    ):
    ...
    if req.grammar is not None:
    # FIXME: this try-except block is for handling unexpected xgrammar issue.
    try:
    req.grammar.accept_token(next_token_id)
    except ValueError as e:
    # Grammar accept_token can raise ValueError if the token is not in the grammar.
    # This can happen if the grammar is not set correctly or the token is invalid.
    logger.error(
    f"Grammar accept_token failed for req {req.rid} with token {next_token_id}: {e}"
    )
    _self_.abort_request(AbortReq(_rid_=req.rid))
    req.grammar.finished = req.finished()
    ...
    ```

## ä¼˜åŒ– Overlap: speculative decoding + constrained decoding

### Overview

ä¼˜åŒ–åçš„è°ƒåº¦é€»è¾‘å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š

- æˆ‘ä»¬å°† GPU åˆ° CPU çš„ verify input æ‹·è´ä½¿ç”¨å¼‚æ­¥æ‹·è´

- ä¹‹å‰çš„ constraint decoding + speculative decoding æ˜¯æ²¡æœ‰è¿›è¡Œè°ƒåº¦ä¾§çš„ overlap çš„ï¼›è¿™é‡Œæˆ‘ä»¬åœ¨ GPU è¿›è¡Œ target verify çš„åŒæ—¶ï¼Œå¤„ç† last batch çš„ accept tokens

- Target sample ä¾èµ– cpu ç”Ÿæˆçš„ vocab maskï¼Œè¿™é‡Œä½¿ç”¨ .to(device) è¿›è¡ŒåŒæ­¥

![](static/Si4pbiIAqoOIbJxhwedcdsxNnHf.png)

### Implementation

- ScheduleBatch ä¸­ç»´æŠ¤ä¸€ä¸ª (request, accepted_token_ids) çš„ listï¼Œå­˜æ”¾ last batch è¿˜æœªç»è¿‡ grammar å¤„ç†çš„ accept_tokens

- event_loop_overlap ä¸­ current batch ä¸­æºå¸¦ last batch çš„ accept_tokens

  ```python
  last_batch, last_result = _self_.result_queue[-1]
  # here batch is copied, so has_grammar need also as a param
  if last_batch.has_grammar:
      batch.last_batch_accept_tokens = (last_batch, last_result)
      # Mark that grammar accept will be processed in the next batch's verify
      last_result.grammar_accept_processed = True
  ```

- Verify()
  - GPU åˆ° CPU çš„ verify input çš„æ‹·è´ç”¨å¼‚æ­¥æ‹·è´
  - GPU è¿›è¡Œ Target Verify çš„åŒæ—¶ï¼ŒCPU å¯¹ä¸Šä¸€è½®çš„ grammar accept token è¿›è¡Œå¤„ç†
  - CPU ç­‰å¾… verify input æ‹·è´å®Œæˆåç”Ÿæˆæœ¬è½®çš„ vocab maskï¼Œç„¶ååŒæ­¥åˆ° GPU ä¸Šå‡†å¤‡è¿›è¡Œ Target Sample

### JSON Unit Test Result

| æµ‹è¯•åœºæ™¯                    | No Overlapï¼ˆåŸºå‡†ï¼‰ | Overlapï¼ˆDouble Syncï¼‰ | Overlapï¼ˆOnce Syncï¼‰ | æœ€ä½³ç­–ç•¥    |
| --------------------------- | ------------------ | ---------------------- | -------------------- | ----------- |
| JSON Generateï¼ˆæ ‡å‡†é•¿ç”Ÿæˆï¼‰ | 0.8557s            | 0.7296sï¼ˆ+14.7%ï¼‰      | 0.6687sï¼ˆ+21.8%ï¼‰    | Once Sync   |
| JSON OpenAIï¼ˆçŸ­æ–‡æœ¬ / APIï¼‰ | 0.4455s            | 0.2549sï¼ˆ+42.8%ï¼‰      | 0.3861sï¼ˆ+13.3%ï¼‰    | Double Sync |
| Mix Concurrentï¼ˆæ··åˆå¹¶å‘ï¼‰  | 0.6386s            | 0.5623sï¼ˆ+11.9%ï¼‰      | 0.5468sï¼ˆ+14.4%ï¼‰    | Once Sync   |


```shell
# no overlap
[2025-12-23 13:20:02] Test: test_json_generate | Duration: 0.8557s | Status: PASSED
[2025-12-23 13:20:03] Test: test_json_openai | Duration: 0.4455s | Status: PASSED
[2025-12-23 13:20:04] Test: test_mix_json_and_other | Duration: 0.6386s | Status: PASSED
D

# overlap with double sync mask
[2025-12-23 13:20:47] Test: test_json_generate | Duration: 0.7296s | Status: PASSED
[2025-12-23 13:20:47] Test: test_json_openai | Duration: 0.2549s | Status: PASSED
[2025-12-23 13:20:48] Test: test_mix_json_and_other | Duration: 0.5623s | Status: PASSED

# overlap with once sync mask
[2025-12-23 14:18:26] Test: test_json_generate | Duration: 0.6687s | Status: PASSED
[2025-12-23 14:18:27] Test: test_json_openai | Duration: 0.3861s | Status: PASSED
[2025-12-23 14:18:27] Test: test_mix_json_and_other | Duration: 0.5468s | Status: PASSED
```

### Benchmark Result(bs = 4)

#### Hiding CPU Overhead

- TPOT (Time Per Output Token) ä» **4.07ms é™ä½åˆ°äº† 3.22ms**ï¼Œé™å¹…è¶…è¿‡ 20%ã€‚

#### Accept Length æå‡

- Accept Length ä» **2.59 æå‡åˆ°äº† 2.9**ã€‚
- å¾ˆç¥å¥‡ï¼Œæ­£å¸¸ä¸åº”è¯¥æœ‰ accept length å¢é•¿è¿™ä¹ˆå¤šçš„æƒ…å†µï¼Œå› ä¸ºä½¿ç”¨ç›¸åŒçš„æ¨¡å‹

#### é¦–å­—å»¶è¿Ÿ (TTFT) å¾®å¢

- TTFT ä» **21ms å¢åŠ åˆ°äº† 27ms**ã€‚
- Overlap æµæ°´çº¿é€šå¸¸éœ€è¦æ›´å¤æ‚çš„åˆå§‹åŒ–è¿‡ç¨‹ï¼ˆä¾‹å¦‚é¢„åˆ†é…æ›´å¤æ‚çš„ Cuda Eventã€å»ºç«‹ Pending Info ç»“æ„ã€é¢„çƒ­æµæ°´çº¿çŠ¶æ€ï¼‰ã€‚
- ç¬¬ä¸€è½® Draft/Verify å¾€å¾€æ— æ³•äº«å—åˆ° overlap çš„çº¢åˆ©ï¼ˆå› ä¸ºæ²¡æœ‰ä¸Šä¸€è½®ï¼‰ï¼Œåè€Œæ‰¿æ‹…äº†é¢å¤–çš„è°ƒåº¦é€»è¾‘å¼€é”€ã€‚

### GSM8K

```yaml
# No overlap
python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 1319
Accuracy: 0.232
Invalid: 0.003
Latency: 44.037 s
Output throughput: 3763.649 token/s

# Overlap
python3 benchmark/gsm8k/bench_sglang.py --num-shots 8 --num-questions 1319 --parallel 1319
Accuracy: 0.230
Invalid: 0.003
Latency: 36.554 s
Output throughput: 4559.657 token/s
```

## **Reference**

[1]. [Clover: Regressive Lightweight Speculative Decoding with Sequential Knowledge](https://arxiv.org/pdf/2405.00263)

[2]. [Fast Inference from Transformers via Speculative Decoding](https://arxiv.org/abs/2211.17192)

[3]. [Lookahead: An Inference Acceleration Framework for Large Language Model with Lossless Generation Accuracy](https://arxiv.org/pdf/2312.12728)

[4]. [Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads](https://arxiv.org/abs/2401.10774)

[5]. [EAGLE: Speculative Sampling Requires Rethinking Feature Uncertainty](https://arxiv.org/abs/2401.15077)

[6]. [EAGLE-2: Faster Inference of Language Models with Dynamic Draft Trees](https://arxiv.org/abs/2406.16858)

[7]. [EAGLE-3: Scaling up Inference Acceleration of Large Language Models via Training-Time Test](https://arxiv.org/abs/2503.01840)

[8]. [Speculative Decoding Slides](https://docs.google.com/presentation/d/1iD0ud3Otd1VbB4Q-G7_UQDFgRfVrIEQr3XDyKkcy-xc/edit?slide=id.p#slide=id.p)
