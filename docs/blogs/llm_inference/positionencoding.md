---
title: Position Encoding in Transformer
date: 2025/9/30
update:
comments: true
katex: true
tags:
  - LLM Inference
---

# Position Encoding in Transformer

一般分为两大类：绝对位置编码（Absolute Position Encoding）和相对位置编码（Relative Position Encoding）。本文将介绍这两种位置编码的区别及其重要性，并重点解析一种结合了两者优点的创新方法——旋转位置编码（Rotary Position Embedding, RoPE）

## Why we need Position Encoding

在自然语言处理领域，Transformer 模型已成为一项革命性的技术。然而，其核心的自注意力机制本身并不具备捕捉序列中单词顺序的能力，即“位置无关性”。为了解决这一问题，位置编码应运而生。本文将深入解释两种主要的位置编码方式——绝对位置编码和相对位置编码，阐述它们为何至关重要，并详细解析一种名为旋转位置编码（Rotary Position Embedding, RoPE）的创新方法，是如何巧妙地通过绝对位置编码的形式，融合了相对位置编码的信息。

什么是绝对位置编码和相对位置编码？

1. 绝对位置编码（Absolute Position Encoding）

绝对位置编码为序列中的每个位置分配一个独一无二的、固定的编码向量。这个编码向量旨在告诉模型一个词在序列中的确切位置（例如，第 1 个、第 2 个或第 10 个词）。

在最初的 Transformer 论文《Attention Is All You Need》中，作者使用正弦和余弦函数来生成这些位置编码。其数学表达式如下：

PE(pos,2i)=sin(pos/10000
2i/d
model
​

)
PE(pos,2i+1)=cos(pos/10000
2i/d
model
​

)
其中，pos 是词元在序列中的位置，i 是编码向量的维度索引，d
model
​
是模型的嵌入维度。这种方法的优点在于，即使在训练中未见过的更长序列，它也能生成位置编码。这些位置编码向量会与词嵌入向量相加，然后一同输入到 Transformer 的后续层中。

2. 相对位置编码（Relative Position Encoding）

与绝对位置编码不同，相对位置编码并不关心一个词的绝对位置，而是关注词与词之间的相对距离。例如，模型应该知道“猫”这个词在“追”这个词之后一个位置，而不管它们在句子中的具体位置如何。

相对位置编码通常在自注意力机制计算注意力分数时引入。它会根据查询（Query）和键（Key）之间的相对距离，为注意力分数添加一个偏置项。这样做的好处是，模型可以更好地泛化到不同长度的序列，并且更专注于局部上下文关系。

为什么需要位置编码？
Transformer 的核心是自注意力机制，它会计算序列中每个词与其他所有词之间的关联度。在计算这些关联度时，它将输入视为一个无序的词袋（Bag of Words）。例如，对于“我爱北京天安门”和“天安门爱我北京”这两个句子，如果不引入位置信息，自注意力机制会认为它们的表示是相同的，这显然是不合理的。

因此，位置编码的根本目的就是为模型注入序列的顺序信息，使其能够理解词语的先后关系，从而正确地理解句子的语法和语义。

RoPE：融合绝对与相对的巧妙设计
旋转位置编码（RoPE）是一种新颖的位置编码方式，它通过一种绝对位置编码的实现方式，却在自注意力机制中自然地引入了相对位置信息。

核心思想：

RoPE 的核心思想是将词向量看作是复数，并通过乘以一个与位置相关的旋转矩阵，将绝对位置信息编码进去。具体来说，对于一个位置为 m 的词向量 x
m
​
，RoPE 会对其进行如下变换：

f(x
m
​
,m)=R
m
​
x
m
​

其中，R
m
​
是一个旋转矩阵，它的大小和旋转角度由位置 m 决定。

如何融合相对位置信息？

RoPE 的神奇之处在于当计算查询向量 q（在位置 m）和键向量 k（在位置 n）之间的注意力分数时，由于旋转的性质，它们的内积只与相对位置 (m−n) 有关。

我们可以将查询和键的转换过程表示为：
q
m
′
​
=f(q
m
​
,m)=R
m
​
q
m
​

k
n
′
​
=f(k
n
​
,n)=R
n
​
k
n
​

计算它们之间的内积（即注意力分数）时：
$ (q'\_m)^T k'\_n = (R_m q_m)^T (R_n k_n) = q_m^T R_m^T R_n k_n$

由于旋转矩阵是正交矩阵，其转置等于其逆，即 R
m
T
​
=R
m
−1
​
。因此，上式可以写为：
q
m
T
​
R
m
−1
​
R
n
​
k
n
​
=q
m
T
​
R
n−m
​
k
n
​

从这个最终的表达式可以看出，查询向量 q
m
​
和键向量 k
n
​
之间的内积，等价于将 k
n
​
乘以一个只与它们的相对位置 (n−m) 有关的旋转矩阵 R
n−m
​
，然后再与 q
m
​
做内积。

总结 RoPE 的融合机制：

形式上的绝对位置编码：RoPE 为每个位置 m 定义了一个唯一的旋转矩阵 R
m
​
，这与绝对位置编码的思想一致，即每个位置都有一个特定的编码方式。

效果上的相对位置编码：在自注意力机制的核心计算——查询和键的内积中，由于旋转的数学特性，两个位置的绝对旋转操作，最终等效于一个只依赖于它们相对位置的旋转操作。这使得注意力分数自然地包含了相对距离的信息。

因此，RoPE 通过一种优雅的数学方式，将绝对位置信息（通过旋转）和相对位置信息（通过内积的性质）完美地结合在一起。这种方法不仅具有良好的理论解释性，而且在实践中也被证明在多种长文本任务上表现出色，并被广泛应用于 Llama 等主流大语言模型中。
