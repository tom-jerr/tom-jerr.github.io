---

title: ä¸€æ­¥æ­¥å®ç° CUDA Vector Add ä¼˜åŒ–
created: 2025-10-21
tags:

- LLMInference

---

# ä¸€æ­¥æ­¥å®ç° CUDA Vector Add ä¼˜åŒ–

ğŸ“š æœ¬æ–‡å°†ä»æœ€åŸºç¡€çš„ vector add å®ç°å¼€å§‹ï¼Œä½¿ç”¨ nsight compute å·¥å…·è¿›è¡Œæ€§èƒ½åˆ†æå¯»æ‰¾ç“¶é¢ˆå¹¶ä¸€æ­¥æ­¥è¿›è¡Œä¼˜åŒ–ã€‚é€šè¿‡è¿™ç§æ–¹å¼æ¥å­¦ä¹  CUDA çš„ç¼–ç¨‹å’Œä¼˜åŒ–ã€‚

## Prerequisite

### Arithmetic Intensity(AI)

ç®—æœ¯å¼ºåº¦æ˜¯è¡¡é‡ä¸€ä¸ªè®¡ç®—ä»»åŠ¡ï¼ˆå¦‚ CUDA Kernelï¼‰æ˜¯â€œè®¡ç®—å¯†é›†å‹â€è¿˜æ˜¯â€œè®¿å­˜å¯†é›†å‹â€çš„æ ¸å¿ƒæŒ‡æ ‡ã€‚

**1. å®šä¹‰ï¼š** å®ƒè¢«å®šä¹‰ä¸ºâ€œæ€»å…±æ‰§è¡Œçš„æµ®ç‚¹è®¡ç®—æ“ä½œæ¬¡æ•°â€ä¸â€œæ€»å…±ä¼ è¾“çš„æ•°æ®å­—èŠ‚æ•°â€ä¹‹é—´çš„æ¯”ç‡ã€‚

- **`AI = (æ€»è®¡ç®—æ“ä½œæ•°) / (æ€»è®¿å­˜å­—èŠ‚æ•°)`**
- **å•ä½ï¼š** `FLOPs/Byte` (å³ï¼Œæ¯ä¼ è¾“ä¸€ä¸ªå­—èŠ‚çš„æ•°æ®ï¼Œèƒ½å¯¹åº”æ‰§è¡Œå¤šå°‘æ¬¡æµ®ç‚¹è®¡ç®—)

**2. ä¸ºä»€ä¹ˆå®ƒå¦‚æ­¤é‡è¦ï¼Ÿ** AI å†³å®šäº†ä¸€ä¸ªç¨‹åº**ç†è®ºä¸Šçš„æ€§èƒ½ç“¶é¢ˆ**ã€‚æˆ‘ä»¬å¯ä»¥ç”¨å®ƒæ¥å¯¹æ¯”ä¸€ä¸ªç¨‹åºå’Œä¸€ä¸ªç¡¬ä»¶ï¼ˆGPUï¼‰çš„ç‰¹æ€§ï¼š

- **ç¡¬ä»¶çš„â€œAIâ€**ï¼šGPU ä¹Ÿæœ‰ä¸€ä¸ªå¹³è¡¡ç‚¹ï¼Œå³å®ƒçš„ `å³°å€¼è®¡ç®—èƒ½åŠ› (GFLOPs/s)` / \`å³°å€¼å†…å­˜å¸¦å®½ (GB/s)
- **ç¨‹åºçš„ AI**ï¼šå†…æ ¸çš„ `FLOPs / Bytes`ã€‚

### Roofline

Roofline æ¨¡å‹ï¼ˆå±‹é¡¶çº¿æ¨¡å‹ï¼‰æ˜¯ä¸€ç§ç”¨æ¥**åˆ†æç¨‹åºæ€§èƒ½ç“¶é¢ˆ**ï¼ˆè®¡ç®—å—é™è¿˜æ˜¯å¸¦å®½å—é™ï¼‰çš„æ–¹æ³•ã€‚\
å®ƒæŠŠ**è®¡ç®—æ€§èƒ½**ï¼ˆFLOPs/sï¼‰å’Œ**è®¿å­˜æ€§èƒ½**ï¼ˆBytes/sï¼‰è”ç³»åœ¨ä¸€èµ·ï¼Œã€‚ä»¥å¯è§†åŒ–çš„æ–¹å¼å±•ç¤ºæ€§èƒ½ä¸Šé™

$$
AchievableÂ FLOPs=min(AIÃ—MemoryÂ BW,PeakÂ FLOPs)
$$

![](img/roofline.png)

### Stall in NCU

| ç±»å‹                     | ä»£è¡¨ç­‰å¾…           | å…¸å‹ç“¶é¢ˆ | ä¼˜åŒ–æ–¹å‘           |
| ------------------------ | ------------------ | -------- | ------------------ |
| **All Scoreboard**       | æ‰€æœ‰ä¾èµ–æœªæ»¡è¶³     | å®Œå…¨ç­‰å¾… | æé«˜å¹¶å‘ã€å¼‚æ­¥è®¿é—® |
| **Long Scoreboard**      | DRAM è®¿é—®æœªè¿”å›    | å†…å­˜å»¶è¿Ÿ | ä¼˜åŒ–è®¿å­˜ã€å…±äº«å†…å­˜ |
| **Short Scoreboard**     | L1/å¯„å­˜å™¨ä¾èµ–      | æ•°æ®ä¾èµ– | è°ƒæ•´æŒ‡ä»¤é¡ºåº       |
| **Execution Dependency** | ç®—æœ¯æŒ‡ä»¤ç»“æœæœªå°±ç»ª | è¿ç®—é“¾é•¿ | å¢åŠ  ILP           |
| **Barrier/Branch**       | åŒæ­¥æˆ–åˆ†æ”¯ç­‰å¾…     | æ§åˆ¶æµ   | å‡å°‘åˆ†æ”¯/åŒæ­¥      |

## baseline

```cpp
// FP32
// ElementWise Add grid(N/256),
// block(256) a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)
__global__ void vector_add_kernel(const float *a, const float *b, float *c,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  int n) {
Â  int idx = blockIdx.x * blockDim.x + threadIdx.x;
Â  if (idx < n) {
Â  Â  c[idx] = a[idx] + b[idx];
Â  }
}
```

### åˆ†æ

è¯¥ç‰ˆæœ¬çš„ vector addï¼Œæ‰§è¡Œä¸€æ¬¡è®¡ç®—éœ€è¦ä¸‰æ¬¡è®¿å­˜ï¼Œæ¯æ¬¡è¯» 4 bytes (read A, read B, write C)

$$
AI = 1 / (3 \\times 4) =1/12\\approx 0.083
$$

è¯´æ˜è¿™æ˜¯ä¸€ä¸ª **memory-bound** çš„ç¨‹åº

- æŸ¥çœ‹ ncu çš„ SLO æˆ‘ä»¬å¯ä»¥çœ‹åˆ°ï¼Œç¡®å®å†…å­˜è¾¾åˆ°å³°å€¼æ€§èƒ½çš„ 90% ä»¥ä¸Š

![](img/vector_add_slo.png)

- æŸ¥çœ‹ Memory Workload Analysisï¼Œæˆ‘ä»¬å¯ä»¥å‘ç°æˆ‘ä»¬å®Œå…¨æ²¡æœ‰ç”¨åˆ° shared memoryï¼Œç›´æ¥ç”¨ L2 Cache å’Œ L1 Cache

![](img/memory_chat.png)

- Kernel çš„ä¸»è¦ç“¶é¢ˆæ˜¯ **`Stall Long Scoreboard`**â€”â€”å³**å†…å­˜å»¶è¿Ÿåœé¡¿**ã€‚GPU éšè—è¿™ç§â€œç­‰å¾…â€çš„å”¯ä¸€æœºåˆ¶ï¼Œå°±æ˜¯ **Occupancy (å ç”¨ç‡)**

## Optimization V1 -- vectorized access

### ä¼˜åŒ–

- çŸ¢é‡åŒ–å†…å­˜è®¿é—®ï¼šæˆ‘ä»¬ä¸å†ä¸€æ¬¡è¯»å–ä¸€ä¸ª floatï¼Œè€Œæ˜¯åŒæ—¶å– 4 ä¸ª floatï¼Œ**ä¸€ä¸ª wrap é‡Œé¢çš„æ‰€æœ‰çº¿ç¨‹åŒæ—¶è·å– 4 floatï¼Œå¯ä»¥å˜ä¸º LDG.128 ä¸€æ¬¡æ€§è·å– 128 bit æ•°æ®**
- :warning: è¿™é‡Œæœ‰ä¸ª trade-offï¼Œå¦‚æœæˆ‘ä»¬æ¯ä¸ª thread ä½¿ç”¨äº†è¿‡å¤šçš„å¯„å­˜å™¨ï¼Œé‚£ä¹ˆ SM ä¸Šæ´»è·ƒçš„ Warp ä¼šå˜å°‘ï¼Œå¯¼è‡´ Occupany ä¼šä¸‹é™ï¼Œè¿™åŒæ ·å¯¹æ€§èƒ½å½±å“å¾ˆå¤§

```cpp
// ElementWise Add + Vec4
// grid(N/256), block(256/4)
// a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)
__global__ void vector_add_kernel_vec4(const float *a, const float *b, float *c,
Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â  Â int n) {
Â  int idx = (blockIdx.x * blockDim.x + threadIdx.x) * 4;
Â  if (idx < n) {
Â  Â  float4 a_vec = FLOAT4(a); // registers
Â  Â  float4 b_vec = FLOAT4(b);
Â  Â  float4 c_vec;
Â  Â  c_vec.x = a_vec.x + b_vec.x;
Â  Â  c_vec.y = a_vec.y + b_vec.y;
Â  Â  c_vec.z = a_vec.z + b_vec.z;
Â  Â  c_vec.w = a_vec.w + b_vec.w;
Â  Â  FLOAT4(c[idx]) = c_vec;
Â  }
}
```

### åˆ†æ

- é€šè¿‡è¿™ç§ä¼˜åŒ–åï¼Œæˆ‘ä»¬å·²ç»å°†å†…å­˜åˆå¹¶è®¿é—®ï¼ŒMemory çš„æ•ˆç‡å·²ç»æå‡äº†ï¼Œä½†æ˜¯ Compute æ•ˆç‡è¿˜æ˜¯å¾ˆä½

- **æ—¶é—´å‡ ä¹ç›¸åŒ**ï¼š21.66 Î¼s â†’ 21.25 Î¼sã€‚

- **å†…å­˜å¸¦å®½åˆ©ç”¨ç‡éƒ½é«˜ (~96%)**ï¼šè¯´æ˜ä¸¤è€…éƒ½è¢« **memory-bound** é™åˆ¶ï¼Œè€Œéç®—åŠ›ç“¶é¢ˆã€‚

- **Compute Throughput** åè€Œä¸‹é™ï¼ˆ14% â†’ 3.6%ï¼‰ï¼Œå› ä¸ºç›¸åŒçš„æ•°æ®é‡ä¸‹ï¼Œ`float4` ç‰ˆæœ¬éœ€è¦æ›´å°‘çš„æŒ‡ä»¤ï¼Œå› æ­¤ç®—æœ¯éƒ¨åˆ†ç›¸å¯¹å æ¯”å˜å°ã€‚

- ï¼šå‘é‡åŒ–è®¿é—® (`float4`) ä¼˜åŒ–äº†è®¿å­˜æ•ˆç‡ï¼ˆä¸€æ¬¡è¯»å– 4 ä¸ª floatï¼‰ï¼Œ**ä½†æ€»å†…å­˜å¸¦å®½å·²æ¥è¿‘é¥±å’Œï¼Œæ‰€ä»¥æ€§èƒ½æå‡æœ‰é™ã€‚**

  ![](img/slo_v1.png)

## Optimization V2 -- fp16 compute

### ä¼˜åŒ–

- åœ¨ 30 ç³»åŠä»¥ä¸Šæ˜¾å¡ï¼Œå¯¹ fp16 å’Œ fp32 çš„è®¡ç®—å‡ ä¹æ— å·®åˆ«ï¼Œéƒ½ä½¿ç”¨ç›¸åŒçš„ ALU è¿›è¡Œè®¡ç®—ï¼Œä½†æ˜¯ fp16 æ¯æ¬¡å­˜å–æ¯” fp32 å°‘ä¸€èˆ¬çš„å†…å­˜ï¼Œæ‰€ä»¥**å¯ä»¥èŠ‚çœå†…å­˜å¸¦å®½**ï¼Œä»è€Œå¤§å¹…æé«˜æ•ˆç‡
- FP16 è®¡ç®—ï¼šæˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ RTX Titanï¼Œè¯¥æ˜¾å¡ fp16 çš„è®¡ç®— FLOPS æ˜¯ fp32 çš„ 2 å€å·¦å³

```cpp
// FP16
// ElementWise Add grid(N/256),
// block(256) a: Nx1, b: Nx1, c: Nx1, c = elementwise_add(a, b)
__global__ void elementwise_add_f16_kernel(half *a, half *b, half *c, int N) {
Â  int idx = blockIdx.x * blockDim.x + threadIdx.x;
Â  if (idx < N)
Â  Â  c[idx] = __hadd(a[idx], b[idx]);
}
```

### åˆ†æ

$$
AI = \frac{1 FLOPs}{2 * 3 Bytes} = 1/6 \approx 0.17 FLOPs /Byte
$$

:warning: **fp16 è®¡ç®—çš„ç²¾åº¦ä¼šæ¯” fp32 ä½å¾ˆå¤š**ï¼Œéœ€è¦è€ƒè™‘åº”ç”¨çš„åœºæ™¯æ˜¯å¦å¯ä»¥æ¥å—ï¼Œæµ‹è¯•å‘ç°åŸºæœ¬ç²¾åº¦åœ¨ 0.2 ä»¥ä¸‹

- **Memory Throughput ä¸‹é™ (96%â†’71%)**ï¼šFP16 æ•°æ®é‡åªæœ‰ float çš„ä¸€åŠï¼Œæ˜¾å­˜ä¼ è¾“å‹åŠ›å°ã€‚
- **Compute Throughput ä¸Šå‡ (14%â†’22%)**ï¼šè™½ç„¶æ•°æ®å˜å°ï¼Œä½† ALU å¯¹ FP16 å¤„ç†æ›´å¿«ï¼ˆéƒ¨åˆ†æ¶æ„å¯åŒå‘å°„æˆ–æ›´é«˜å¯†åº¦å¤„ç†ï¼‰ã€‚

## Optimization V3 -- fp16 + more compute per thread

### ä¼˜åŒ–

- `half2` åŒæ‰“åŒ…ä¸€æ¬¡è®¿é—®ä¸¤å€æ•°æ®é‡
  - èŠ‚çœäº†å†…å­˜å¸¦å®½
  - æ‘Šé”€ç®—æœ¯æˆæœ¬

```cpp
__global__ void elementwise_add_f16x2_kernel(half *a, half *b, half *c, int N) {
Â  int idx = 2 * (blockIdx.x * blockDim.x + threadIdx.x);
Â  if (idx < N) {
Â  Â  half2 reg_a = HALF2(a[idx]);
Â  Â  half2 reg_b = HALF2(b[idx]);
Â  Â  half2 reg_c;
Â  Â  reg_c.x = __hadd(reg_a.x, reg_b.x);
Â  Â  reg_c.y = __hadd(reg_a.y, reg_b.y);
Â  Â  HALF2(c[idx]) = reg_c;
Â  }
}
```

### åˆ†æ

$$
AI = \frac{2*(_hadd) FLOPs}{4 * 3 Bytes} = 1/6 \approx 0.17 FLOPs /Byte
$$

- **æ‰§è¡Œæ—¶é—´ç»§ç»­ä¸‹é™**ï¼š13.41 Î¼s â†’ 11.17 Î¼s
- **Memory Throughput æ¢å¤ä¸Šå‡ (71%â†’87%)**ï¼š`half2` åŒæ‰“åŒ…ä¸€æ¬¡è®¿é—®ä¸¤å€æ•°æ®é‡ï¼Œè®¿é—®æ•ˆç‡æå‡ã€‚
- **Compute Throughput ç•¥é™ (22%â†’14%)**ï¼šç®—æœ¯è´Ÿè½½è¢«æ‘Šè–„ï¼ˆè®¡ç®—é‡å°‘ï¼Œä½†è®¿å­˜æ›´å¿«ï¼‰ã€‚

## Optimization V4 -- fp16 + more compute per thread + memory alignment

### ä¼˜åŒ–

- å†…å­˜å¯¹é½ 128 bitï¼Œå¯ä»¥çŸ¢é‡åŒ–è®¿é—®å†…å­˜ï¼Œå‡å°‘æŒ‡ä»¤æ•°
- æ¯ä¸ªçº¿ç¨‹ä¸€æ¬¡å¤„ç† 8 ä¸ª halfï¼ˆå³ 4 æ¬¡ `__hadd2`ï¼‰ï¼ŒåŠ ä¸Š `#pragma unroll` â†’ å±•å¼€å¾ªç¯ï¼Œé¿å…åˆ†æ”¯å¼€é”€

| ä¼˜åŒ–ç»´åº¦         | æ™®é€šç‰ˆæœ¬ï¼ˆ\_\_haddï¼‰    | ä¼˜åŒ–ç‰ˆï¼ˆ\_\_hadd2 + packï¼‰ | æ€§èƒ½æ•ˆæœ                   |
| ---------------- | ----------------------- | -------------------------- | -------------------------- |
| **è®¡ç®—æ¨¡å¼**     | æ¯æ¬¡ç®— 1 ä¸ª half        | æ¯æ¬¡ç®— 2 ä¸ª half           | âœ… è®¡ç®—ååæå‡çº¦ 2Ã—       |
| **å¯„å­˜å™¨åˆ©ç”¨ç‡** | è¾ƒåˆ†æ•£ï¼ŒæŒ‡ä»¤ä¾èµ–å¤š      | æ›´è¿ç»­ï¼ŒæŒ‡ä»¤å¤ç”¨ç‡é«˜       | âœ… æé«˜ ILPã€å‡å°‘ stall    |
| **warp æ•ˆç‡**    | è®¿å­˜æŒ‡ä»¤å¤šï¼Œwarp å¸¸ç­‰å¾… | warp å†…è®¿å­˜å¯¹é½ä¸€è‡´        | âœ… warp active cycles æ›´é«˜ |

```cpp
__global__ void elementwise_add_f16x8_pack_kernel(half *a, half *b, half *c, int N) {
  int idx = 8 * (blockIdx.x * blockDim.x + threadIdx.x);
  // temporary register(memory), .local space in ptx, addressable
  half pack_a[8], pack_b[8], pack_c[8]; // 8x16 bits=128 bits.
  // reinterpret as float4 and load 128 bits in 1 memory issue.
  LDST128BITS(pack_a[0]) = LDST128BITS(a[idx]); // load 128 bits
  LDST128BITS(pack_b[0]) = LDST128BITS(b[idx]); // load 128 bits
#pragma unroll
  for (int i = 0; i < 8; i += 2) {
    // __hadd2 for half2 x 4
    HALF2(pack_c[i]) = __hadd2(HALF2(pack_a[i]), HALF2(pack_b[i]));
  }
  // reinterpret as float4 and store 128 bits in 1 memory issue.
  if ((idx + 7) < N) {
    LDST128BITS(c[idx]) = LDST128BITS(pack_c[0]);
  } else {
    for (int i = 0; idx + i < N; i++) {
      c[idx + i] = __hadd(a[idx + i], b[idx + i]);
    }
  }
}
```

### åˆ†æ

$$
AI = \frac{4*(_hadd2) FLOPs}{16 * 3 Bytes} = 1/6 \approx 0.17 FLOPs /Byte
$$

- ä¸€æ¬¡åŠ è½½ 128 bit æ•°æ®ï¼Œå‡å°‘å†…å­˜äº‹åŠ¡æ•°é‡ï¼ŒèŠ‚çœæ—¶é—´
- å•ä¸ªçº¿ç¨‹çš„å·¥ä½œé‡æå‡
- å®é™… benchmark å’Œ profile å‘ç°ï¼Œåœ¨ RTX Titan æ˜¾å¡ä¸Šï¼Œå¯¹äº 1M å¤§å°çš„æ•°æ®è¿™ä¸ªç®—å­çš„å®é™… Occupancy æ¯” `fp16x2` ç®—å­æ›´å°‘ï¼ŒåŠ é€Ÿæ•ˆæœç•¥ä¸æ˜æ˜¾

## Summary

vector add ç®—å­æ˜¯ä¸€ä¸ªå…¸å‹çš„ memory-bound çš„ç®—å­ï¼Œæˆ‘ä»¬éœ€è¦å°½å¯èƒ½èŠ‚çœå†…å­˜å¸¦å®½å¹¶æé«˜è®¡ç®—æ•ˆç‡ã€‚

> ä¸‹é¢æˆ‘ä»¬ä½¿ç”¨ benchmark æµ‹è¯•æ‰€æœ‰ç®—å­çš„å¼€é”€ï¼Œè§‚å¯Ÿå’Œæˆ‘ä»¬çš„åˆ†æå“ªé‡Œä¸åŒ

### æ´å¯Ÿ 1ï¼šé—®é¢˜è§„æ¨¡ vs. å¯åŠ¨å¼€é”€ (Kernel Launch Overhead)

è¿™ä¸ªæµ‹è¯•å®Œç¾åœ°å±•ç¤ºäº†ä¸ºä»€ä¹ˆ GPU ä¸é€‚åˆå¤„ç†â€œå°ä»»åŠ¡â€ã€‚

- åœ¨ 1K (N=1024) æ—¶

  - CPU Time: **0.0030 ms**

  - GPU (æœ€å¿«): **0.0078 ms**

  - **åŸå› ï¼š** è°ƒç”¨ä¸€ä¸ª CUDA å†…æ ¸ï¼ˆ`__global__ void`ï¼‰æœ¬èº«å°±æœ‰**å›ºå®šçš„å¼€é”€**ï¼ˆ`Kernel Launch Overhead`ï¼‰ï¼Œè¿™ä¸ªå¼€é”€é€šå¸¸éœ€è¦å‡ ä¸ªå¾®ç§’ (microseconds)ã€‚å¯¹äº 1K è¿™æ ·çš„å°é—®é¢˜ï¼ŒGPU å®é™…æ‰§è¡Œè®¡ç®—çš„æ—¶é—´ï¼ˆå¯èƒ½åªæœ‰ 1-2 å¾®ç§’ï¼‰è¿œå°äºå¯åŠ¨å®ƒæ‰€èŠ±è´¹çš„æ—¶é—´ï¼ˆ5-6 å¾®ç§’ï¼‰ã€‚CPU ç›´æ¥åœ¨æœ¬åœ°æ‰§è¡Œå¾ªç¯ï¼Œæ²¡æœ‰ä»»ä½•å¯åŠ¨å¼€é”€ï¼Œæ‰€ä»¥æ›´å¿«ã€‚

- åœ¨ 1M (N=1048576) æ—¶

  - CPU Time: **3.0058 ms**
  - GPU (æœ€å¿«): **0.0132 ms** (å³ `GPU FP16 x2` ç‰ˆæœ¬)
  - **åŸå› ï¼š** å½“é—®é¢˜è§„æ¨¡å˜å¾—è¶³å¤Ÿå¤§æ—¶ï¼Œ**å¤§è§„æ¨¡å¹¶è¡Œï¼ˆMassive Parallelismï¼‰**çš„ä¼˜åŠ¿å¼€å§‹æ˜¾ç°ã€‚å¯åŠ¨å†…æ ¸çš„ 5-6 å¾®ç§’å¼€é”€åœ¨æ€»å…± 13.2 å¾®ç§’çš„æ‰§è¡Œæ—¶é—´ä¸­å˜å¾—å¾®ä¸è¶³é“ã€‚æ­¤æ—¶ï¼ŒGPU çš„**å·¨å¤§å†…å­˜å¸¦å®½**ï¼ˆ`GPU FP16 x2` è¾¾åˆ°äº† **475.7 GB/s**ï¼‰å½»åº•å‡»è´¥äº† CPU çš„å†…å­˜å¸¦å®½ï¼ˆè¢«é™åˆ¶åœ¨ **4.2 GB/s**ï¼‰ã€‚

______________________________________________________________________

### æ´å¯Ÿ 2ï¼šå†…å­˜å¸¦å®½æ˜¯ç“¶é¢ˆ (Memory-Bound)

é€å…ƒç´ åŠ æ³•ï¼ˆVector Additionï¼‰æ˜¯ä¸€ä¸ªç»å…¸çš„\*\*å†…å­˜å¸¦å®½å—é™ï¼ˆMemory-Boundï¼‰\*\*é—®é¢˜ï¼Œå®ƒçš„è®¡ç®—å¯†é›†åº¦ (AI) æä½ã€‚è¿™æ„å‘³ç€æ€§èƒ½çš„ç“¶é¢ˆæ˜¯\*\*ä½ èƒ½å¤šå¿«åœ°ä»æ˜¾å­˜ä¸­è¯»å– a å’Œ bï¼Œå¹¶å†™å› cã€‚

è¿™ä¸ªæµ‹è¯•ç»“æœæ¸…æ¥šåœ°è¯æ˜äº†è¿™ä¸€ç‚¹ï¼š

-  FP32 vs. FP16 (åœ¨ 1M æ•°æ®é›†)

  - `GPU Standard` (FP32): 0.0250 ms

  - `GPU FP16 Std`: 0.0163 ms

- **åŸå› ï¼š** FP32 å†…æ ¸éœ€è¦ç§»åŠ¨çš„æ•°æ®é‡æ˜¯ $1M \times (4+4+4) = 12 \text{ MB}$ã€‚è€Œ FP16 å†…æ ¸åªéœ€è¦ç§»åŠ¨ $1M \times (2+2+2) = 6 \text{ MB}$ã€‚

- **æ•°æ®é‡å‡åŠï¼Œæ€§èƒ½å‡ ä¹ç¿»å€**ï¼ˆ`0.0250 / 0.0163 = 1.53x` åŠ é€Ÿï¼‰ã€‚è¿™è¯å®äº†ç“¶é¢ˆåœ¨å†…å­˜ï¼Œè€Œä¸æ˜¯è®¡ç®—ã€‚

______________________________________________________________________

### æ´å¯Ÿ 3ï¼šbaseline å®ç°åœ¨è¾ƒå°çš„é—®é¢˜è§„æ¨¡ä¸‹å·²è¶³å¤Ÿå¥½

- åœ¨æ•°æ®å°äº 64K å¤§å°çš„æƒ…å†µä¸‹ï¼Œbaseline æ˜¯æœ€ä¼˜çš„æ–¹æ³•

```shell
========== Testing 1K (N=1024) ==========
--- Speedup Analysis ---
Standard vs Vec4:       0.98x
Standard vs FP16:       0.98x
Standard vs FP16x2:     0.97x
Standard vs FP16x8:     0.95x
Standard vs FP16x8_pack:  0.99x

========== Testing 4K (N=4096) ==========
--- Speedup Analysis ---
Standard vs Vec4:       1.02x
Standard vs FP16:       0.78x
Standard vs FP16x2:     0.79x
Standard vs FP16x8:     0.75x
Standard vs FP16x8_pack:  0.76x

========== Testing 16K (N=16384) ==========
--- Speedup Analysis ---
Standard vs Vec4:       1.00x
Standard vs FP16:       0.99x
Standard vs FP16x2:     0.99x
Standard vs FP16x8:     0.87x
Standard vs FP16x8_pack:  0.91x


========== Testing 64K (N=65536) ==========
--- Speedup Analysis ---
Standard vs Vec4:       0.96x
Standard vs FP16:       0.98x
Standard vs FP16x2:     1.00x
Standard vs FP16x8:     0.92x
Standard vs FP16x8_pack:  0.88x
```

### æ´å¯Ÿ 4ï¼š1M æ•°æ®æµ‹è¯•ä¸‹ `FP16 x2` æ˜¯æœ€ä½³çš„ "å·¥ä½œç²’åº¦"

åœ¨æœ€å¤§çš„ 1M æµ‹è¯•ä¸­ï¼Œæˆ‘ä»¬çœ‹ FP16 çš„å„ç§å®ç°ï¼š

1. **`GPU FP16 x2`**: **0.0132 ms** (ğŸ†)
2. `GPU FP16 x8_pack`: 0.0158 ms
3. `GPU FP16 x8`: 0.0161 ms
4. `GPU FP16 Std`: 0.0163 ms

- **`FP16 Std` (1 çº¿ç¨‹/1 å…ƒç´ )** æ˜¯æœ€æ…¢çš„ï¼Œå› ä¸ºå®ƒæœ€æœ´ç´ ï¼Œæ¯ä¸ªçº¿ç¨‹åªåšäº†æœ€å°‘çš„å·¥ä½œï¼Œè°ƒåº¦çš„å¼€é”€ç›¸å¯¹æœ€å¤§ã€‚
- **`FP16 x8` å’Œ `x8_pack` (1 çº¿ç¨‹/8 å…ƒç´ )** è¡¨ç°æ›´å¥½ï¼Œå› ä¸ºå®ƒä»¬è®©æ¯ä¸ªçº¿ç¨‹åšäº†æ›´å¤šå·¥ä½œï¼Œå‡å°‘äº†æ€»çš„è°ƒåº¦å¼€é”€å¹¶ä½¿ç”¨äº†å‘é‡åŒ–æŒ‡ä»¤ï¼ˆå¦‚ Nsight åˆ†ææ‰€ç¤ºï¼‰ã€‚
- **`FP16 x2` ä¸ºä»€ä¹ˆæ›´ä¼˜ï¼Ÿ**
  - `FP16 x2` å†…æ ¸ä½¿ç”¨äº† `half2` æ•°æ®ç±»å‹è¿›è¡ŒåŠ è½½ã€‚è€Œ `x8_pack` ä½¿ç”¨çš„æ˜¯ 128-bit (`float4`) æ“ä½œã€‚
  - åœ¨è¿™ä¸ªç‰¹å®šçš„æµ‹è¯•ä¸­ï¼Œ**`half2` è¾¾åˆ°äº†â€œå·¥ä½œç²’åº¦â€å’Œâ€œç¡¬ä»¶äº²å’Œæ€§â€çš„ sweet spot**ã€‚å®ƒæ¯” `x8_pack` æ›´ç®€å•ï¼Œéœ€è¦æ›´å°‘çš„å¯„å­˜å™¨(`x8_pack` éœ€è¦ 22 ä¸ªï¼Œè€Œ `x2`åªéœ€è¦ 16 ä¸ª)ï¼Œ**å®é™…å ç”¨ç‡ (Achieved Occupancy)æ›´é«˜**ã€‚

## é™„å½•

```shell
Vector Addition Performance Test
================================

========== Testing 1K (N=1024) ==========
Mismatch at index 2: 2.39844 != 2.4 (error: 0.0015626)
Correctness: FAIL (Max error: 0.037506)
Array Size: 1024 elements (0.00 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           0.0030 ms (Bandwidth:    4.1 GB/s)
GPU Standard:       0.0078 ms (Bandwidth:    1.6 GB/s)
GPU Vec4:           0.0079 ms (Bandwidth:    1.5 GB/s)
GPU FP16 Std:       0.0079 ms (Bandwidth:    0.8 GB/s)
GPU FP16 x2:        0.0080 ms (Bandwidth:    0.8 GB/s)
GPU FP16 x8:        0.0082 ms (Bandwidth:    0.8 GB/s)
GPU FP16 x8_pack:   0.0078 ms (Bandwidth:    0.8 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:    0.39x
CPU vs GPU Vec4:        0.38x
CPU vs GPU FP16:        0.38x
CPU vs GPU FP16x2:      0.38x
CPU vs GPU FP16x8:      0.37x
CPU vs GPU FP16x8_pack:  0.39x
Standard vs Vec4:       0.98x
Standard vs FP16:       0.98x
Standard vs FP16x2:     0.97x
Standard vs FP16x8:     0.95x
Standard vs FP16x8_pack:  0.99x
FP16 vs FP16x2:         0.99x
FP16x2 vs FP16x8:       0.98x
FP16x8 vs FP16x8_pack:  1.05x

========== Testing 4K (N=4096) ==========
Mismatch at index 0: 9.60156 != 9.6 (error: 0.00156212)
Correctness: FAIL (Max error: 0.037506)
Array Size: 4096 elements (0.00 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           0.0120 ms (Bandwidth:    4.1 GB/s)
GPU Standard:       0.0060 ms (Bandwidth:    8.1 GB/s)
GPU Vec4:           0.0059 ms (Bandwidth:    8.3 GB/s)
GPU FP16 Std:       0.0077 ms (Bandwidth:    3.2 GB/s)
GPU FP16 x2:        0.0077 ms (Bandwidth:    3.2 GB/s)
GPU FP16 x8:        0.0081 ms (Bandwidth:    3.0 GB/s)
GPU FP16 x8_pack:   0.0080 ms (Bandwidth:    3.1 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:    1.98x
CPU vs GPU Vec4:        2.01x
CPU vs GPU FP16:        1.55x
CPU vs GPU FP16x2:      1.56x
CPU vs GPU FP16x8:      1.48x
CPU vs GPU FP16x8_pack:  1.50x
Standard vs Vec4:       1.02x
Standard vs FP16:       0.78x
Standard vs FP16x2:     0.79x
Standard vs FP16x8:     0.75x
Standard vs FP16x8_pack:  0.76x
FP16 vs FP16x2:         1.01x
FP16x2 vs FP16x8:       0.95x
FP16x8 vs FP16x8_pack:  1.01x

========== Testing 16K (N=16384) ==========
Mismatch at index 0: 38.4062 != 38.4 (error: 0.00624847)
Correctness: FAIL (Max error: 0.100006)
Array Size: 16384 elements (0.00 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           0.0472 ms (Bandwidth:    4.2 GB/s)
GPU Standard:       0.0044 ms (Bandwidth:   45.1 GB/s)
GPU Vec4:           0.0044 ms (Bandwidth:   45.1 GB/s)
GPU FP16 Std:       0.0044 ms (Bandwidth:   22.4 GB/s)
GPU FP16 x2:        0.0044 ms (Bandwidth:   22.4 GB/s)
GPU FP16 x8:        0.0050 ms (Bandwidth:   19.5 GB/s)
GPU FP16 x8_pack:   0.0048 ms (Bandwidth:   20.4 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:   10.82x
CPU vs GPU Vec4:       10.82x
CPU vs GPU FP16:       10.74x
CPU vs GPU FP16x2:     10.75x
CPU vs GPU FP16x8:      9.36x
CPU vs GPU FP16x8_pack:  9.80x
Standard vs Vec4:       1.00x
Standard vs FP16:       0.99x
Standard vs FP16x2:     0.99x
Standard vs FP16x8:     0.87x
Standard vs FP16x8_pack:  0.91x
FP16 vs FP16x2:         1.00x
FP16x2 vs FP16x8:       0.87x
FP16x8 vs FP16x8_pack:  1.05x

========== Testing 64K (N=65536) ==========
Mismatch at index 0: 53.5938 != 53.6 (error: 0.00625229)
Correctness: FAIL (Max error: 0.100006)
Array Size: 65536 elements (0.00 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           0.1875 ms (Bandwidth:    4.2 GB/s)
GPU Standard:       0.0096 ms (Bandwidth:   82.3 GB/s)
GPU Vec4:           0.0099 ms (Bandwidth:   79.3 GB/s)
GPU FP16 Std:       0.0097 ms (Bandwidth:   40.4 GB/s)
GPU FP16 x2:        0.0096 ms (Bandwidth:   41.0 GB/s)
GPU FP16 x8:        0.0103 ms (Bandwidth:   38.0 GB/s)
GPU FP16 x8_pack:   0.0109 ms (Bandwidth:   36.2 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:   19.63x
CPU vs GPU Vec4:       18.91x
CPU vs GPU FP16:       19.28x
CPU vs GPU FP16x2:     19.57x
CPU vs GPU FP16x8:     18.13x
CPU vs GPU FP16x8_pack: 17.27x
Standard vs Vec4:       0.96x
Standard vs FP16:       0.98x
Standard vs FP16x2:     1.00x
Standard vs FP16x8:     0.92x
Standard vs FP16x8_pack:  0.88x
FP16 vs FP16x2:         1.01x
FP16x2 vs FP16x8:       0.93x
FP16x8 vs FP16x8_pack:  0.95x

========== Testing 256K (N=262144) ==========
Mismatch at index 0: 14.3984 != 14.4 (error: 0.00156307)
Correctness: FAIL (Max error: 0.037506)
Array Size: 262144 elements (0.00 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           0.7488 ms (Bandwidth:    4.2 GB/s)
GPU Standard:       0.0111 ms (Bandwidth:  282.9 GB/s)
GPU Vec4:           0.0105 ms (Bandwidth:  299.7 GB/s)
GPU FP16 Std:       0.0108 ms (Bandwidth:  145.7 GB/s)
GPU FP16 x2:        0.0101 ms (Bandwidth:  155.5 GB/s)
GPU FP16 x8:        0.0107 ms (Bandwidth:  146.7 GB/s)
GPU FP16 x8_pack:   0.0109 ms (Bandwidth:  143.9 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:   67.36x
CPU vs GPU Vec4:       71.34x
CPU vs GPU FP16:       69.35x
CPU vs GPU FP16x2:     74.03x
CPU vs GPU FP16x8:     69.83x
CPU vs GPU FP16x8_pack: 68.53x
Standard vs Vec4:       1.06x
Standard vs FP16:       1.03x
Standard vs FP16x2:     1.10x
Standard vs FP16x8:     1.04x
Standard vs FP16x8_pack:  1.02x
FP16 vs FP16x2:         1.07x
FP16x2 vs FP16x8:       0.94x
FP16x8 vs FP16x8_pack:  0.98x

========== Testing 1M (N=1048576) ==========
Mismatch at index 0: 57.5938 != 57.6 (error: 0.00625229)
Correctness: FAIL (Max error: 0.100006)
Array Size: 1048576 elements (0.01 GB)

--- Execution Times (Kernel Only) ---
CPU Time:           3.0058 ms (Bandwidth:    4.2 GB/s)
GPU Standard:       0.0250 ms (Bandwidth:  503.4 GB/s)
GPU Vec4:           0.0233 ms (Bandwidth:  539.8 GB/s)
GPU FP16 Std:       0.0163 ms (Bandwidth:  384.9 GB/s)
GPU FP16 x2:        0.0132 ms (Bandwidth:  475.7 GB/s)
GPU FP16 x8:        0.0161 ms (Bandwidth:  391.2 GB/s)
GPU FP16 x8_pack:   0.0158 ms (Bandwidth:  398.0 GB/s)

--- Speedup Analysis ---
CPU vs GPU Standard:  120.26x
CPU vs GPU Vec4:      128.94x
CPU vs GPU FP16:      183.90x
CPU vs GPU FP16x2:    227.29x
CPU vs GPU FP16x8:    186.88x
CPU vs GPU FP16x8_pack:190.17x
Standard vs Vec4:       1.07x
Standard vs FP16:       1.53x
Standard vs FP16x2:     1.89x
Standard vs FP16x8:     1.55x
Standard vs FP16x8_pack:  1.58x
FP16 vs FP16x2:         1.24x
FP16x2 vs FP16x8:       0.82x
FP16x8 vs FP16x8_pack:  1.02x

========== Test Complete ==========
âœ  build git:(master) âœ—
```
