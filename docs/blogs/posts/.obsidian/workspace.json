{
  "main": {
    "id": "d9e59e36b3e39527",
    "type": "split",
    "children": [
      {
        "id": "2a60c2ff87f61d49",
        "type": "tabs",
        "children": [
          {
            "id": "304cfa464c7fa0b7",
            "type": "leaf",
            "state": {
              "type": "markdown",
              "state": {
                "file": "FlashAttention 原理 v1-v2.md",
                "mode": "source",
                "source": false
              },
              "icon": "lucide-file",
              "title": "FlashAttention 原理 v1-v2"
            }
          }
        ]
      }
    ],
    "direction": "vertical"
  },
  "left": {
    "id": "3ebac05633c06523",
    "type": "split",
    "children": [
      {
        "id": "03fcc67ef28fa02a",
        "type": "tabs",
        "children": [
          {
            "id": "7331db51128bd2e4",
            "type": "leaf",
            "state": {
              "type": "file-explorer",
              "state": {
                "sortOrder": "alphabetical",
                "autoReveal": false
              },
              "icon": "lucide-folder-closed",
              "title": "Files"
            }
          },
          {
            "id": "eec3434c13d6a17b",
            "type": "leaf",
            "state": {
              "type": "search",
              "state": {
                "query": "tag:#online_softmax",
                "matchingCase": false,
                "explainSearch": false,
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical"
              },
              "icon": "lucide-search",
              "title": "Search"
            }
          },
          {
            "id": "224e2bedc15faa86",
            "type": "leaf",
            "state": {
              "type": "bookmarks",
              "state": {},
              "icon": "lucide-bookmark",
              "title": "Bookmarks"
            }
          }
        ]
      }
    ],
    "direction": "horizontal",
    "width": 300
  },
  "right": {
    "id": "26cb5c87cf8ed7f0",
    "type": "split",
    "children": [
      {
        "id": "4d9a8c14ac26c996",
        "type": "tabs",
        "children": [
          {
            "id": "5cf6b4dfc7f71e86",
            "type": "leaf",
            "state": {
              "type": "backlink",
              "state": {
                "file": "FlashAttention 原理 v1-v3.md",
                "collapseAll": false,
                "extraContext": false,
                "sortOrder": "alphabetical",
                "showSearch": true,
                "searchQuery": "",
                "backlinkCollapsed": false,
                "unlinkedCollapsed": true
              },
              "icon": "links-coming-in",
              "title": "Backlinks for FlashAttention 原理 v1-v3"
            }
          },
          {
            "id": "eedc04d01e8fc362",
            "type": "leaf",
            "state": {
              "type": "outgoing-link",
              "state": {
                "file": "VLLM 与 SGLang Scheduler 差异.md",
                "linksCollapsed": false,
                "unlinkedCollapsed": true
              },
              "icon": "links-going-out",
              "title": "Outgoing links from VLLM 与 SGLang Scheduler 差异"
            }
          },
          {
            "id": "c28295f376bdc59d",
            "type": "leaf",
            "state": {
              "type": "tag",
              "state": {
                "sortOrder": "frequency",
                "useHierarchy": true,
                "showSearch": false,
                "searchQuery": ""
              },
              "icon": "lucide-tags",
              "title": "Tags"
            }
          },
          {
            "id": "f8cb133fb029fe0d",
            "type": "leaf",
            "state": {
              "type": "outline",
              "state": {
                "file": "FlashAttention 原理 v1-v2.md",
                "followCursor": false,
                "showSearch": false,
                "searchQuery": ""
              },
              "icon": "lucide-list",
              "title": "Outline of FlashAttention 原理 v1-v2"
            }
          }
        ],
        "currentTab": 3
      }
    ],
    "direction": "horizontal",
    "width": 300
  },
  "left-ribbon": {
    "hiddenItems": {
      "switcher:Open quick switcher": false,
      "graph:Open graph view": false,
      "canvas:Create new canvas": false,
      "daily-notes:Open today's daily note": false,
      "templates:Insert template": false,
      "command-palette:Open command palette": false,
      "bases:Create new base": false
    }
  },
  "active": "304cfa464c7fa0b7",
  "lastOpenFiles": [
    "Implement of Concurrent.md",
    "FlashAttention 原理 v1-v2.md",
    "img/未确认 335329.crdownload",
    "img/flash_v2_warp.jpg",
    "img/未确认 825289.crdownload",
    "img/flash_v2_sram.jpg",
    "img/未确认 995475.crdownload",
    "img/causal_mask.jpg",
    "img/未确认 56531.crdownload",
    "img/flash_attn_v1.png",
    "img/flash_v2.jpg",
    "img/未确认 675784.crdownload",
    "img/flash_v1.jpg",
    "img/未确认 889239.crdownload",
    "img/non-matmul.jpg",
    "img/未确认 676096.crdownload",
    "img/parallelization_kv.gif",
    "img/未确认 398215.crdownload",
    "DP Attention.md",
    "img/tiling_flash_attention.png",
    "img/flash_attention_algo.png",
    "img/未确认 626940.crdownload",
    "PageAttention.md",
    "SGLang 的 KV Cache.md",
    "SGLang Scheduler 技术变迁.md",
    "VLLM 与 SGLang Scheduler 差异.md",
    "Untitled.base",
    "Vector Add Optimization Example.md",
    "上篇：初识 Nebula Graph —— 向量类型支持.md"
  ]
}