# LLM Inference System Technique Guided Book

## Overview

| å±‚çº§     | åç§°                                       | è¯´æ˜                     | ä»£è¡¨æŠ€æœ¯ / æ¨¡å—                                                                                                              |
| ------ | ---------------------------------------- | ---------------------- | ---------------------------------------------------------------------------------------------------------------------- |
| **L0** | **Hardware Layerï¼ˆç¡¬ä»¶å±‚ï¼‰**                  | æä¾›ç®—åŠ›ä¸å¸¦å®½çš„ç‰©ç†åŸºç¡€           | GPU (H100, MI300, Ascend), HBM, NVLink, PCIe, InfiniBand                                                               |
| **L1** | **CUDA Runtime & Kernel Layerï¼ˆè®¡ç®—è¿è¡Œæ—¶å±‚ï¼‰**  | å°è£…ç¡¬ä»¶èµ„æºçš„ç¼–ç¨‹æ¥å£ä¸å¹¶è¡Œæ‰§è¡Œæ¨¡å‹     | CUDA Core, Tensor Core, CUDA Graph, Stream, Kernel Launch, Memory Hierarchy (global/shared/register)                   |
| **L2** | **Distributed Communication Layerï¼ˆé€šä¿¡å±‚ï¼‰** | æä¾›å¤š GPU/å¤šèŠ‚ç‚¹ååŒæœºåˆ¶        | NCCL, RCCL, UCX, RDMA, NVLink/NVSwitch, collective ops (AllReduce/AllGather/Broadcast)                                 |
| **L3** | **Model Execution Layerï¼ˆæ¨¡å‹æ‰§è¡Œå±‚ï¼‰**         | è´Ÿè´£æ‰§è¡Œ Transformer ç­‰æ¨¡å‹è®¡ç®— | FlashAttention, Fused MLP, Rope, kv-cache, Quantization kernel, Triton / TileLang / CUTLASS                            |
| **L4** | **System Algorithms Layerï¼ˆç³»ç»Ÿç®—æ³•å±‚ï¼‰**       | é«˜å±‚æ¨ç†ä¸ä¼˜åŒ–ç®—æ³•              | Speculative decoding, Paged KV cache, Continuous batching, Prefill/decode pipeline, Dynamic batching                   |
| **L5** | **Parallelism Strategy Layerï¼ˆå¹¶è¡Œç­–ç•¥å±‚ï¼‰**    | è´Ÿè´£å¤§æ¨¡å‹åœ¨å¤š GPU ä¸Šçš„åˆ’åˆ†ä¸åŒæ­¥    | TPï¼ˆTensor Parallelismï¼‰, PPï¼ˆPipeline Parallelismï¼‰, DPï¼ˆData Parallelismï¼‰, MoE Parallel, ZeRO/FSDP, sequence parallel     |
| **L6** | **Serving & Scheduler Layerï¼ˆæœåŠ¡è°ƒåº¦å±‚ï¼‰**     | æ¨ç†ç³»ç»Ÿç¼–æ’ã€èµ„æºè°ƒåº¦ã€è¯·æ±‚é˜Ÿåˆ—ç®¡ç†     | vLLM / TensorRT-LLM / TGI / SGLang, token scheduler, memory pool, request batching, graph capture, speculative serving |
| **L7** | **Application Layerï¼ˆåº”ç”¨å±‚ï¼‰**               | é¢å‘æœ€ç»ˆç”¨æˆ·æˆ–ä¸‹æ¸¸ç³»ç»Ÿçš„æ¥å£å±‚        | API, RESTful service, chat interface, RAG integration, caching system                                                  |
## Details

## **L0. Hardware Layerï¼ˆç¡¬ä»¶å±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£åº•å±‚ç®—åŠ›ç»“æ„ã€è®¿å­˜ç“¶é¢ˆä¸é€šä¿¡æ‹“æ‰‘ï¼Œä¸ºæ€§èƒ½åˆ†æå’Œä¼˜åŒ–å¥ å®šåŸºç¡€ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- GPU è®¡ç®—å•å…ƒï¼šSMã€Warpã€Threadã€Blockã€Tensor Core
    
- å†…å­˜å±‚æ¬¡ç»“æ„ï¼šRegister / Shared / L2 / HBM
    
- Memory Bandwidth ä¸ Compute Bound åˆ†æ
    
- GPU æ‹“æ‰‘ï¼šPCIe / NVLink / NVSwitch / InfiniBand
    
- NVLink/NVSwitch æ‹“æ‰‘å¯¹ AllReduce æ€§èƒ½çš„å½±å“
    
- GPU æ€§èƒ½å»ºæ¨¡ï¼šRooflineã€Occupancyã€Throughputã€Latency
    

### ğŸ§© å·¥å…·

- `nvidia-smi`ã€`nvcc --ptxas-options=-v`
    
- Nsight Compute / Nsight Systems / CUPTI / perf
    
- DCGMï¼ˆNVIDIA GPU telemetryï¼‰
    
- NVIDIA-SMI topo æŸ¥è¯¢ GPU äº’è”ç»“æ„
    

### ğŸ” æ¨èé˜…è¯»

- NVIDIA GPU Architecture Whitepapers (Volta â†’ Hopper)
    
- â€œRoofline Model for GPUsâ€ (Williams et al., 2009)
    

---

## **L1. CUDA Runtime & Kernel Layerï¼ˆè®¡ç®—è¿è¡Œæ—¶å±‚ï¼‰**

> ç›®æ ‡ï¼šèƒ½ç¼–å†™/ä¼˜åŒ– GPU kernelï¼Œç†è§£ CUDA ç¼–ç¨‹æ¨¡å‹ã€å¹¶è¡Œæ‰§è¡Œä¸å†…å­˜ç®¡ç†ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- CUDA ç¼–ç¨‹æ¨¡å‹ï¼ˆthread/block/gridï¼ŒSIMTï¼‰
    
- Warp æ‰§è¡Œã€åˆ†æ”¯å‘æ•£ã€åŒæ­¥æœºåˆ¶ï¼ˆ`__syncthreads()`ã€barrierï¼‰
    
- CUDA Stream ä¸ Graph æ‰§è¡Œæœºåˆ¶ï¼ˆå‡å°‘ launch overheadï¼‰
    
- Kernel fusionã€kernel launch overhead
    
- Memory managementï¼šPinned memory / Unified memory / Async copy / Memory pool
    
- Tensor Core MMA æŒ‡ä»¤ã€CUDA WMMAã€CUTLASS åŸºç¡€
    
- Kernel profiling & warp stall åˆ†æ
    

### ğŸ§© å·¥å…·

- Nsight Compute, Nsight Systems
    
- CUDA Graph capture API
    
- CUPTI event counters
    

### ğŸ” æºç å‚è€ƒ

- [CUTLASS](https://github.com/NVIDIA/cutlass)ï¼ˆå®˜æ–¹ç®—å­æ¨¡æ¿åº“ï¼‰
    
- [Tiny-CUDA-NN](https://github.com/NVlabs/tiny-cuda-nn)
    
- [FlashAttention CUDA kernel](https://github.com/Dao-AILab/flash-attention)
    

### ğŸ” æ¨èè®ºæ–‡

- **FlashAttention**: â€œFast and Memory-Efficient Exact Attentionâ€ (Dao et al., 2022)
    
- **Kernel fusion**: â€œA study of Deep Learning Operator Fusionâ€ (Google XLA team)
    

---

## **L2. Distributed Communication Layerï¼ˆé€šä¿¡å±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£å¤š GPU / å¤šèŠ‚ç‚¹é—´é€šä¿¡æœºåˆ¶ï¼ŒæŒæ¡é«˜æ•ˆ AllReduce / Scatter / Gatherã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- NCCL é€šä¿¡åŸè¯­ï¼šAllReduce / AllGather / ReduceScatter / Broadcast
    
- Ring vs Tree æ‹“æ‰‘ç®—æ³•
    
- Overlap compute & commï¼ˆé€šä¿¡ä¸è®¡ç®—é‡å ï¼‰
    
- CUDA-aware communication & RDMA
    
- Hierarchical communicationï¼ˆNode-local + Inter-nodeï¼‰
    
- NCCL groupã€stream groupã€collective fusion
    
- InfiniBand, UCX, GPUDirect RDMA
    

### ğŸ§© å·¥å…·

- `nccl-tests` / NCCL Debug env vars (`NCCL_DEBUG=INFO`)
    
- Nsight Systems æŸ¥çœ‹é€šä¿¡é‡å 
    
- `ibstat`, `nvidia-smi topo -m`
    

### ğŸ” æºç å‚è€ƒ

- [NCCL](https://github.com/NVIDIA/nccl)
    
- [DeepSpeed ZeRO](https://github.com/microsoft/DeepSpeed)
    
- [Megatron-LM communication utils](https://github.com/NVIDIA/Megatron-LM)
    

### ğŸ” æ¨èè®ºæ–‡

- **Baidu Ring AllReduce** (Li et al., 2017)
    
- **Megatron-LM** (Shoeybi et al., 2019)
    

---

## **L3. Model Execution Layerï¼ˆæ¨¡å‹æ‰§è¡Œå±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£ Transformer çš„æ¨ç†è®¡ç®—å›¾åŠå…¶ç®—å­ä¼˜åŒ–ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- Transformer ç»“æ„ï¼ˆAttentionã€FFNã€LayerNormã€Ropeï¼‰
    
- FlashAttention / Fused MLP / QKV projection ä¼˜åŒ–
    
- KV Cache ç®¡ç†ï¼ˆpagedã€dynamicã€hierarchicalï¼‰
    
- é‡åŒ–ä¸ç®—å­èåˆï¼šINT8 / FP8 / SmoothQuant / AWQ
    
- Triton / TileLang / CUTLASS ç¼–å†™é«˜æ€§èƒ½ç®—å­
    
- Kernel autotuningï¼ˆmeta-schedulerï¼‰
    

### ğŸ§© å·¥å…·

- Triton profiler (`triton.testing`)
    
- PyTorch Profiler / TensorRT Profiler
    
- vLLM memory profiler
    

### ğŸ” æºç å‚è€ƒ

- [TileLang](https://github.com/tilelang/tilelang)ï¼ˆæ¯” Triton æ›´åº•å±‚çš„è°ƒåº¦ DSLï¼‰
    
- [TVM](https://github.com/apache/tvm)ï¼ˆç®—å­ç¼–è¯‘æ¡†æ¶ï¼‰
    
- [vLLM kernel repo](https://github.com/vllm-project/vllm)
    
- [TensorRT-LLM kernels](https://github.com/NVIDIA/TensorRT-LLM)
    

### ğŸ” æ¨èè®ºæ–‡

- **FlashAttention-2** (Dao et al., 2023)
    
- **PagedAttention** (vLLM, 2023)
    
- **SmoothQuant** (Xiao et al., 2022)
    
- **AWQ** (Lin et al., 2023)
    

---

## **L4. System Algorithms Layerï¼ˆç³»ç»Ÿç®—æ³•å±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£æ¨ç†é˜¶æ®µçš„è°ƒåº¦ç®—æ³•ä¸ memory ç®¡ç†é€»è¾‘ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- Continuous batchingï¼ˆåŠ¨æ€æ‰¹å¤„ç†ï¼‰
    
- Speculative decoding / draft verification
    
- Prefill & Decode pipeline
    
- KV cache eviction / PagePool / Cache table ç®¡ç†
    
- CUDA graph å¤ç”¨ï¼ˆgraph captureï¼‰
    
- Dynamic shape inference
    
- Context parallel / attention mask åŠ¨æ€æ„å»º
    
- Memory manager / Arena allocator
    

### ğŸ§© å·¥å…·

- Nsight Systems timeline åˆ†æ
    
- vLLM profiling hooks (`--enable-memory-profile`)
    
- PyTorch CUDA Graph API
    

### ğŸ” æºç å‚è€ƒ

- [vLLM scheduler](https://github.com/vllm-project/vllm/blob/main/vllm/core/scheduler.py)
    
- [SGLang batch manager](https://github.com/sgl-project/sglang)
    
- [TensorRT-LLM runtime](https://github.com/NVIDIA/TensorRT-LLM/tree/main/tensorrt_llm/runtime)
    

### ğŸ” æ¨èè®ºæ–‡

- **vLLM** (Kwon et al., 2023)
    
- **SpecInfer** (2023)
    
- **SGLang** (2024) â€“ dynamic serving with speculative decoding
    

---

## **L5. Parallelism Strategy Layerï¼ˆå¹¶è¡Œç­–ç•¥å±‚ï¼‰**

> ç›®æ ‡ï¼šæŒæ¡å¤§æ¨¡å‹æ¨ç†çš„å¹¶è¡Œç­–ç•¥ã€é€šä¿¡æ¨¡å¼å’Œå¼ é‡åˆ’åˆ†æ–¹æ¡ˆã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- Tensor Parallelism (intra-layer)
    
- Pipeline Parallelism (inter-layer)
    
- Data Parallelism (batch-level)
    
- Expert / Mixture-of-Experts Parallelism
    
- Sequence Parallel / Context Parallel
    
- ZeRO Stage 1â€“3ã€FSDPã€Parameter Sharding
    
- Load balancing ä¸ activation checkpointing
    

### ğŸ” æºç å‚è€ƒ

- [Megatron-LM](https://github.com/NVIDIA/Megatron-LM)
    
- [DeepSpeed](https://github.com/microsoft/DeepSpeed)
    
- [TensorRT-LLM parallel modules](https://github.com/NVIDIA/TensorRT-LLM)
    

### ğŸ” æ¨èè®ºæ–‡

- **Megatron-LM** (Shoeybi et al., 2019)
    
- **DeepSpeed ZeRO** (Rajbhandari et al., 2020)
    
- **GSPMD (Google)** (Xu et al., 2021)
    
- **Alpa** (Zheng et al., 2022)
    

---

## **L6. Serving & Scheduler Layerï¼ˆæ¨ç†æœåŠ¡å±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£ LLM æ¨ç†æœåŠ¡å¦‚ä½•è°ƒåº¦è¯·æ±‚ã€ç®¡ç†å†…å­˜ä¸èµ„æºã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- Token-level schedulerï¼ˆprefill/decode overlapï¼‰
    
- Batch padding ä¸ token streaming
    
- Async engine & request queue
    
- CUDA Graph & Stream Reuse
    
- Memory pool åˆ†é…ä¸ç¢ç‰‡å›æ”¶
    
- Multi-model multiplexingï¼ˆå¤šæ¨¡å‹å…±å­˜ï¼‰
    
- RESTful API / GRPC / Streaming Output
    
- Profiling ä¸ SLA ç›‘æ§
    

### ğŸ” æºç å‚è€ƒ

- [vLLM serving entry (`engine.py`)](https://github.com/vllm-project/vllm)
    
- [TensorRT-LLM Inference Server](https://github.com/NVIDIA/TensorRT-LLM)
    
- [TGI (Hugging Face)](https://github.com/huggingface/text-generation-inference)
    
- [SGLang runtime](https://github.com/sgl-project/sglang)
    

### ğŸ” æ¨èè®ºæ–‡

- **vLLM: Easy, Fast, and Cheap LLM Serving** (2023)
    
- **SGLang** (2024)
    
- **SpecInfer** (2023)
    

---

## **L7. Application Layerï¼ˆåº”ç”¨å±‚ï¼‰**

> ç›®æ ‡ï¼šç†è§£ LLM æ¨ç†ç³»ç»Ÿå¦‚ä½•ä¸ä¸Šå±‚åº”ç”¨å¯¹æ¥ã€‚

### ğŸ“š æ ¸å¿ƒçŸ¥è¯†ç‚¹

- Prompt cache / embedding cache / RAG pipeline
    
- Token streaming protocolï¼ˆWebSocket / HTTP chunkï¼‰
    
- Multi-turn session & memory context
    
- Load balancing / autoscaling / failover
    
- Monitoring & observabilityï¼ˆPrometheus, Grafanaï¼‰
    
- Cost optimization & resource scheduling
    

### ğŸ” ç¤ºä¾‹é¡¹ç›®

- OpenAI API serving architecture
    
- ChatGPT / Claude session management
    
- vLLM + FastAPI / RAG Fusionï¼ˆLangChain, LlamaIndexï¼‰