### 1. 核心数学概念（面试必问基础）

不要死记硬背，要理解公式在代码里是怎么体现的。

- 量化公式（Affine Quantization）：

  $$Q(x) = \\text{clamp}(\\text{round}(\\frac{x}{S} + Z), Q\_{min}, Q\_{max})$$

  $$x\_{approx} = (Q(x) - Z) * S$$

  - **S (Scale):** 缩放因子。

  - **Z (Zero-point):** 零点（非对称量化才有，对称量化 Z=0）。

  - **Infra 考点：** **对称量化 (Symmetric)** 通常比 **非对称 (Asymmetric)** 更受欢迎，因为计算时不需要处理 $Z$，可以少一次加法/减法操作，这对极致优化的 Kernel 很重要。

- **量化粒度 (Granularity)：** **决定精度的关键**。

  - **Per-Tensor:** 整个矩阵共用一个 Scale。简单，但精度烂。

  - **Per-Channel (Weight):** 每一行或每一列共用一个 Scale。这是目前 Weight-only 量化的标配。

  - **Per-Token (Activation):** 每一行（即每个 Token）动态计算 Scale。这对 Activation 量化至关重要，因为 Activation 有离群值（Outliers）。

  - **Group-wise:** 比如每 128 个参数共用一个 Scale。这是 AWQ/GPTQ 常用的策略，平衡了元数据开销和精度。

### 2. 两大技术路线（W4A16 vs W8A8）

在推理引擎（如你的 MiniInfer）中，主要有两种流派，必须分清楚：

#### A. Weight-only Quantization (W4A16)

- **含义：** 权重是 INT4，激活值（Input）依然是 FP16。

- **目的：** **省显存**（Memory Bound 场景），主要用于让单卡跑更大的模型（如 24G 显存跑 Llama-3-70B 的 4bit 版）。

- **核心算法：**

  - **GPTQ:** 利用 Hessian 矩阵信息来调整权重误差。

  - **AWQ (Activation-aware Weight Quantization):** 核心洞察是“只有 1% 的权重对精度很重要”，根据 Activation 的幅度来保护那些重要的权重。

- **System 挑战 (你的 MiniInfer 可以做的)：**

  - 计算时，必须把 INT4 的权重在寄存器（Register）里**实时反量化 (Dequantize)** 成 FP16，然后再做 FP16 的 GEMM。

  - **考点：** 这种方式**不一定**比 FP16 快，因为多了反量化的计算开销。只有在 Batch Size 很小（Memory Bound）时才有加速比。

#### B. Weight & Activation Quantization (W8A8 / INT8)

- **含义：** 权重和激活值都是 INT8。

- **目的：** **提速度**（Compute Bound 场景），利用 Tensor Core 的 INT8 计算能力。

- **核心算法：**

  - **SmoothQuant:** 解决 LLM 激活值中存在巨大 Outlier 的问题。通过把 Activation 的量化难度“迁移”一部分给 Weight（数学上的等价变换）。

- **System 挑战：**

  - 需要使用 `IGEMM` (Integer GEMM)。

  - Tensor Core 的 INT8 吞吐量是 FP16 的 2 倍。这才是字节 Data-AML 追求的“极致算力”。

______________________________________________________________________

### 3. 前沿硬件特性（字节 Seed/AML 关注点）

- **FP8 (Floating Point 8):** H100/H800 的杀手锏。

  - **E4M3 vs E5M2:** FP8 有两种格式。通常 Weight 用 E4M3（精度高），Activation 用 E5M2（范围大，为了包住 Outliers）或者也用 E4M3。

  - **优势：** 相比 INT8，FP8 分布更符合神经网络的参数分布，训练和推理都更容易收敛，不需要复杂的 SmoothQuant 技巧。

  - **SGLang 现状：** SGLang 和 vLLM 正在全面转向 FP8 支持。

______________________________________________________________________

### 4. 针对你的项目（MiniInfer/SGLang）的实战建议

为了在面试中展示你懂量化，建议在你的项目中加入以下具体功能：

#### 任务一：在 MiniInfer 中实现 KV Cache 量化 (高性价比)

这是目前工业界最通用的优化，且不涉及复杂的矩阵乘法。

- **原理：** KV Cache 占据了大量显存。将其从 FP16 转为 INT8 或 FP8 存储。

- **实现步骤：**

  1. 在写入 KV Cache 前，计算 Scale，转为 INT8。

  1. 读取 KV Cache 进 Attention 计算时，实时转回 FP16。

- **面试加分点：** 解释这如何增加了有效 Batch Size，从而提升了吞吐量。

#### 任务二：理解 Weight Packing (位操作)

- 如果你做 W4A16，内存里怎么存 4-bit 数据？C++ 没有 `int4` 类型。

- **做法：** 用 `int32` 存 8 个 4-bit 数。

- **System 难点：** CUDA Kernel 里如何高效地用位运算（Shift, Mask）把这 8 个数解包出来？这也是面试常考的 CUDA 编程题。

#### 任务三：SmoothQuant 的 System 视角

- 看看 SmoothQuant 的论文，不需要实现，但要懂原理：$Y = (X \\cdot \\text{diag}(s)^{-1}) \\cdot (\\text{diag}(s) \\cdot W)$。

- 面试官问：_“为什么说 SmoothQuant 把量化的难度从 Activation 转移到了 Weight？”_

  - **答案：** Activation 里的 Outlier 很难量化，所以除以一个系数 $s$ 把它变小；为了保持数学等价，Weight 必须乘以 $s$。因为 Weight 是静态的（Offline），我们可以提前处理好，不用担心 Weight 变大变难量化，但 Activation 变平滑了，就可以用 INT8 做了。

### 总结：你需要准备的技能树

1. **基础：** 熟练写出 $X\_{real} = S * X\_{int}$，知道 Per-channel 和 Per-token 的区别。

1. **算法（懂原理即可）：** 知道 **AWQ** 保护关键权重，**SmoothQuant** 平滑激活值离群点。

1. **工程（重点）：**

   - 知道 **KV Cache Quantization** 是怎么做的。

   - 知道 **De-quantization Kernel** 是如何融合进 GEMM 的。

   - 知道 **FP8** 是 H800 时代的趋势。

**如果你能在 MiniInfer 里加上一个简单的 "KV Cache Int8" 开关，并测出显存节省了 50%，面试 OceanBase 和字节时，这将是一个非常强有力的亮点。**
