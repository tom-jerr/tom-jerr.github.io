
### Pytorch Pin Memory 原理

**只有一步 (DMA 拷贝):** 当你需要把数据传给 GPU 时，因为数据已经在锁页内存中了，物理地址固定。GPU 可以直接通过 DMA 从这块内存读取数据到显存。
    
> **优点：**
> 
> 1. **省去了一次 CPU 拷贝**。
>     
> 2. **异步操作：** 因为 DMA 独立于 CPU 工作，CPU 发出“传输指令”后就可以立马去干别的事（比如准备下一个 Batch 的数据），实现了 CPU 和 GPU 的并行流水线。
>

**未开启 Pin Memory:**

```
[用户数据 (Pageable)]  -->  (CPU 复制)  -->  [临时缓冲区 (Pinned)]  -->  (DMA 复制)  -->  [GPU 显存]
```

**开启 Pin Memory:**

```
[用户数据 (Pinned)]  --------------------->  (DMA 复制)  -------------------------->  [GPU 显存]
```
### SGLang 和 VLLM 的哪些地方有差别


### SGLang 和 VLLM 的调度哪个比较好

## Speculative Decoding
### Speculative Decoding 引入了一个 Draft Model (小模型) 和 Target Model (大模型)。请分析在 Batch Size = 1 和 Batch Size = 128 两种极端情况下，投机采样的收益有何不同？为什么？
- Speculative Decoding 的 motivation 实际上是 batch size 较小的时候，llm inference 是 memory-bound 的，无法充分利用 GPU 计算资源。而通过引入一个小模型，可以在大模型计算的同时，利用小模型的计算结果提前生成一些 token，从而提升整体的吞吐量。
- 当 batch size 较大时，llm inference 已经能够较好地利用 GPU 计算资源，大模型已经处于 Compute-bound 状态。此时再强行加入 Draft Model 的生成和验证步骤，会挤占原本用于服务其他 Request 的算力，导致整体 Throughput 下降。

### 假设 Draft Model 预测了 5 个 Token，经过大模型验证后，第 3 个 Token 被拒绝了（Rejected）。此时 KV Cache 应该如何回滚（Rollback）？这在底层内存管理上有什么开销？
## 量化

### Weight only int4量化和fp8量化，你觉得二者有什么应用上的区别


### 为什么只量化weight，不量化activation


### 动态量化和静态量化的区别？


## Kernel

### 什么是bank conflict，为什么会产生 bank conflict，有哪些解决方法？cutlass 中如何做的？

### 为什么 gemm 解决 bank conflict 使用转置存储？不使用有什么影响？

### 为什么 shared memory 可以优化 gemm，什么情况下需要使用__syncthreads进行同步?


### flash attention 的理解，hopper 架构新特性？


### 怎么定位性能热点，对于cuda kernel有哪些优化手段


### 对c++ template的理解


## 模型推理

### llm推理为什么需要 kvcache，怎么计算kvcache大小，如何管理kvcache？


### 怎么理解延迟与吞吐，在具体业务场景下怎么选择


### 主流的moe模型推理时通讯过程存在的问题以及解决办法?


## Project

### MiniInfer

#### 为什么 kv cache 预先分配？

- 减少 OOM 可能，在初始化阶段就暴露问题，保证推理服务稳定性
	- 一次性把 GPU 上剩余的显存全部，划分成无数个固定大小的块（Blocks），用于存放 KV Cache。
- 减少内存碎片，可以不用分配连续显存，类似操作系统页表
- 减少 CPU 开销，所有分配在模型加载后一次性调用，后续避免调用昂贵的 `CudaMalloc()`

#### 为什么用 `(2, hf_config.num_hidden_layers, config.num_kvcache_blocks, self.block_size, num_kv_heads, head_dim)` 这种方式来进行分配？
- 使用 `torch.empty()` 速度极快，因为它不初始化（不清零）内存，只是在显存上划了个地盘。这块内存里原本的垃圾数据还在，之后写入 KV 时会覆盖它。
- 读取某个 Token 的 KV 时，它的所有 Head 数据在内存中是连续的

#### pytorch 的 Pin Memory 和 异步传输？
- **`pin_memory=True` (锁页内存):**
    - 操作系统通常会对内存进行分页和交换（Swap）。如果内存是“可分页”的，CPU 在拷贝数据给 GPU 之前，必须先把它复制到一个临时的“锁页”缓冲区。
    - 设置 `pin_memory=True` 直接在 CPU 上申请锁页内存。这样 GPU 的 DMA（直接内存访问）控制器可以直接读取这块内存，**省去了一次 CPU 内部的拷贝**，大幅提升传输带宽。

- **`non_blocking=True` (异步传输):**
    - 默认情况下，`.cuda()` 是同步的，CPU 会卡住等待数据完全传到显存。
    - 设置 `non_blocking=True` 后，CPU 发出传输指令后**立即返回**，继续执行下一行 Python 代码（比如准备下一个 Batch 的数据），而不需要傻等数据传输完成。这实现了 **CPU 计算与 GPU 数据传输的流水线重叠（Overlap）**。
- CUDA Stream 上的任务是严格顺序的
	- 虽然 CPU 发出了 `model(input_tensor)` 指令，但这个计算指令会被放入**同一个 Stream 队列**。 GPU 会严格遵守队列顺序：**必须等前面的“搬运指令”执行完，才会执行后面的“计算指令”。**
	- **所以，数据绝对是安全的**